<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://lizhongzheng.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://lizhongzheng.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-28T01:28:57+00:00</updated><id>https://lizhongzheng.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Delay-Doppler Domain Signaling and OTFS</title><link href="https://lizhongzheng.github.io/blog/2025/OTFS/" rel="alternate" type="text/html" title="Delay-Doppler Domain Signaling and OTFS"/><published>2025-01-06T00:00:00+00:00</published><updated>2025-01-06T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2025/OTFS</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2025/OTFS/"><![CDATA[<h2 id="the-high-level-reasons-for-otfs">The High Level Reasons for OTFS</h2> <p>OTFS stands for “Orthogonal Time Frequency Space”, which is a modulation method proposed around 2016 as an alternative modulation scheme. It aims to replace or partially replace the current OFDM modulation. The point is that OFDM modulates information-carrying symbols on different sub-carriers with different frequencies. In a multi-path environment with high mobility, each received version of these frequency components has a Doppler shift as the reflectors move. This causes the symbol on one sub-carrier to have an “echo” on different sub-carriers, hence interference with the symbols on those sub-carriers. It is generally known that OFDM is vulnerable to severe Doppler shift. When the Doppler shift is comparable to sub-carrier spacing, there can be significant inter-symbol interference.</p> <p>For example, an LTE system might have a carrier frequency of \(f_c = 3.5\) GHz and sub-carrier spacing of \(15\) kHz. If a mobile receiver moves at \(v=180\) km/h, the Doppler shift can be up to</p> \[f_c \cdot \frac{v}{c} = 3.5 \cdot 10^9 (\text{Hz}) \cdot \frac{180 \cdot 10^3 (\text{m/h})}{3600 (\text{s/h}) \cdot 3 \cdot 10^8 (\text{m/s})} \approx 0.6 (\text{kHz}),\] <p>which is not a negligible amount. This worsens when we use higher carrier frequencies in the tens of GHz range and when mobile devices move at a higher speed, such as in high-speed trains or drones. The situation is bad not because of the one-on-one inter-sub-carrier interference but because one symbol can interfere with a consecutive group of sub-carriers.</p> <p>The delay-doppler domain signal representation can be viewed as an alternative set of basis functions. The promise is that if there is a single reflector with a fixed delay \(\tau\) and moves at a fixed speed corresponding to a fixed Doppler shift \(\nu\), then <strong>the signal on one of the basis functions is simply moved to a single different basis function</strong>. Now, since there are only a few such reflectors and they move at a manageable speed (not multiplied by the carrier frequency or the speed of light), the result is a fixed and sparse inter-symbol-interference pattern. This makes OTFS modulation particularly easy for equalization in high mobility environments.</p> <p>The point of this tutorial is to derive this new set of basis functions and compare that with the normal OFDM basis. This serves as a baseline for our future development of 2-D filter for equalization in OTFS systems.</p> <h2 id="time-frequency-domain-representation">Time-Frequency Domain Representation</h2> <p>For the purpose of comparison, we write out the standard time-frequency representation of signals, which is the foundation of the OFDM modulation and closely related to OTFS.</p> <p>To start with, we quote the well-known sampling theory. For \(x_0(t)\) that is band limited to \(\left(-\frac{\Delta f}{2}, \frac{\Delta f}{2} \right)\), we know we can reconstruct with samples with interval \(T = \frac{1}{\Delta f}\).</p> \[x[n, m=0] = x_0(t) \big | _{t = n T}\] <p>That is, we can write</p> \[x_0(t) = \sum_n x[n, 0] \cdot \mathrm{sinc}\left(\frac{\pi (t -nT) }{T} \right)\] <p>Now recall the aliasing effect, an arbitrary signal \(x(t)\) can always be written as</p> \[x(t) = \sum_m x_m(t), \qquad \forall t\] <p>where each \(x_m(t)\) is limited to the band \(((m-\frac{1}{2} )\Delta f, (m+\frac{1}{2}) \Delta f)\), and a corresponding sampling theorem:</p> \[x_m(t) = \sum_n x[n,m] \cdot \mathrm{sinc}\left( \frac{\pi (t-nT)}{T}\right) \cdot e^{j 2 \pi m \frac{t}{T}}, \quad \forall t\] <p>This basically says that the modulated sinc functions are a complete set of basis functions. Here, we use the superscript “TF” for basis functions in the time-frequency (TF) domain.</p> \[\begin{aligned} \phi^{\mathsf{TF}}_{m,n}(t) &amp;\stackrel{\Delta}{=} \mathrm{sinc}\left( \frac{\pi (t-nT)}{T} \right) \cdot e^{j 2\pi m\frac{t}{T}}, \quad \forall t, m, n \end{aligned}\] <p>and write the following as a representation of an arbitrary signal \(x(t)\), or a way to modulate symbols \(x[m,n]\) on a waveform.</p> \[\begin{aligned} x(t) &amp;= \sum_{m,n} x[m,n] \cdot \phi^{\mathsf{TF}}_{m,n}(t) \end{aligned}\] <p>We will see that OTFS is just a different set of basis functions in the delay-Doppler (DD) domain for which the channel response of a multi-path doppler wireless channel is sparse. The key concept for this construction is the <a href="https://en.wikipedia.org/wiki/Zak_transform">Zak Transform</a>.</p> <h2 id="zak-transform">Zak Transform</h2> <hr/> <dl> <dt>Definition: Zak Transform</dt> <dd>For a complex continuous waveform \(x(t)\) and a fixed \(T&gt;0\) and \(\Delta f = 1/T\), the Zak transform is</dd> </dl> \[Z_x(\tau, \nu) \stackrel{\Delta}{=} \sqrt{T} \sum_{k=-\infty}^\infty x(\tau + kT) \cdot e^{-j2\pi k\nu T}\] <hr/> <p><br/></p> <p><strong>Property 1: Linearity</strong></p> \[Z_{a x+ b y} (\tau, \nu) = a \cdot Z_x(\tau, \nu) + b \cdot Z_y(\tau, \nu)\] <p><strong>Property 2: Periodicity in Doppler and Quasi-Periodicity in Delay</strong></p> <p>For all integers \(n, m\):</p> \[\begin{align*} Z_x (\tau, \nu + m \Delta f) &amp;= Z_x (\tau, \nu)\\ Z_x (\tau + nT , \nu) &amp; = Z_x(\tau, \nu) \cdot e^{j2\pi n \nu T} \end{align*}\] <p>These two ban both be derived by directly plugging in the definition.</p> <p><strong>Property 3: Inverse Transform</strong></p> \[x(t) = \sqrt{T} \int_0^{\Delta f} Z_x(t, \nu) \; d \nu\] <details><summary>Proof of Property 3</summary> \[\begin{align*} &amp;\sqrt{T} \int_0^{\Delta f} Z_x(t, \nu) \; d \nu \\ &amp;= \sqrt{T} \int_0^{\Delta f} \left(\sqrt{T} \sum_{k=-\infty}^\infty x(t + kT) \cdot e^{-j2\pi k\nu T} \right) \;d \nu\\ &amp;= T \sum_{k=-\infty}^\infty X(t+ kT) \cdot \left(\int_0^{\frac{1}{T}} e^{-j 2\pi k \nu T} \; d \nu \right)\\ &amp;= T \sum_{k=-\infty}^\infty X(t+ kT) \cdot \left(\frac{1}{T} \cdot \delta(k)\right) \\ &amp; = x(t) \end{align*}\] </details> <p><br/></p> <p><strong>Property 4: Fourier Transform</strong></p> \[\mathcal F_x (f) =\frac{1}{\sqrt{T}} \int_0^T Z_x(\tau, f) e^{-j2\pi f \tau} \; d\tau\] <details><summary>Proof for Property 4:</summary> \[\begin{align*} &amp; \frac{1}{\sqrt{T}} \int_0^T Z_x(\tau, f)\cdot e^{-j2\pi f \tau} \; d\tau\\ &amp;= \frac{1}{\sqrt{T}} \int_0^T \left( \sqrt{T} \sum_{k=-\infty}^\infty x(\tau + kT) \cdot e^{-j2\pi kf T}\right) \cdot e^{-j 2\pi f \tau} \; d \tau\\ &amp;= \int_0^T \sum_{k=-\infty}^\infty x(\tau + kT) \cdot e^{-j 2\pi f(\tau + kT)} \; d\tau\\ &amp;= \int_{-\infty}^\infty x(t) \cdot e^{-j2\pi f t} \; dt\\ &amp;= \mathcal F_x(f) \end{align*}\] </details> <p><br/></p> <p><strong>Property 5: Delay and Frequency Shift</strong></p> <p>If \(r(t) = x(t-\tau_0), \forall t\), then</p> \[Z_r(\tau, \nu) = Z_x(\tau - \tau_0, \nu)\] <p>If \(r(t) = x(t) e^{j2\pi \nu_0 t}, \forall t\), then</p> \[Z_r(\tau, \nu) = Z_x(\tau , \nu-\nu_0) \cdot e^{j2\pi \nu_0 \tau}\] <p>If there is a single reflector with delay \(\tau_0\) and Doppler shift \(\nu_0\), i.e. the received signal</p> \[r(t) = x(t-\tau_0) \cdot e^{j 2\pi \nu_0(t-\tau_0)}\] <p>Then in Zak representation</p> \[Z_r(\tau, \nu) = Z_x(\tau - \tau_0, \nu - \nu_0) \cdot e^{j 2\pi \nu_0 (\tau-\tau_0)}\] <p><br/></p> <p><strong>Property 6: Convolution and Product</strong></p> <p>If \(a(t)\) and \(b(t)\) have Zak transform of \(Z_a(\tau, \nu), Z_b(\tau, \nu)\), resp., and define</p> \[\begin{align*} c(t) &amp;= \int_{-\infty}^\infty a(t') \cdot b(t-t')\; dt'\\ d(t) &amp;= a(t)\cdot b(t) \end{align*}\] <p>Then</p> \[\begin{align*} Z_c(\tau, \nu) &amp;= \frac{1}{T} \int_0^T Z_a(\tau - \tau', \nu) \cdot Z_b(\tau', \nu) \; d\tau'\\ Z_d(\tau, \nu)&amp;= \sqrt{T} \int_0^{\Delta f} Z_a(\tau, \nu-\nu') \cdot Z_b(\tau, \nu') \; d\nu' \end{align*}\] <p><br/></p> <h3 id="functional-basis-from-zak-transform">Functional Basis from Zak Transform</h3> <p>We are now ready to define the Zak Transform, in a similar way that Fourier Transform is defined, for continuous time signal \(x(t), t \in \mathbb R\).</p> <hr/> <dl> <dt>Theorem: Zak Transform and Inverse</dt> <dd>For a complex continuous waveform \(x(t)\) and a fixed \(T&gt;0\) and \(\Delta f = 1/T\), define for each \(\tau_0 \in [0, T), \nu_0 \in [0, \Delta f)\),</dd> </dl> \[\begin{align*} p_{(\tau_0, \nu_0)} (t) \stackrel{\Delta}{=} \sqrt{T} \cdot \sum_{k = -\infty}^\infty e^{j2\pi \nu_0 k T} \cdot \delta(t-\tau_0 -kT) \end{align*}\] <p>The Zak Transform can be written by sampling \(x(t)\) with the above impulse train:</p> \[Z_x(\tau_0, \nu_0) = \int_{-\infty}^\infty x(t) \cdot p^*_{(\tau_0, \nu_0)}(t) \; dt\] <p>Inversely, we have</p> \[x(t) = \int_0^T \int_0^{\Delta f} Z_x(\tau_0, \nu_0) \cdot p_{(\tau_0, \nu_0)} (t) \; d\nu_0 d\tau_0\] <p>Each \(p_{(\tau_0, \nu_0)}(t)\) is called a <strong>Zak Basis Function</strong>.</p> <hr/> <p><strong>Remarks:</strong></p> <details><summary>1.Comparison to Fourier Transform (FT)</summary> <p>In Fourier Transform, we also have each complex exponential as a “Fourier Basis Function”:</p> \[p_f(t) = e^{j 2\pi f t}\] <p>The inverse FT can be thought as writing \(x(t)\) as linear combinations of the basis functions:</p> \[x(t) = \int_{-\infty}^\infty \mathcal{F}\_x(f) \cdot p_f(t) \; df\] <p>The forward FT can be thought as making projection to find, for \(x(t)\), the coefficient on each basis function</p> \[\mathcal {F}_X(f) = \int_{t=-\infty}^\infty x(t) \cdot p^*_f(t) \; dt\] <p>There is a clear parallelism.</p> <p><br/></p> </details> <details><summary>2.A Delay-Doppler Domain View of the Basis Functions</summary> <p><br/> One can easily check that the Zak transform of a basis function $p_{(\tau_0, \nu_0)}(t)$ is</p> \[Z_{p_{(\tau_0, \nu_0)}} (\tau, \nu) = \sum_{m=-\infty}^\infty \sum_{n=-\infty}^\infty \delta (\tau - \tau_0 - nT) \cdot \delta (\nu - \nu_0 - m \Delta f) \cdot e^{j2\pi \nu_0 n T}\] <p>This is the impulse train that satisfies both the properties of periodicity in Doppler and quasi-periodicity in delay. There a a continuum of basis functions here for $\tau_0 \in [0, T), \nu_0 \in [0, \Delta f)$, corresponding to the uncountably many dimensions in the space of general functions. <br/></p> </details> <details><summary>3.A Channel in Delay-Doppler (DD) Domain</summary> <p><br/> A key assumption is that a wireless channel can be written in the DD domain as</p> \[h(\tau, \nu) = \sum_j h_j \cdot \delta(\tau -\tau_j) \cdot \delta(\nu - \nu_j)\] <p>where the sum is over multi-paths. When using this channel model directly in the DD domain, one would convolve $h(\tau, \nu)$ with $Z_x(\tau, \nu)$ in both the delay and the Doppler shift.</p> <p>The key assumption is that there are a very limited number of multi-path, which makes the channel sparse in the DD domain. The same channel represented in the TF domain would be no so sparse.</p> <p><br/></p> </details> <p><br/></p> <h3 id="time-and-frequency-limits-and-degrees-of-freedom">Time and Frequency Limits and Degrees of Freedom</h3> <p>Different from the time-frequency domain representation, which is naturally related to the bandwidth limit and time limit in signal transmissions, the delay-Doppler domain representation does not give rise to the functional basis that is limited in the time or frequency domain. This can be seen from the fact that \(p_{(\tau_0, \nu_0)}(t)\) defined above is neither time or frequency limited. We need to implement these limits in a separate step that appears different from the TF domain treatment.</p> <p>One can think of the parameters \(T\) and \(\Delta f = 1/T\) in the the definition of the Zak transform as a single degree of freedom. In a spread-spectrum system, we often need to consider multiplexing many lines of data in a collection of degrees of freedoms. Here, we consider this collection as signals that are time limited in \(t \in [0, NT)\) and frequency limited in \(f \in [0, M \cdot \Delta f)\). For convenience, we call this space of waveforms \(\Omega(NT, M \Delta f)\).</p> <p><strong>Remark:</strong> To be precise, there is no such a waveform that is perfectly time limited and band limited. The notion of limitation here is an approximation, which gets more precise in the L2 sense as \(M\cdot N\to \infty\). This is a standard procedure in the development of Fourier Transform. <a href="https://www.mit.edu/~6.450/handouts/6.450book.pdf">Gallager’s book</a> chapter 4 is a good reference for this topic, among many others.</p> <details><summary>We have a brief summary of those treatment here.</summary> <p><br/> Here, we use the normalize Sinc function as</p> \[\mathrm{sinc}(t) \stackrel{\Delta}{=} \frac{\sin(\pi t)}{\pi t}\] <p>and the T-F Fourier Transform, not in \(\omega\):</p> \[{\mathcal F}_x(f) \stackrel{\Delta}{=} \int_{-\infty}^\infty x(t) \cdot e^{-j2\pi f t} \; dt\] <p>This gives the easy Fourier Transform pair of \(\mathrm{sinc}(at) \leftrightarrow \frac{1}{a} \mathrm{rect}\left(\frac{f}{a} \right),\) where \(\mathrm{rect}(t/a) = 1\) if \(t \in (-a/2, a/2).\)</p> <p><br/></p> <p>Now, consider sampling \(x(t)\) with interval \(T_s\), or sampling frequency of \(F_s = 1/T_s\).</p> \[x(t) = \sum_{n=-\infty}^\infty x[n] \cdot \mathrm{sinc}\left(\frac{t-n T_s}{T_s}\right)\] <p>We know that this equation holds only for band-limited \(x(t)\), with \(\mathcal F_x(f) \neq 0\) only on \(f \in [-\frac{1}{2}F_s, \frac{1}{2}F_s]\), i.e.</p> \[\mathcal F_x(f) = \mathcal F_x(f) \cdot \mathrm{rect} \left(\frac{ f}{F_s}\right).\] <p>We call \(W = \frac{1}{2} F_s\), the bandwidth of the waveform \(x(t)\). Now to specify this waveform within a time window of \(t \in [0, T]\), we need to specify all the samples within the window</p> \[x[n] = x(t) \big|_{t=nT_s} : nT_s \in [0, T]\] <p>or equivalently \(n = 0, 1, \ldots \frac{T}{T_s} = 2WT\); and we should think of the out-of-window part of the signal, \(x(t), t \not\in [0, t]\) as small, with a vanishing power.</p> <p><br/></p> <p>As an alternative way to write the same waveform as linear combination of basis functions, we can consider dividing the entire bandwidth into \(M\) sub-bands. That is</p> \[x(t) = \sum_{m=0}^{M-1} x_m(t) \cdot e^{j 2\pi (\frac{2m+1}{2M}-\frac{1}{2}) F_s t}\] <p>where each \(x_m(t)\) is the baseband signal occupies \(\frac{1}{m}\) of the total bandwidth, \([-\frac{1}{2M} F_s, \frac{1}{2M} F_s]\), modulated with \(e^{j 2\pi (\frac{2m+1}{2M}-\frac{1}{2}) F_s t}\). Each \(x_m(t)\) can be written from its samples at a sampling rate of \(F_s/M\) as</p> \[x_m(t) = \sum_{n=0}^{N-1} x[m,n] \cdot \mathrm{sinc}\left( \frac{t - n M T_s }{MT_s}\right)\] <p>where to cover a time window of \(T\), we need \(N = \frac{T}{MT_s}\). The overall waveform can be written as</p> \[\begin{align*} x(t) &amp;= \sum_{n=0}^{N-1} \sum_{m=0}^{M-1} x[m,n] \cdot \phi_{m,n}(t)\\ &amp;= \sum_{n=0}^{N-1} \sum_{m=0}^{M-1} x[m,n] \cdot \mathrm{sinc}\left( \frac{t - n M T_s }{MT_s}\right) e^{j 2\pi (\frac{2m+1}{2M}-\frac{1}{2}) F_s t} \end{align*}\] <p>which defines the basis functions \(\phi_{m,n}\) as the modulated sinc functions. There are \(M\cdot N = \frac{T}{T_s} = 2WT\) such basis functions. Gallager calls these basis function “Sinc-Weighted Sinusoid Functions”.</p> </details> <hr/> <p>Our next step is to construct a set of ortho-normal basis for the space time and frequency limited signals \(\Omega(NT, M\Delta f)\). This can be done in a bruteforce way: we can take the set of ortho-normal basis functions \(\{p_{(\tau_0, \nu_0)}(t), \tau_0 \in [0, T), \nu_0 \in [0, \Delta f)\}\) defined in Theorem 1, truncate each of them in both time and frequency, and then pick out a finite number of orthogonal ones.</p> <p>For the truncation, we use the following two functions defined as rectangles in time and frequency, resp.:</p> \[\begin{align*} q(t) &amp;\stackrel{\Delta}{=} \begin{cases}1 &amp; t \in [0, NT)\\ 0 &amp;\mbox{otherwise}\end{cases}\\\\ {\mathcal F}_s(f) &amp;\stackrel{\Delta}{=} \begin{cases}1 &amp; f \in [0, M \Delta f)\\ 0 &amp;\mbox{otherwise}\end{cases}\\\\ s(t) &amp;= M \Delta f \cdot \mathrm{sinc}(M\Delta f \cdot t)\cdot e^{j 2\pi \frac{M \Delta f}{2} t} \end{align*}\] <p>Now we use these two functions to truncate the basis functions as</p> <hr/> <dl> <dt>Definition:</dt> <dd> <p>For each \(\tau_0 \in [0, T), \nu_0 \in [0, \Delta f)\), the truncated basis function of \(\Omega(NT, M\Delta f)\) is</p> </dd> </dl> \[\psi_{(\tau_0, \nu_0)}(t) \stackrel{\Delta}{=} (p_{(\tau_0, \nu_0)}(t) \cdot q(t) ) * s(t) = \sqrt{T} \cdot \sum_{n=0}^{N-1}s(t - \tau_0 - nT) \cdot e^{j 2\pi \nu_0 nT}\] <p>for each \(p_{(\tau_0, \nu_0)}\) defined in Theorem 1.</p> <hr/> <p>The next step is to select a subset of orthogonal basis functions. To do that, we simply do even sampling on \([0, NT] \times [0, M\Delta f]\).</p> <dl> <dt>Definition: DD-Domain Basis for \(\Omega (NT, M\Delta f)\)</dt> <dd> <p>For \(k = 0, 1, \ldots, N-1, l = 0, 1, \ldots, M-1\)</p> </dd> </dl> \[\alpha_{(k,l)} (t) \stackrel{\Delta}{=} \frac{1}{\sqrt{MN}} \cdot \psi_{(\tau_0, \nu_0)}(t) \Big|_{\tau_0 = \frac{l}{M}T, \nu_0 = \frac{k}{N}\Delta f}\] <hr/> <p>Now all we need to verify is that these constructed basis functions are indeed ortho-normal.</p> <dl> <dt>Theorem: Orthonormal Basis</dt> <dd>The basis defined above is an \(MN\)-dimensional ortho-normal basis. That is</dd> </dl> \[\displaystyle \int_{-\infty}^\infty \alpha_{(k, l)}(t) \cdot \alpha^\dagger_{(k', l')}(t) \; dt = \delta(k-k') \cdot \delta(l-l')\] <hr/> <details><summary>Proof: Hopefully the construction is intuitive enough, but a detailed proof is given here.</summary> <p><br/></p> <p>We need to show that \(\alpha_{(k,l)}(t)\) and \(\alpha_{(k', l')}(t)\) are orthogonal if \((k,l) \neq (k',l')\).</p> <p>Write</p> \[\begin{align*} \alpha_{(k,l)}(t) &amp;= \frac{\sqrt{T}}{\sqrt{MN}} \cdot \sum_{n=0}^{N-1} e^{j2 \pi \frac{k}{NT} nT} \cdot M \Delta f \cdot e^{j 2\pi \frac{M \Delta f}{2} (t- \frac{lT}{M} - nT)} \cdot \mathrm{sinc}\big(M \Delta f \cdot (t- \frac{lT}{M} - nT)\big) \end{align*}\] <p>The important fact that we need is</p> \[\begin{align*} &amp;\int_{-\infty} ^\infty a\cdot \mathrm{sinc}(a (t-\tau_1)) \cdot a \cdot \mathrm{sinc}(a(t-\tau_2)) \; dt\\ =&amp; a \cdot \mathrm{sinc}(a (t-\tau_1)) * a \cdot \mathrm{sinc}(a (t-\tau_2))\\ =&amp; a \cdot \mathrm{sinc}(a (\tau_1-\tau_2)) \end{align*}\] <p>Apply this with \(a = M\Delta f\), we have</p> \[\begin{align*} \langle \alpha_{(k,l)}, \alpha_{(k', l')}\rangle &amp;= \int_{-\infty}^\infty \alpha_{(k,l)}(t) \cdot \alpha^\dagger _{(k',l')}(t) \; dt\\ =&amp;\frac{T}{MN} \sum_{n_1=0}^{N-1} \sum_{n_2 = 0}^{N-1} e^{j2\pi \frac{n_1 k-n_2 k'}{N}} e^{j 2\pi \frac{M}{2} (\frac{l'-l}{M} + (n_2-n_1)) }\cdot (M\Delta f) \mathrm{sinc}\big( (l'-l) +M(n_2-n_1) \big) \end{align*}\] <p>The argument in the \(\mathrm{sinc}\) function is an integer, and the result is zero only when the argument is \(0\). Because \(l, l' \in [0, M-1]\), this happens if and only if \(l = l', n_1 = n_2\), which greatly simplifies the calculation. (This same orthogonality of sinc functions with some regular spaced delays is also used in the sampling theorem. ) Use this, we have:</p> \[\begin{align*} \langle \alpha_{(k,l)}, \alpha_{(k', l')}\rangle &amp;= \delta(l'-l) \cdot \frac{1}{N} \sum_{n=0}^{N-1} e^{j2\pi \frac{n(k-k')}{N}}\\ &amp;= \delta(l'-l) \cdot \delta(k'-k) \end{align*}\] <p>The last sum is the sum of samples of a sinusoid over \((k-k')\) cycles, which is also a frequently used fact.</p> </details> <p><br/></p> <hr/> <p>For the sake of the following discussion, we now define an approximate version of the above orthonormal basis. For convenience, we start with a new notation</p> \[g(t; A) \stackrel{\Delta}{=} \begin{cases} \frac{1}{\sqrt{A}} &amp; t \in [0, A) \\ 0 &amp; \mbox{otherwise} \end{cases}\] <p>which is the rectangle function over \([0,A)\) normalized to have unit power. Now we write from the definition</p> \[\begin{align*} \alpha_{(k,l)}(t) &amp;= \frac{\sqrt{T}}{\sqrt{MN}} \cdot \sum_{n=0}^{N-1} s \left(t - l \cdot \frac{T}{M} - nT \right) \cdot e^{j2\pi \frac{kn}{N}}\\ &amp;= \frac{\sqrt{M}}{\sqrt{NT}} \cdot \sum_{n=0}^{N-1} \mathrm{sinc}\left( \frac{M}{T} \cdot t - l - nM \right) \cdot e^{j\pi (\frac{M}{T}\cdot t - l - nM)}\cdot e^{j2\pi \frac{kn}{N}} \\ &amp; \approx \frac{1}{\sqrt{N}} \cdot \sum_{n=0}^{N-1} g\left(t - l - nM; \frac{T}{M}\right) \cdot e^{j\pi (\frac{M}{T}\cdot t - l - nM)}\cdot e^{j2\pi \frac{kn}{N}}\\ &amp; \stackrel{\Delta}{=} \tilde{\alpha}_{(k,l)}(t) \end{align*}\] <p>Roughly speaking, if we replace the sinc function in \(s(t)\), for which the main slobe has width \(\frac{1}{M\Delta f} = \frac{T}{M}\), with a rectangle function of the same width and the same unit power \(g(t; \frac{T}{M})\), we get a new set of ortho-normal basis. This gives an intuitive view of the basis: \(\alpha_{(k,l)}\), or approximately \(\tilde{\alpha}_{(k,l)}\) consists of \(N\) narrow impulses, with energy focused in the narrow time interval \([\frac{l}{M}T + nT, \frac{l+1}{M}T + nT)\), for \(l = 0, 1, \ldots, M-1, n=0, \ldots, N-1\). These \(N\) narrow impulses are shifted in phase with \(e^{j 2\pi \frac{kn}{N}}\) determined by \(k\).</p> <p><br/></p> <h2 id="the-construction-of-the-otfs-modulation">The Construction of the OTFS Modulation</h2> <p>The following are the main steps to construct the OTFS modulation.</p> <ol> <li> <p>We can write every waveform in \(\Omega(NT, M\Delta f)\) as linear combinations of the orthonormal basis defined in the last section. We can potentially modulate information on these coefficients and transmit the corresponding waveform.</p> </li> <li> <p>For the same waveform, we will derive a map between this set of coefficients and the coefficients on other more familiar basis. This is clearly a linear map that can be thought as a change of basis. An easy implementation of this map allows us to turn the data symbols that we want to modulate on the DD-domain basis into the symbols that we can modulate on another basis, and thus construct a usable OTFS modulation system.</p> </li> </ol> <h3 id="otfs-modulation">OTFS Modulation</h3> <p>With the definition of the DD-Domain basis functions, it is natural to define new modulation scheme as</p> \[x(t) = \sum_{k=0}^{N-1}\sum_{l=0}^{M-1} x_{\mathsf{DD}}[k,l] \cdot \alpha_{(k,l)}(t)\] <p>where \(x_{\mathsf{DD}}[k,l], k=0, \ldots, N-1, l = 0, \ldots, M-1\) are \(MN\) independent digital symbols, multiplexed in one waveform. To demodulate these symbols, one can use \(\alpha_{(k,l)}(t)\) as the output matched filters to separate these data symbols. The point is that when there is only a few paths with different delays and Doppler-shifts that remain constant, the interference between these separate data symbols can be light.</p> <p>In comparison, the time-frequency domain modulation can be written as</p> \[x(t) = \sum_{n=0}^{N-1} \sum_{m=0}^{M-1} x_{\mathsf{TF}}[n, m] \cdot \beta_{(n, m)}(t)\] <p>where</p> \[\beta_{(n,m)}(t) = \frac{1}{\sqrt{MN}} g(t-nT; T) e^{j 2\pi m \Delta f (t-nT)}\] <p>with \(g(t; T) = \frac{1}{\sqrt{T}}\) for \(t \in [0, T)\). One can see that the set of functions \(\beta_{(n,m)}\) also forms an orthonormal basis of \(\Omega(NT, M\Delta f)\), and this more familiar form of T-F domain modulation is precisely what we do in OFDM, which is only a change of basis from OTFS.</p> <p>Now we need to write out this change of coordinate explicitly.</p> <h2 id="the-sparsity-in-dd-domain-a-numerical-experiment">The Sparsity in DD-Domain, a Numerical Experiment</h2>]]></content><author><name>Lizhong Zheng</name></author><category term="Wireless-Communication"/><category term="wireless-communication"/><summary type="html"><![CDATA[The High Level Reasons for OTFS]]></summary></entry><entry><title type="html">Learning with Side Information</title><link href="https://lizhongzheng.github.io/blog/2024/Side-Information/" rel="alternate" type="text/html" title="Learning with Side Information"/><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/Side-Information</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/Side-Information/"><![CDATA[<blockquote> <h3 id="the-key-points">The Key Points</h3> <p>We introduce our first multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to extract a \(k\)-dimensional feature of the data \(\mathsf x\) that carries useful information to infer the value of the label \(\mathsf y\) while excluding the information that a jointly distributed side information \(\mathsf s\) can provide. We use this example to develop the underlying geometric structure of multi-variate dependence and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions.</p> </blockquote> <h2 id="previously">Previously</h2> <p>We started by observing that the dependence between two random variables \(\mathsf x\) and \(\mathsf y\) can be decomposed into a number of modes, each mode as a simple correlation between a feature \(f_i(\mathsf x)\) and a feature \(g_i(\mathsf y)\). We formulated the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a> as the optimization problem</p> \[\begin{equation} (f^\ast_i, g^\ast_i, i=1, \ldots, k) =\arg\min_{\substack{f_1, \ldots f_k \in \mathcal {F_X},\\g_1, \ldots g_k \in \mathcal {F_Y}}} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2 \end{equation}\] <p>with the constraints that both \(f^\ast_1, \ldots, f^\ast_k\) and \(g^\ast_1, \ldots, g^\ast_k\) are collections of orthogonal feature functions; and that the correlation \(\sigma_i = \rho(f^\ast_i(\mathsf x), g^\ast_i(\mathsf y))\) are arranged in a descending order.</p> <p>We proposed the <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-Score networks</a> as a numerical approach, using interconnected neural networks to learn the modal decomposition from data. For two collections of feature functions written in vector form as \(\underline{f}, \underline{g}\), the H-score is defined as</p> \[\mathscr{H}(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])\] <table> <tbody> <tr> <td><img src="/assets/img/Hscorenetwork.png" alt="test image" width="250" style="float:left; padding-right:30px"/></td> <td><img src="/assets/img/nested H2.png" alt="test image" width="450"/></td> </tr> <tr> <td><b> H-Score Network </b></td> <td><b> Nested H-Score Network to find features orthogonal to a given \(\bar{f}\) </b></td> </tr> </tbody> </table> <p><br/></p> <p>We showed that maximizing the H-score is equivalent to solving the optimization problem in (1) but without some of the constraints. As a step to enrich our toolbox for extracting feature functions, we also developed the <a href="https://lizhongzheng.github.io/blog/2024/nested-H-score/">Nested H-Score networks</a> to learn feature functions that are orthogonal to a given functional subspace. We give examples to demonstrate that such a projection operation in the functional space can be quite versatile, including enforcing all the desired constraints in the original modal decomposition problem (1).</p> <p>These previous results now include the main tools we need to proceed: to find a limited number of information-carrying feature functions and to make projections in functional space, which can all be learned directly from data using interconnected neural networks. In this page, we start to make the case that these are the critical building blocks for more complex multi-variate learning problems.</p> <p><br/></p> <h2 id="learning-with-side-information">Learning with Side Information</h2> <table> <tbody> <tr> <td><img src="/assets/img/sideinfo.png" alt="test image" width="350"/></td> </tr> <tr> <td><b> Feature Extraction with Side Information </b></td> </tr> </tbody> </table> <p><br/></p> <p>We consider the multi-variate learning problem as shown in the figure. Here, we assume that the data \(\mathsf x\), the label \(\mathsf y\), and the side information \(\mathsf s\) are jointly distributed according to some model \(P_{\mathsf {xys}}\) which is not known. Our goal is to find a \(k\)-dimensional feature function \(f_{[k]} = [f_1, \ldots, f_k] : \mathcal X \to \mathbb R^k\), such that when we observe the value of \(\mathsf x\), we can infer the value of \(\mathsf y\) based on these \(k\) features. The only difference between this and a conventional learning problem is that we assume the side information \(\mathsf s\) can be observed at the decision maker. That is, our decision is based on the value of \(\mathsf s\) and the features: \(\widehat{\mathsf y} (f_{[k]}(x), s)\). More importantly, when we extract the feature functions, we know that such side information is available to the decision-maker.</p> <p>As a starting point, we assume we have plenty of samples \((x_i, y_i, s_i), i=1, \ldots\) jointly sampled from the unknown model. Also, we assume that the decision-maker employs the optimal way to combine the side information \(\mathsf s\) and the received \(k\) features of \(\mathsf x\) to estimate the value of \(\mathsf y\). We can imagine a neural network is used to learn this decision function perfectly. The focus of this problem is how to find the \(k\) feature functions to best facilitate this decision-making.</p> <p>The main tension of the problem is on having a limited number of \(k\) features, which is often a much lower dimensional representation of the data \(\mathsf x\). Intuitively, we want the features to be about the dependence between \(\mathsf x\) and \(\mathsf y\) so that we can make good predictions, yet we would like to avoid reporting any information that the side information \(\mathsf s\) can provide. Related problems can be found in the context of <a href="https://en.wikipedia.org/wiki/Information-theoretic_security">protecting sensitive information</a>, <a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)">fairness in machine learning</a>, and many other <a href="http://web.eng.ucsd.edu/~yhk/nit.html">multi-terminal information theory</a> problems. The difficulty here is that we would like to learn these feature functions using neural networks and thus enjoy the computational efficiency and flexibility therein, but we have the additional task of tuning the feature functions to avoid overlapping contents. It turns out that what we need is a projection operation in the functional space.</p> <p><br/></p> <h2 id="decomposition-of-multi-variate-dependence">Decomposition of Multi-Variate Dependence</h2> <p>For this problem, the dependence we would like to work on is the dependence between \(\mathsf x\) and \((\mathsf {s,y})\). We write the PMI as</p> \[\mathrm{PMI}_{\mathsf {x; s,y}} = \log \frac{P_{\mathsf {xsy}}}{P_{\mathsf x}\cdot P_{\mathsf {sy}}} \; \in \mathcal {F_{X\times S\times Y}}\] <p>For this space of joint functions, we define inner product with reference distribution \(R_{\mathsf {xsy}} = R_{\mathsf x}R_{\mathsf {sy}}\), parallel the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">definition of modal decomposition</a>, with random variable \(\mathsf y\) replaced by the tuple \(\mathsf {(s,y)}\). We will take the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/#properties-of-modal-decomposition">local assumption</a> that \(P_\mathsf x \approx R_\mathsf x, P_{\mathsf {sy}} \approx R_{\mathsf {sy}}\). Under this assumption, in the following, we do not distinguish between \(\mathrm{PMI}_\mathsf{x;s,y}\) and its approximation</p> \[\widetilde{\mathrm{PMI}}_{\mathsf{x; s,y}} = \frac{P_\mathsf{xsy}}{P_\mathsf x\cdot P_{\mathsf {sy}}}-1.\] <p>Now we consider the subset of joint distributions that satisfy the Markov condition \(\mathsf {x-s-y}\), which means that \(\mathsf x\) is independent of \(\mathsf y\) given \(\mathsf s\). The corresponding PMI functions form a linear subspace. The linearity follows from the fact that conditional independence is a set of linear (equality) constraints on the joint distribution. We denote this subspace as \(\mathcal M\).</p> <p>For a general PMI function, we now decompose that into the component in \(\mathcal M\) and that orthogonal to \(\mathcal M\).</p> <p><br/></p> <hr/> <dl> <dt>Definition: Markov Component and Conditional Dependence Components</dt> <dd> <p>For any given PMI function, $\mathrm{PMI}_{\mathsf {x; s,y}} $, the <strong>Markov component</strong> is</p> </dd> </dl> \[\pi_M \stackrel{\Delta}{=}\Pi_M(\mathrm{PMI}_{\mathsf{x; s,y}}) = \arg\min_{\pi \in \mathcal M}\; \left\Vert \mathrm{PMI}_{\mathsf{x; s,y}} - \pi\right\Vert^2\] <p>The optimization is overall all valid PMI functions \(\pi \in \mathcal M\), i.e., with a corresponding joint distribution of \(\mathsf{x, s, y}\) that satisfies the Markov constraint \(\mathsf {x-s-y}\). The norm \(\Vert\cdot \Vert\) in the objective function is defined on the functional space with reference distribution \(R_{\mathsf {xsy}} = P_{\mathsf x}P_{\mathsf{sy}}\).</p> <dl> <dt>Definition: Conditional Dependence Component</dt> <dd> <p>The conditional dependence component of the above PMI function is</p> </dd> </dl> \[\pi_C \stackrel{\Delta}{=}\mathrm{PMI}_{\mathsf{x; s,y}} - \pi_M\] <hr/> <p><br/></p> <p>By definition, \(\pi_C\) is the error of a linear projection, so we have \(\pi_C \perp \mathcal M\), and the Pythagorean relation.</p> \[\begin{align*} \Vert \mathrm{PMI}_{\mathsf{x; s,y}}\Vert^2 &amp;= \Vert \pi_M\Vert^2 + \Vert \pi_C\Vert^2\\ I (\mathsf{x; (s,y)}) &amp;= I(\mathsf{x;s}) + I(\mathsf{x;y|s}) \end{align*}\] <p>As we stated in the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/#properties-of-modal-decomposition">properties of modal decomposition</a>, under the local assumption, we have \(\Vert \mathrm{PMI}_{\mathsf {x; s,y}}\Vert^2 \approx 2\cdot I(\mathsf x; \mathsf {s,y})\). Here one can show an additional fact that \(\Vert \pi_M\Vert^2 \approx 2 \cdot I(\mathsf x; \mathsf s)\). From that, we also have \(\Vert \pi_c \Vert^2 \approx 2 \cdot I(\mathsf x; \mathsf y \vert \mathsf s)\). Thus, the above Pythagorean relation is simply a geometric version of the chain rule of mutual information.</p> <p>It decomposes the dependence between \(\mathsf x\) and a pair of random variables \(\mathsf {s,y}\) into two orthogonal components: one follows the Markov constraint and only captures the \(\mathsf {x-s}\) dependence, and the other is the conditional dependence between \(\mathsf x\) and \(\mathsf y\) conditioned on \(\mathsf s\).</p> <p>Going back to our problem of learning with side information. It is clear at this point that in extracting the features of $\mathsf x$, we do not want any component in \(\mathcal M\) since that is only helpful to predict the value of \(\mathsf s\), which is already available at the decision maker.</p> <p>The optimal choice of feature functions under this setup should be the \(k\) strongest modes of \(\pi_C\)! By definition, we need to find the component of the PMI that is orthogonal to \(\mathcal M\), which requires a projection operation in the functional space, and we can do that with a nested H-Score network.</p> <p><br/></p> <h2 id="solution-by-nested-h-score-networks">Solution by Nested H-Score Networks</h2> <table> <tbody> <tr> <td><img src="/assets/img/nn_side.png" alt="test image" width="400"/></td> </tr> <tr> <td><b> Nested H-Score Network for Learning with Side Information </b></td> </tr> </tbody> </table> <p><br/></p> <p>The figure shows a solution to use nested H-Score networks to learn the modal decomposition of the \(\pi_C\) component. Again, it is easier to see how it works with sequential training. We can first train the \(\bar{f}, \bar{g}\) functions using the top H-score, which gives a modal decomposition of the \(\mathsf {x-s}\) dependence, or equivalently, the \(\pi_M\) component. Then we can freeze these choices of \(\bar{f}, \bar{g}\), and train \(f, g\) with the lower H-score, which gives the modal decomposition of the \(\mathsf {x-(s,y)}\) dependence that is orthogonal to \(\mathrm{span}(\bar{f} \otimes \bar{g})\), i.e., the span of all \(\bar{f}_i \otimes \bar{g}_i, i = 1, \dots, \bar{k}\), which is aligned with the \(\pi_C\) component that we want to find. Of course, in practice, we train all networks simultaneously and get the same desired result.</p> <p>A slightly subtle point here is that in order to make the learned \(f, g\) feature functions to be orthogonal to \(\mathcal{M}\), we need to make sure that \(\mathrm{span}(\bar{f} \otimes \bar{g})\) is large enough to cover \(\mathcal M\). In particular, the dimensionality, i.e.,\(\bar{k}\) the number of nodes at the output layer of the \(\bar{f}\) and \(\bar{g}\) networks, need to be large enough. More precisely, we will need \(\bar{k} \geq \mathrm{rank} (P_{\mathsf {xs}})\). This is feasible, particularly if the side information \(\mathsf s\) is a small categorical variable. In such “nice” cases, we can show that the learned \(f, g\) feature functions, with expressive neural networks and a sufficient amount of training, are the desired modal decomposition of the \(\pi_C\) component and hence are the optimal choice of features for the side information problem. A proof of this can be found in this <a href="http://lizhongzheng.mit.edu/sites/default/files/documents/Multivariate%20Feature%20Extraction.pdf">paper</a>.</p> <p>In the case that we cannot guarantee that \(\bar{k}\) is “large enough,” the situation is a bit more complex. Optimizing the top H-Score would fix \(\bar{f}, \bar{g}\) on a \(\bar{k}\) dimensional subspace of \(\mathcal M\), and the nested structure guarantees that \(f, g\) are trained to be orthogonal to this subspace, instead of \(\mathcal M\), and thus can have components in \(\mathcal M\). In the context of learning with side information, this means that the extracted features would have some reduced level of repetitive information of the side information \(\mathsf s\), but not completely repetition-free.</p> <h3 id="pytorch-implementation">Pytorch Implementation</h3> <p>Here is a <a href="https://colab.research.google.com/drive/1g3_vgDWDpHtKgV1x6aoiRJzZ1DaZTCOo#scrollTo=1cZgm3V9Ad9K">colab demo</a> of PyTorch implementation, which also illustrates how to explore the dependence among multiple data observations using this nested H-score.</p> <h2 id="going-forward">Going Forward</h2> <p>The important message here is that multi-variate dependence, represented in the functional space, can be decomposed into a number of subspaces, each corresponding to the dependence of a subset of the variables. Thus, in problems with more than two random variables involved, such as distributed learning, learning with time-varying models, multiple tasks, sensitive information, etc., we often need to separate the observed information according to these subspaces and treat different parts of the information differently. Nested H-Score networks are a good method for this task of separating different types of information by making projections in the functional space without changing the objective function or brute-force post-processing that is separated from the training process. We view this as an attestation of the power of the geometric view of the statistical dependence and the algorithms based on such insights. We will post some more examples in the same spirit, where the information geometric view can lead to some unconventional ways to use neural networks.</p> <p><br/></p> <hr/> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/">Dr. Xiagxiang Xu</a>.</p>]]></content><author><name>Lizhong Zheng</name></author><category term="ML-Theory"/><category term="H-Score"/><category term="modal-decomposition"/><summary type="html"><![CDATA[The Key Points We introduce our first multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to extract a \(k\)-dimensional feature of the data \(\mathsf x\) that carries useful information to infer the value of the label \(\mathsf y\) while excluding the information that a jointly distributed side information \(\mathsf s\) can provide. We use this example to develop the underlying geometric structure of multi-variate dependence and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions.]]></summary></entry><entry><title type="html">Symbol Detection in Wireless Communication</title><link href="https://lizhongzheng.github.io/blog/2024/symbol-detection/" rel="alternate" type="text/html" title="Symbol Detection in Wireless Communication"/><published>2024-07-01T00:00:00+00:00</published><updated>2024-07-01T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/symbol-detection</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/symbol-detection/"><![CDATA[<blockquote> <h3 id="the-key-points">The Key Points</h3> <p>In our previous developments, we introduced the <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-score network</a>, which is a way to learn informative features from datasets, similar to a normal neural network but was claimed to be more flexible. In this post, we use a concrete example of wireless communications to demonstrate this flexibility. In a nutshell, the advantage of our solution comes from changing the objective from learning to solve a specific inference task to learning feature functions that carry information that can be reused and combined with other sources of information. We demonstrate that this approach can help us to design neural-network-based solutions not to a single case but a parameterized set of cases, as required by this specific engineering problem. We use this example to discuss the general methodology of integrating learning modules into engineering solutions.</p> </blockquote> <h2 id="previously">Previously</h2> <p>This post is based on a sequence of previous posts. We briefly summarize the main points that we will be using here.</p> <p>In <a href="http://localhost:4000/blog/2024/modal-decomposition/">Modal Decomposition</a>, we stated that the dependence between two random variables, \(\mathsf{x, y}\) can be decomposed into a sequence of correlation between feature pairs \(f_i(\mathsf{x}), g_i(\mathsf{y}), i=1, 2, \ldots\). These feature functions are defined using the following optimization:</p> \[\underline{f}^\ast, \underline{g}^\ast = \arg\min_{\substack{\underline{f} \in \mathcal {F_X}^k \\ \underline{g} \in \mathcal {F_Y}^k}} \; \Vert \mathrm{PMI}_{\mathsf{x,y}} -\underline{f} \otimes \underline{y}\Vert^2\] <p>where \(\mathrm{PMI}_{\mathsf{x,y}} = \log \frac{P_{\mathsf{xy}}}{P_{\mathsf{x}} P_{\mathsf{y}}}\) is the point-wise mutual information function; \(\underline{f} = [f_1, \ldots, f_k], \underline{g} = [g_1, \ldots, g_k]\) are two collections of orthonormal feature functions with correlation \(\sigma_i = \rho (f_i(\mathsf{x}), g_i(\mathsf{y}))\) in a descending order.</p> <table> <tbody> <tr> <td><img src="/assets/img/Hscorenetwork.png" alt="test image" width="250" style="float:left; padding-right:30px"/></td> <td><img src="/assets/img/nested H2.png" alt="test image" width="450"/></td> </tr> <tr> <td><b> H-Score Network </b></td> <td><b> Nested H-Score Network to find features orthogonal to a given \(\bar{f}\) </b></td> </tr> </tbody> </table> <p><br/></p> <p>In <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-score</a>, we defined a new metric called the H-score:</p> \[\mathscr{H}(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])\] <p>and a network architecture called the H-score network, as shown in the figure, where we can learn the informative features defined in the modal decomposition by maximizing the H-score with a given dataset.</p> <p>In <a href="https://lizhongzheng.github.io/blog/2024/nested-H-score/">nested H-score network</a>, we introduced a nested architecture, see figure, to enforce orthogonality constraints in the learning process.</p> <p>The main point of this sequence of works is to introduce the key concepts and building blocks of <strong>feature-centric learning</strong>, where we view feature functions as the basic carrier of information and turn the learning objective from making a certain decision to finding feature functions that carry specific information. This general method makes the learning procedure more controllable and the learning results reusable. The H-score can be viewed as a quality metric for features. The nested H-score network can be viewed as a numerical method for the basic projection operation of features. Both are needed for further development.</p> <h2 id="a-wireless-communication-problem">A Wireless Communication Problem</h2> <table> <tbody> <tr> <td><img src="/assets/img/channel.png" alt="Interference Channel" width="300"/></td> </tr> <tr> <td><b> A Wireless Fading Interference Channel </b></td> </tr> </tbody> </table> <p><br/></p> <p>The problem we are working on is called “symbol detection in fading interference channel,” which is a classic problem in wireless communications. For a more complete reference, <a href="https://stanford.edu/~dntse/wireless_book.html">here</a> is a wonderful textbook. Roughly the idea is depicted in the figure: a transmitter tries to send a symbol \(\mathsf {x}_1\) to the receiver, but there is another transmitter, the interferer, which is transmitting his signal at the same time. Thus, the receiver receives a signal \(\mathsf y\) which is a mixture of both the desired signal \(\mathsf {x}_1\) and the interfering signal \(\mathsf {x}_2\), with some additive noise introduced by the receiver circuits. The goal is to design a receiver processing that can figure out the value of the desired signal \(\mathsf {x}_1\).</p> <p>Mathematically, the received signal can be written as</p> \[\mathsf{y} = h_1 \cdot \mathsf{x}_1 + h_2 \cdot \mathsf{x}_2 + \mathsf {w}\] <p>where</p> <ol> <li>Because all the symbols are transmitted at a carrier frequency, it is conventional to think of all the variables as complex-valued. Here, we have assumed the standard approach to isolate a single group of transmitted and received symbols. Thus, all the variables are complex scalars in \(\mathcal C\).</li> <li>\(h_1, h_2\) are called the fading coefficients for the two corresponding channels. These are usually random, depending on the position of the transmitter and the receiver, and the environment around them. If the transmitter moves, these fading coefficients can change over time, too. However, a wireless system usually has a separate procedure to estimate these coefficients. So, these coefficients are considered as known side information at the receiver, called the <strong>channel state information (CSI)</strong>.</li> <li>\(\mathsf w\) is complex Gaussian distributed, independent of all the other variables. This is usually referred to as the “additive Gaussian noise”. If there is no interference, the received signal can be described with a conditional distribution of \(\mathsf y\) given \(\mathsf {x}_1\), which is Gaussian distributed with a mean of \(h_1 x_1\). The decision of what symbol \(\mathsf{x}_1\) is transmitted is now a standard hypothesis testing problem, with Gaussian noise, for which the <a href="https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation">MAP decision</a> is simple and the optimal solution.</li> <li>What makes these problems interesting is that the transmitted signals \(\mathsf{x}_1\) and the signal transmitted by the interferer \(\mathsf{x}_2\) are both digital signals, in the sense that they are chosen from a discrete set of possible values on the complex plane, which is called a constellation. The most commonly used one is called the <a href="https://en.wikipedia.org/wiki/Quadrature_amplitude_modulation">QAM</a>.</li> </ol> <table> <tbody> <tr> <td><img src="/assets/img/Rectangular_constellation_for_QAM.svg.png" alt="test image" width="200"/></td> </tr> <tr> <td><b> Michel Bakni, CC BY-SA 4.0 <a href="https://creativecommons.org/licenses/by-sa/4.0">https://creativecommons.org/licenses/by-sa/4.0</a>, via Wikimedia Commons </b></td> </tr> </tbody> </table> <p><br/></p> <p>In this post, we will assume that \(\mathsf{x}_1\) is equally likely chosen from a binary alphabet \(\mathsf{x}_1 \in \{+1, -1\}\); and the interference symbol \(\mathsf{x}_2\) is chosen uniformly from a 16-QAM constellation, which, as shown in the figure, are the 16-point regular grid around the origin.</p> <p>This is obviously a special and simple case, but it is sufficient to make our points. We are interested only in detecting the value of \(\mathsf{x}_1\), which is a binary decision. However, because of the interference signal, our received signal \(\mathsf {y}\) is corrupted with non-Gaussian noise.</p> <p>To see how this works, consider an example shown in the following figure:</p> <table> <tbody> <tr> <td><img src="/assets/img/SymbolDetectionPlots/decision1.png" alt="test image" width="200"/></td> </tr> <tr> <td><b> A “good” case of interference </b></td> </tr> </tbody> </table> <p><br/></p> <p>Here, we use the color “red” for \(\mathsf{x}_1 = +1\), and “blue” for \(\mathsf{x}_1=-1\). When we transmit \(\mathsf{x}_1= +1\), the interferer transmits a randomly independently chosen point in his 16-QAM constellation. This causes the noiseless signal \(h_1 \mathsf{x}_1 + h_2 \mathsf{x}_2\) to be one of the \(16\) red dots in the figure. If we transmit \(\mathsf{x}_1 = -1\), this signal before adding noise is one of the \(16\) blue dots. The additive noise \(\mathsf {w}\) is then added to the chosen dot, which makes the received symbol \(\mathsf{y}\) to be somewhere around.</p> <p>Our goal is to observe the received symbol \(\mathsf{y}\) and decide whether it was a red or a blue dot transmitted. Mathematically, the decision is between two <strong>mixed Gaussian distributions</strong>. The case shown above is considered a “good” case with weak interference. That is, even with the interference, the red and the blue cases are rather separable. As long as the additive noise \(\mathsf {w}\) is not too large, one can simply draw a line to separate the red and the blue cases rather well. In fact, most of the current commercial wireless communication systems use linear decision functions. That is, a straight line is drawn in this figure to separate the red and the blue.</p> <p>Unfortunately, the configuration of these “dots” is determined by the fading coefficients \(h_1, h_2\), which we can observe but cannot control. There is a small probability that the fading can take values that cause an “undesirable” situation. A few such cases are shown below. If the interference is strong, the two groups of dots can be “interleaved,” making it rather difficult to separate them. Furthermore, depending on the value of the fading coefficients, these cases with strong interference can be rather different, as shown in the figure. That is, we cannot design a decision-maker for one case and use it for another case.</p> <table> <tbody> <tr> <td><img src="/assets/img/SymbolDetectionPlots/decision21.png" alt="test image" width="300" style="float:left; padding-right:50px"/></td> <td><img src="/assets/img/SymbolDetectionPlots/decision31.png" alt="test image" width="300" style="float:left; padding-right:50px"/></td> <td><img src="/assets/img/SymbolDetectionPlots/decision41.png" alt="test image" width="300" style="float:left; padding-right:50px"/></td> </tr> <tr> <td><b> A “marginal” case of interference </b></td> <td><b> A case with strong interference </b></td> <td><b> A different case with strong interference </b></td> </tr> </tbody> </table> <p><br/></p> <h3 id="why-is-this-difficult">Why is this difficult?</h3> <p>Analytically, the optimal decision using the likelihood ratio test (LRT) is not difficult to find. The likelihood function of both the red and the blue cases are simply mixture-Gaussian densities: the average of \(16\) equally weighted Gaussian densities. The log-likelihood ratio function can be rather non-linear. Examples are shown in the figure, respectively, corresponding to the \(3\) cases above. The color code represents the scalar value of this function: red for positive values, which means \(\mathsf{x}_1=+1\) is more likely, and blue for \(\mathsf{x}_1=-1\). A simple threshold test can cut out the decision regions.</p> <table> <tbody> <tr> <td><img src="/assets/img/SymbolDetectionPlots/decision23.png" alt="test image" width="300" style="float:left; padding-right:50px"/></td> <td><img src="/assets/img/SymbolDetectionPlots/decision33.png" alt="test image" width="300" style="float:left; padding-right:50px"/></td> <td><img src="/assets/img/SymbolDetectionPlots/decision43.png" alt="test image" width="300" style="float:left; padding-right:50px"/></td> </tr> <tr> <td><b> Case 1 </b></td> <td><b> Case 2 </b></td> <td><b> Case 3 </b></td> </tr> </tbody> </table> <p><br/></p> <p>In practice, however, this analytically optimal decision is rarely implemented for some practical reasons. One of them is the <strong>lack of non-linear processing circuits</strong> in conventional wireless communication devices. Neural networks, as a general-purpose non-linear processor, are now widely used in handheld wireless devices. This makes neural-network-based solutions to this problem an attractive possibility.</p> <p>The immediate difficulty in building a decision-maker with a neural network is the <strong>lack of training samples</strong>. In this application, we can have both <em>off-line training</em> and <em>on-line training</em>. Offline training samples can be simulated in the wireless environment based on the industry’s extensive channel measurement results. The problem is that there are infinitely many possible situations, as the fading coefficients \(h_1, h_2\) in our example, are continuous-valued. Thus, whichever specific environment or collection of environments we simulate, the online situation is inevitably different. Online training, on the other hand, is more targeted to the case of interest but is much more expensive. Every symbol we use for training is a symbol we cannot use to carry the communication payload. The wireless channel also changes quite fast over time as mobile users move, making online training results expire.</p> <p>Another difficulty in this problem is the <strong>high accuracy requirement</strong>. Symbol detections in wireless communication systems require the error rate to be around \(10^{-3}\). Situations like cases 2 and case 3 in both figures are said to have <em>strong interference</em>, which occurs with a probability of the order of \(10^{-2}\). This means a good receiver needs to do well in all the nice cases like case 1, but what really defines a good receiver is how it handles the strong interference cases. As a result, using the average performance as the objective to train the neural network is not a good idea: how a receiver acts in difficult cases may not count much in the average performance and do not have enough samples in a reasonable-sized training set.</p> <h3 id="why-is-this-a-common-problem">Why is this a common problem?</h3> <p>The reason that we like this symbol detection problem is because it is a very typical case for using neural networks in engineering problems. There is clearly a potential for remarkable performance gain, which can come from</p> <ol> <li>The use of neural networks in the place of specialized non-linear circuits;</li> <li>The training that allows the system to adapt to specific environments.</li> </ol> <p>The difficulties are also clear:</p> <ol> <li>the situations where we want to use the neural networks are almost always different from those we have in the training set;</li> <li>what we hope to get from the neural networks is not just the optimal solution in one specific case but a collection of optimal solutions controlled by parameters (in this example, the channel coefficients.)</li> <li>we also have knowledge of specific structures of the data (for example, the repetitive patterns in the above plots,) which we hope to “tell” the neural network so it does not require more samples to learn this known fact.</li> </ol> <p>At a higher level, the issue is that neural networks are often operated as black boxes. In engineering problems, there is often the need to <em>inject</em> external domain knowledge into the neural networks in order to control the learning procedure, impose restrictions on learning results, reuse learning results as the environment changes, and reduce the overall learning costs.</p> <p>The goal of this page is to develop a generic solution to this family of problems. Since the goal is to reach into the internal operations of neural networks, the H-score networks, which are based on the concept of <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">decomposition</a> of probability models, is a useful tool.</p> <h2 id="a-solution-using-nested-h-score-networks">A solution using nested H-score networks</h2> <p>We write the channel state information as one random variable \(\mathsf{s} = [h_1, h_2]^T\). We write the target random variable, i.e., the one we would like to make a decision on, as \(\mathsf {x} = \mathsf {x}_1 \in \{0, 1\}\). We write the observed variable as \(\mathsf{y}\), and include the randomness from the interfering signal \(\mathsf{x}_2\) in the conditional distribution \(P_{\mathsf{y\vert x, s}}\). With these notations, the probability law that is relevant to this problem is the 3-way dependence of \(\mathsf{x, y, s}\).</p> <p>In the literature of wireless communications, a standard way to work with such a multi-variate dependence is by using the <a href="https://en.wikipedia.org/wiki/Conditional_mutual_information">chain rule</a>.</p> \[I(\mathsf{y; (x, s)}) = I(\mathsf{y;s} ) + I(\mathsf{y;x|s}),\] <p>The quantity of interest is \(I(\mathsf{y;x\vert s})\), which is maximized by choosing the optimal distribution of \(\mathsf{x}\) and a corresponding coding scheme. The resulting maximum of this conditional mutual information is called the <a href="https://web.stanford.edu/~dntse/Chapters_PDF/Fundamentals_Wireless_Communication_chapter5.pdf">coherent capacity of the channel</a>.</p> <p>The point is we would like to separate the contribution of \(\mathsf{x}\) and that of \(\mathsf{s}\) in the three-way dependence. This is a problem we have just studied in the previous <a href="https://lizhongzheng.github.io/blog/2024/Side-Information/">post</a>. The key is to use a nested H-score network, as shown below, to make this separation computationally.</p> <table> <tbody> <tr> <td><img src="/assets/img/nn_side2.png" alt="test image" width="400"/></td> </tr> <tr> <td><b> Nested H-Score Network for Learning with Side Information </b></td> </tr> </tbody> </table> <p><br/> <br/></p> <h3 id="the-assembling-step">The assembling step</h3> <p>Different from the common procedure of training a neural network and use it in the same place, here, we need an extra assembling step to connect the trained feature function modules into the desired decision maker.</p> <p>After training the nested H-score network above, we have four modules \(f, g, \overline{f}, \overline{g}\), with the following meanings</p> \[\begin{align*} P_{\mathsf{y,s}} &amp;\approx P_{\mathsf{y}} \cdot P_{\mathsf{s}} \cdot \left(1 + \overline{f} \otimes \overline{g}\right)\\ P_{\mathsf{y,s,x}} &amp;\approx P_{\mathsf{y}} \cdot P_{\mathsf{s}} P_{\mathsf{x}} \cdot \left(1 + \overline{f} \otimes \overline{g} + f \otimes g\right) \end{align*}\] <p>We need a simple step of using the Bayes rule to get an approximated version of</p> \[P_{\mathsf {x \vert s,y}} = \frac{P_{\mathsf{y,s,x}}}{P_{\mathsf{y,s}}}\] <p>which can be used as the decision maker: use \(\mathsf{y}\) and \(\mathsf{s}\) as inputs, we can decide which value of \(\mathsf{x}\) is more likely.</p> <p>We emphasize here that the extra assembling step is the direct consequence of training not to learn a specific decision-maker, but to learn the useful features. This allows the learned feature modules to be evaluated and reused in different problems. This procedure is a key step to move away from task-specific learning and toward learning reusable information contents. It is also a key step in breaking the blackbox of end-to-end training.</p> <h3 id="blacktriangle-demo-pytorch-implementation">\(\blacktriangle\) Demo: Pytorch implementation</h3> <p><a href="https://colab.research.google.com/drive/18z_9zt7Ey_gqszPeHNbvjqvPQnuS3jYL?usp=sharing">Here</a> is a code for this experiment. The key performance result given in the following figure compares the current state-of-art decision after linear processing, the proposed solution based on the nested H-score network, and the theoretical optimal MAP decisions.</p> <table> <tbody> <tr> <td><img src="/assets/img/SymbolDetectionPlots/BER_plot.png" alt="test image" width="200"/></td> </tr> <tr> <td><b> Performance Comparison between the Current and the Proposed Solutions</b></td> </tr> </tbody> </table> <p>We have two disclaimers:</p> <ol> <li>The training and the performance evaluation of this experiment both take some time. The training can be done offline, which is basically free for communication systems. The performance evaluation part is left in the experiment. Readers are recommended to select a subset to run.</li> <li>The above curve is plotted by fixing the strengths of the signal and the interference: \(\vert h_1\vert \in [0.0 .. 8.0], \vert h_2\vert =1\) at fixed values. This is not common for communication engineers. A more commonly used approach would choose signal strengths to be random, following certain distribution e.g. Rayleigh Fading, and make the plot with average interference strength. This plot is also included in our code. We choose to this one as it reveals the non-linear nature of the problem and the fact that the proposed solution can follow the non-linear behavior of the theoretical optimal solution quite well.</li> </ol> <h2 id="the-lessons">The Lessons</h2> <p>We take a little time to reflect on the lessons we learned through this experiment.</p> <h3 id="good-engineering-solutions">Good engineering solutions</h3> <p>We are quite happy with the final result of this experiment. It shows that we can indeed use neural-network-based solutions in very mature engineering problems. The symbol detection problem is a well-understood and widely used problem in the industry, with well-defined performance measures and benchmarks. It is also known to require very precise processing and has a very limited online computation budget. The fact that our solution can outperform the existing solutions, which run on everybody’s cell phone, is quite impressive.</p> <p>The especially remarkable part is that by using the nested H-score network, our solution is optimal not for a single scenario but a parameterized sequence of scenarios, with our choice of the parameters as the channel state information. Our solution also shifted the computation requirement: we used extensive offline training, allowing us to have zero online retraining or re-adaptation beyond simply setting the CSI parameter to indicate which scenario we are in. These are all desired/required by the target application.</p> <h3 id="the-separation-feature-learning-and-assembling">The separation: feature learning and assembling</h3> <p>A key conceptual change we propose with this sequence of blogs and experiments is the shift from learning for a specific inference task to learning information-carrying feature functions. This concept is analogous to the separation of source and channel coding in information theory. We believe this is a critical step to make training large neural networks, which consume vast amounts of data and computation, contribute to learning reusable knowledge.</p> <p>There are two key technical components in such a conceptual shift. First, we can no longer rely on task-specific performance metrics to train the neural networks. Instead, a metric directly evaluating the information contents, on its quality and relevance, is needed in the training process. The geometric concept and the H-score in our works are for this purpose. Second, after training, there needs to be a separate assembling stage, where feature functions from different sources are put together to form a decision-maker. We also see this process in this experiment.</p> <h2 id="going-forward-what-is-a-white-box-solution-">Going Forward: What is a “White Box Solution” ?</h2> <p>There are many discussions about “applying AI to specific domains” or “verticals.” These are, of course, correct visions for the future development that we embrace. The question is, how do we do that? How do we turn a black box solution into a white box solution? What exactly is interpretable learning? Does it count if we just add some metrics, maybe an information-theoretic metric, as a regulator in the training? This sequence of blogs tries to answer these questions by examples.</p> <p>First, conceptually, we believe that fundamentally new information metrics are needed. In these learning problems, information carriers are no longer bits, and the information carried by a feature function should not only be measured by “how much information there is” but also by “what the information is about?”. A vector description of the information is thus needed. This concept is a fundamental extension of the classical information theory. We show in this series that such a concept leads to the definition of the H-score and a collection of new operations.</p> <p>Second, at an operational level, we should expect concrete operations when we open a black box. We summarize them into the following three capabilities, which we call the “SET” capabilities.</p> <ul> <li> <p><strong>Separable:</strong> When we train a neural network for a complex task, we should be able to extract its answer to a simple sub-task and know which part of the network is responsible for that answer. For example, if we can recognize a person from an image, we should know his/her gender or hair color; if we train a neural network to detect symbols from the received signal, then somewhere in the neural network, we must have an estimate of the channel state.</p> </li> <li> <p><strong>Exchangeable:</strong> We should be able to replace the answer of a large neural network to an element question with a different answer and still run the rest of the system, as a way to control the behavior of the overall system. We have seen such experiments as changing the gender of the object in an image. Our experiment changing the channel state information as a parameter is another example.</p> </li> <li> <p><strong>Transferrable:</strong> Here, we do not mean to train a large network with one dataset and then directly use it on a different problem to see how it goes. Instead, we would like to take only the necessary elements of the learned results and use them as components in the solution for a different task. For example, we can use the trained modules in our experiments in a channel estimation task.</p> </li> </ul> <p>The point of these SET capabilities is to define a clear set of goals of tangible performance improvements based on better interpretability of neural networks. With this, the way forward is simply to generate more examples with such capabilities.</p> <p><br/></p> <hr/> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/">Dr. Xiagxiang Xu</a>.</p>]]></content><author><name>Lizhong Zheng</name></author><category term="ML-Application"/><category term="H-Score"/><category term="wireless-communication"/><summary type="html"><![CDATA[The Key Points In our previous developments, we introduced the H-score network, which is a way to learn informative features from datasets, similar to a normal neural network but was claimed to be more flexible. In this post, we use a concrete example of wireless communications to demonstrate this flexibility. In a nutshell, the advantage of our solution comes from changing the objective from learning to solve a specific inference task to learning feature functions that carry information that can be reused and combined with other sources of information. We demonstrate that this approach can help us to design neural-network-based solutions not to a single case but a parameterized set of cases, as required by this specific engineering problem. We use this example to discuss the general methodology of integrating learning modules into engineering solutions.]]></summary></entry><entry><title type="html">The Nested H-Score Network</title><link href="https://lizhongzheng.github.io/blog/2024/nested-H-score/" rel="alternate" type="text/html" title="The Nested H-Score Network"/><published>2024-06-30T00:00:00+00:00</published><updated>2024-06-30T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/nested-H-score</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/nested-H-score/"><![CDATA[<blockquote> <h3 id="the-key-points">The Key Points</h3> <p>In our previous posts, we started to develop a geometric view of the feature extraction problem. We started with the geometry of functional spaces by defining inner products and distances and then relating the task of finding information-carrying features to finding approximations of functions in this space. Based on this approach, we proposed the use of the H-Score networks as one method to learn informative feature functions from data using neural networks. In this post, we describe a new architecture, the <strong>nested H-Score network</strong>, which is used to make projections in the functional space with neural networks. Projections are perhaps the most fundamental geometric operations, which are now made possible and efficient through the training of neural networks. We will show some examples of how to use this method to regulate the feature functions, incorporate external knowledge, and prioritize or separate information sources, which are the critical steps toward multi-variate and distributed learning problems.</p> </blockquote> <table> <tbody> <tr> <td><img src="/assets/img/Hscorenetwork.png" alt="test image" width="250"/></td> </tr> <tr> <td><b> H-Score Network </b></td> </tr> </tbody> </table> <p><br/></p> <h2 id="previously">Previously</h2> <p>In <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">this page</a>, we defined H-Score network as shown in the figure, where the two sub-networks are used to generate features of \(\mathsf x\) and \(\mathsf y\): \(\underline{f} \in \mathcal {F_X}^k, \underline{g}\in \mathcal {F_Y}^k\). The two features are used together to evaluate a metric, the H-score,</p> \[\mathscr{H}(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])\] <p>where the covariance and the expectation are evaluated by the empirical averages over the batch of samples. We use back-propagation to train the two networks to find</p> \[\underline{f}^\ast, \underline{g}^\ast = \arg\max_{\underline{f}, \underline{g}} \; \mathscr{H}(\underline{f}, \underline{g}).\] <p>The resulting optimal choice \(\underline{f}^\ast, \underline{g}^\ast\) is promised to be a good set of feature functions. They are the solutions of the <strong>approximated</strong> and <strong>unconstrained</strong> versions of the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a> problem, which has a connection to a number of theoretical problems, and, in a short sentence, picks “informative” features.</p> <p>The H-Score network is different from the modal decomposition problem in the following ways:</p> <ol> <li><strong>Approximation</strong>: <ul> <li>Neural networks can have limited expressive power and imperfect convergence;</li> <li>The use of finite sample batches introduces randomness in the empirical averages and generalization error;</li> <li>We use a <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/#properties-of-modal-decomposition">local approximation</a> when formulating the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a>. For the rest of this page, we will take the local approximation for granted. For example, we will not distinguish between the function \(\mathrm{PMI}_{\mathsf {xy}} = \log \frac{P_{\mathsf {xy}}}{P_\mathsf x\cdot P_\mathsf y} \; \in \; \mathcal {F_{X\times Y}}\) and its approximation \(\widetilde{\mathrm{PMI}}_{\mathsf {xy}} = \frac{P_{\mathsf {xy}} - P_{\mathsf x}\cdot P_{\mathsf y}}{P_\mathsf x\cdot P_{\mathsf y}} \; \in \; \mathcal {F_{X\times Y}}\). $$</li> </ul> </li> <li><strong>Constraints</strong>: in the formulation of modal decomposition, we restricted the feature functions to satisfy a set of constraints that the features are all normalized, orthogonal to each other, and that they are organized in descending order in correlation coefficients. A detailed discussion can be found <a href="http://lizhongzheng.github.io/blog/2024/H-Score/#low-rank-approximation-of-probabilistic-models">here</a>.</li> </ol> <p><br/></p> <p>The goal of this post is to develop a new architecture to put these missing constraints back in the learning process. By doing that, we also introduce a systematic way to inject controls into the learning with H-score networks.</p> <h2 id="nested-h-score-network">Nested H-Score Network</h2> <p>We start by consider the following problem to extract a single mode from a given model \(P_{\mathsf {xy}}\), but with a simple constraint: for a given function \(\bar{f}: \mathcal X \to \mathbb R\), we would like to find a mode, i.e. a pair of features \(f(\cdot), g(\cdot)\), to be the optimal rank-$1$ approximation as before, but under the constraint that \(f \perp \bar{f}\), i.e. \(\mathbb E_{\mathsf x \sim P_\mathsf x}[f(\mathsf x) \cdot \bar{f}(\mathsf x)] = 0\):</p> \[(f^\ast, g^\ast) = \arg\min_{\small{\begin{array}{l}(f, g): f\in \mathcal {F_X}, g \in \mathcal {F_Y}, \\ \quad \mathbb E[f(\mathsf x)\cdot \bar{f}(\mathsf x)]=0\end{array}}} \; \left\Vert \mathrm{PMI} - f \otimes g \right\Vert^2\] <p>where \(\mathrm{PMI}\) is the point-wise mutual information function with \(\mathrm{PMI} (x,y) = \log \frac{P_{\mathsf {xy}}(x,y)}{ P_\mathsf x(x)P_\mathsf y(y)}\), for \(x\in \mathcal X, y \in \mathcal Y\).</p> <table> <tbody> <tr> <td><img src="/assets/img/nested H2.png" alt="test image" width="450"/></td> </tr> <tr> <td><b> Nested H-Score Network to find features orthogonal to a given $\bar{f}$ </b></td> </tr> </tbody> </table> <p><br/></p> <p>While there are many different ways to solve optimization problems with constraints, the figure shows the <strong>nested H-Score network</strong> we use, which is based on simultaneously training a few connected neural networks.</p> <p>In the figure, the blue box \(\bar{f}\) is the given function that the chosen feature function needs to be orthogonal to. The three red sub-networks are used to generate \(1\)-dimensional feature functions \(\bar{g}, f\) and \(g\). (As we will see soon, that \(f\) and \(g\) can, in fact, be higher dimensional, hence drawn slightly bigger in the figure.)</p> <p>With a batch of samples \((x_i, y_i), i=1, \ldots, n\), the network is trained with the following procedure.</p> <blockquote> <p><strong>Training for Nested H-Score Networks</strong></p> <ol> <li>Each sample pair \((x_i, y_i)\) is used to compute \(\bar{f}(x_i)\), and forward pass through the neural networks to evaluate \(f(x_i), \bar{g}(y_i), g(y_i)\);</li> <li>The output \(\bar{f}(\mathsf x_i), \bar{g}(\mathsf y_i), i=1, \ldots, n\) are used together to evaluate the H-score for \(1\)-D feature pairs shown on the top box; the two pairs of features \([\bar{f}, f]\) and \([\bar{g}, g]\) are used to evaluate a 2-D H-score in the lower box. In both cases, the expectations are replaced by the empirical means over the batch;</li> <li>The gradients to maximize the sum of the two H-scores are back-propagated through the networks to update weights. Because of the separation of the neural network modules, the sum of the gradients from the two H-scores is used to train \(\bar{g}\); only the gradients from the bottom H-score are used to train \(f\) and \(g\) networks.</li> <li>Iterate the above procedure until convergence.</li> </ol> </blockquote> <p>To see how this achieves the goal, it might be easier to look at a slightly different way to train the network, namely the <strong>sequential training</strong>. Here, we first only use the top H-Score to train the feature function \(\bar{g}\). From the definition of the <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-Score</a>, we know this finds \(\bar{g}^\ast\) by solving the following optimization.</p> \[\begin{align} \bar{g}^\ast = \arg \min_{\bar{g}} \; \left\Vert\mathrm{PMI} - \bar{f} \otimes \bar{g}\right\Vert^2 \end{align}\] <p>After that, we freeze \(\bar{g}^\ast\), and use the bottom H-Score to train both \(f\) and \(g\). With this choice of \(\bar{g}^\ast\), we have that the minimum error, \(\mathrm{PMI} - \bar{f} \otimes \bar{g}^\ast\) must be orthogonal to \(\bar{f}\) in the sense that for every \(y\), \(\mathrm{PMI}(\cdot, y)\) as a function over \(\mathcal X\) is orthogonal to \(\bar{f}\), since otherwise the L2 error can be further reduced. The maximization of the bottom H-Score is now the following optimization problem:</p> \[\begin{align} (f^\ast, g^\ast) = \arg\min_{f, g} \; \left\Vert \mathrm{PMI} - (\bar{f}\otimes \bar{g}^\ast + f\otimes g) \right\Vert^2 \end{align}\] <p>This can be read as the rank-\(1\) approximation to \((\mathrm{PMI}- \bar{f}\otimes \bar{g}^\ast)\), which is orthogonal to \(\bar{f}\). So the resulting optimal choice of \(f^\ast\) must also be orthogonal to \(\bar{f}\) as we hoped.</p> <p>A few remarks are in order now.</p> <ol> <li> <p>One should check that at this point if we freeze the choice \(f^\ast, g^\ast\) and allow \(\bar{g}\) to update, \(\bar{g}^\ast\) defined in (1) actually maximizes both H-Scores. Thus if we turn the sequential training into iteratively optimized \(\bar{g}\) and \(f, g\), we get the same results.</p> </li> <li> <p>In practice, we would not wait for the convergence of one step before starting the next, and thus the proposed training procedure is what we call <strong>simultaneous training</strong>, where all neural networks are updated simultaneously. It can be shown that in this setup, the results are the same as those from the sequential training. However, in our later examples of using the nested H-Score architectures, often with subtle variations, we need to discuss in each case whether this still holds.</p> </li> <li> <p>There are, of course, other ways to make sure the learned feature function is orthogonal to the given \(\bar{f}\). For example, one could directly project the learned feature function against \(\bar{f}\). Here, the projection operation is separated from the training of the neural network, raising the issue that the objective function used in training may not be perfectly aligned with that in the space after the projection. In nested H-Score networks, the orthogonality constraint is naturally included in the training of the neural networks. There is also some additional flexibility with this design. For example, in several follow-up cases using nested H-Score, we would learn \(\bar{f}\) from data at the same time.</p> </li> </ol> <p><br/></p> <h2 id="example-ordered-modal-decomposition">Example: Ordered Modal Decomposition</h2> <p>We now go back to the problem that motivated this study: we would like to use the nested H-Score network to solve the modal decomposition problem. That is, we want to find \(k\) feature pairs that not only form a good low-rank approximation to the model but hope the extracted feature functions to be orthonormal and the modes to be in descending order of strengths.</p> <p>We would like to emphasize the importance of this step: we are deviating from the standard operation of neural networks, which generate feature functions that are in the form of an arbitrary linear combination of the standard orthonormal and ordered modes. This is one key reason that the learning results of neural networks have little hope of allowing any interpretation. Sorting out the features in a standard form is an important starting point if we want to control, measure, and reuse the learned features.</p> <p>As we will use the nested structure repeatedly, to avoid having too many lines in our figures, we will adopt a new <strong>concatenation</strong> symbol, “\(\,+\hspace{-.7em}+\,\)”, where simply takes all the inputs to form a vector output. In some drawings, such an operation to merge data is simply denoted by a dot in the graph, but here, we use a special symbol to emphasize the change of dimensionality. For example, the concatenation operation in the nest H-Score network is replaced with a new figure as follows.</p> <table> <tbody> <tr> <td><img src="/assets/img/concatenation.png" alt="test image" width="250"/></td> </tr> <tr> <td><b> The Concatenation Symbol </b></td> </tr> </tbody> </table> <p><br/></p> <p>Now the nested H-Score network that would generate orthogonal modes in descending order is as follows.</p> <table> <tbody> <tr> <td><img src="/assets/img/h_nest.png" alt="test image" width="500"/></td> </tr> <tr> <td><b> Nested H-Score Network for Ordered Modal Decomposition </b></td> </tr> </tbody> </table> <p><br/></p> <p>In the figure, we used the notation \(f_{[k]} = [f_1, \ldots, f_k]\). Again, it is easier to understand the operation from sequential training. We can first train the \(f_1, g_1\) block with the top H-Score box, and this finds the first mode \(f^\ast_1, g^\ast_1 = \zeta_1(P_{\mathsf {xy}})\). After that, we train \(f_2, g_2\) with the first mode frozen. The nested network ensures that the resulting mode is orthogonal to the first mode, which by definition is the second mode \(\zeta_2(P_{\mathsf {xy}})\). Following this sequence, we can get the \(k^{th}\) mode that is orthogonal to all the previous \(k-1\) ones. It takes proof to state that we can indeed simultaneously train all sub-networks, which we omit from this page.</p> <h3 id="blacktriangle-pytorch-implementations">\(\blacktriangle\) Pytorch Implementations</h3> <p>The <a href="https://colab.research.google.com/drive/1C9mdtDZ7GFvyiYxEboemJ3Ed18sUMkVB?usp=sharing">first Colab demo</a> illustrates how to implement a nested H-score network in PytTorch, where we compare the extracted modes with the theoretical results.</p> <p>As in the previous post, we can also apply the nested H-score on sequential data. In <a href="https://colab.research.google.com/drive/1JtjS1LfWpf0eWx3xWTKYlW1myCTE4vqb#scrollTo=mcfGopzulQsY">the second demo</a>, we compare the nested H-score with the vanilla H-score, which also demonstrates the impact of feature dimension.</p> <h2 id="going-forward">Going Forward</h2> <p>Nested H-Score Networks are our way to make projections in the space of feature functions using interconnected neural networks. This is a fundamental operation in the functional space. In fact, in many learning problems, especially when the problem is more complex, such as with multi-modal data, multiple tasks, distributed learning constraints, time-varying models, privacy/security/fairness requirements, or when there is external knowledge that needs to be incorporated in the learning, such projection operations become critical. In our next post, we will give one such example with a multi-terminal learning problem.</p> <p><br/></p> <hr/> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/">Dr. Xiagxiang Xu</a>.</p>]]></content><author><name>Lizhong Zheng</name></author><category term="ML-Theory"/><category term="H-Score"/><category term="modal-decomposition"/><summary type="html"><![CDATA[The Key Points In our previous posts, we started to develop a geometric view of the feature extraction problem. We started with the geometry of functional spaces by defining inner products and distances and then relating the task of finding information-carrying features to finding approximations of functions in this space. Based on this approach, we proposed the use of the H-Score networks as one method to learn informative feature functions from data using neural networks. In this post, we describe a new architecture, the nested H-Score network, which is used to make projections in the functional space with neural networks. Projections are perhaps the most fundamental geometric operations, which are now made possible and efficient through the training of neural networks. We will show some examples of how to use this method to regulate the feature functions, incorporate external knowledge, and prioritize or separate information sources, which are the critical steps toward multi-variate and distributed learning problems.]]></summary></entry><entry><title type="html">The H Score</title><link href="https://lizhongzheng.github.io/blog/2024/H-Score/" rel="alternate" type="text/html" title="The H Score"/><published>2024-06-24T00:00:00+00:00</published><updated>2024-06-24T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/H-Score</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/H-Score/"><![CDATA[<blockquote> <h3 id="the-key-points">The Key Points</h3> <p>H-score is our first step in introducing neural-network-based methods to compute <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a> from data. It is a loss metric used in the training of neural networks in a specific configuration, which we call the “H-Score Network.” The result is approximately equivalent to a conventional end-to-end neural network with cross-entropy loss, with the additional benefit of allowing direct control of the chosen feature functions. This is an important conceptual step to turn the goal of learning from learning a probability model to learning feature functions. In practice, H-score networks have a number of natural extensions that allow more flexible operations to incorporate external knowledge.</p> </blockquote> <p><br/></p> <h2 id="the-h-score-network">The H-score Network</h2> <h3 id="low-rank-approximation-of-probabilistic-models">Low-Rank Approximation of Probabilistic Models</h3> <p>The motivation of <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a> is to find the rank-\(k\) approximation</p> \[\begin{equation} \{(\sigma_i^\ast, f^\ast_i, g^\ast_i), i=1, \ldots, k\} =\arg\min_{\substack{(\sigma_i \in \mathbb{R}, f_i \in \mathcal {F_X}, g_i \in \mathcal {F_Y}),\\ i=1, \ldots, k}} \; \left\Vert \mathrm{PMI}- \sigma_i\cdot \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2 \end{equation}\] <p><strong>The Constraints:</strong> The above problem is slightly different from our original definition of modal decomposition, where we had a sequential way to find the \(k\) modes. This sequential procedure solves the optimization problem (1). It also ensures the solution to satisfy a number of constraints:</p> <ol> <li>the feature functions all have zero mean and unit variance;</li> <li>the scaling factor of the \(i^{th}\) mode, $\sigma_i$, is separated from the normalized features;</li> <li>the feature functions are orthogonal: \(\mathbb E[f^\ast_i f^\ast_j] = \mathbb E[g^\ast_i g^\ast_j] = \delta_{ij}\),</li> <li>there is a descending order in \(\sigma_i\), the correlation between \(f^\ast_i\) and \(g^\ast_i\).</li> </ol> <p>We refer to the optimal choice of \(\{(\sigma^\ast_i, f^\ast_i g^\ast_i), i = 1, 2, \ldots,k\}\) that satisfies these constraints, or obtained with the sequential procedure, as the <strong>modes</strong> of the given dependence model.</p> <p>One can also run an optimization (1) without any of these constraints. The result is also a set of feature function pairs, one of many equally good low-rank approximations to the PMI function, which can be obtained as linear combinations of the ordered orthonormal feature functions in modal decomposition.</p> <p>It turns out that this later unordered low-rank approximation is more closely related to the operation of commonly used <strong>neural networks</strong>, which will be discussed later in this page.</p> <p><br/></p> <h3 id="h-score-the-definition">H-Score: the Definition</h3> <p>In this section, we ignore the constraints in modal decomposition and work only on the objective function. For convenience, we introduce a vector notation: we write column vectors \(\underline{f} = [f_1(\mathsf x), \ldots, f_k(\mathsf x)]^T\), \(\underline{g} = [g_1(\mathsf y), \ldots, g_k(\mathsf y)]^T\). Since we do not restrict the feature functions to be normalized, we do not need the scaling factors $\sigma_i$’s. Now, we use a short-hand notation.</p> \[\sum_{i=1}^k f_i(x)\cdot g_i(y) = \underline{f} \otimes \underline{g}\] <p>and the optimization (1) becomes a simple form:</p> \[\underline{f}^\ast, \underline{g}^\ast = \arg\min_{\substack{\underline{f} \in \mathcal{F_X}^k\\ \underline{g} \in \mathcal{F_Y}^k}} \; \left\Vert \mathrm{PMI} - \underline{f} \otimes \underline{g}\right\Vert^2\] <p>This objective function is still not easy to use since to evaluate it, we need to know \(\mathrm{PMI}\), which is equivalent to the probability model \(P_{\mathsf{XY}}\) we need to learn. The following is a nice trick: for a given model \(\mathrm{PMI}\), the above optimization is equivalent to</p> \[\underline{f}^\ast, \underline{g}^\ast = \arg\max_{\substack{\underline{f} \in \mathcal{F_X}^k\\ \underline{g} \in \mathcal{F_Y}^k}} \; \left\Vert \mathrm{PMI}\right \Vert^2 - \left\Vert \mathrm{PMI} - \underline{f} \otimes \underline{g}\right\Vert^2\] <details><summary>Now, with a few steps of algebra hidden below, it is not hard to check that this objective function can be approximated in a computable form</summary> <p>Consider the above objective function:</p> \[\begin{align} &amp;\left\Vert \mathrm{PMI}\right \Vert^2 - \left\Vert \mathrm{PMI} - \underline{f} \otimes \underline{g}\right\Vert^2\nonumber\\ &amp;= 2 \left\langle {\mathrm{PMI}}, \underline{f} \otimes \underline{g} \right\rangle - \left\Vert \underline{f} \otimes \underline{g} \right\Vert^2\nonumber\\ &amp;= 2 \sum_{x,y} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \mathrm{PMI}(x,y) \cdot \left(\sum_{i=1}^k f_i (x) g_i(y) \right) \nonumber\\ &amp; \qquad -\sum_{xy} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \left(\sum_{i=1}^k f_i (x) g_i(y)\right)^2\\ &amp;\approx 2 \sum_{x,y} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \left(\frac{P_{\mathsf{xy}}(x,y) - P_{\mathsf{x}}(x)P_{\mathsf{y}}(y)}{P_{\mathsf{x}}(x)P_{\mathsf{y}}(y)}\right) \cdot \left(\sum_{i=1}^k f_i (x) g_i(y) \right) \nonumber\\ &amp; \qquad -\sum_{xy} P_{\mathsf x}(x) P_{\mathsf y}(y) \cdot \left(\sum_{i=1}^k f_i (x) g_i(y)\right)^2\\ &amp;= 2 \sum_{i=1}^k \left(\mathbb E_{\mathsf {x,y} \sim P_{\mathsf {xy}}}\left[ f_i(\mathsf x) g_i(\mathsf y)\right] - \mathbb E_{\mathsf x\sim P_\mathsf x}[f_i(\mathsf x)]\cdot \mathbb E_{\mathsf y\sim P_{\mathsf y}}[g_i(\mathsf y)]\right) \nonumber\\ &amp;\qquad \qquad - \sum_{i=1}^k\sum_{j=1}^k \mathbb E_{\mathsf x\sim P_{\mathsf x}}[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E_{\mathsf y\sim P_\mathsf y}[g_i(\mathsf y)g_j(\mathsf y)] \nonumber \end{align}\] <p>where in (2) we evaluated the inner product and norm with respect to the reference distribution \(R_{\mathsf{xy}} = P_{\mathsf{x}} P_{\mathsf{y}}\), and in (3) we used the local approximation\(\mathrm{PMI} \approx \frac{P_{\mathsf{xy}}}{P_{\mathsf{x}}P_{\mathsf{y}}} -1\).</p> </details> <p>This resulting objective function, with a scaling factor of \(1/2\), is what we call the <strong><em>H-score</em></strong>.</p> <p><br/></p> <hr/> <dl> <dt>Definition: H-score</dt> <dd>The H-score for \(k\) pairs of feature functions \(\underline{f} \in \mathcal {F_X}^k, \underline{g} \in \mathcal {F_Y}^k\) is \(\mathscr{H}(\underline{f}, \underline{g}) \in \mathbb {R}^+\):</dd> </dl> \[\begin{align*} \mathscr{H}(\underline{f}, \underline{g}) &amp;\stackrel{\Delta}{=} \sum_{i=1}^k \mathrm{cov}[ f_i(\mathsf x) g_i(\mathsf y)] - \frac{1}{2} \sum_{ij} \mathbb E[f_i(\mathsf x)f_j(\mathsf x)] \cdot \mathbb E[g_i(\mathsf y)g_j(\mathsf y)]\\ &amp;=\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T]) \end{align*}\] <hr/> <p><br/></p> <p><strong>Remarks</strong></p> <ol> <li><strong>Empirical Average:</strong> It is important that the H-score is defined with expectation terms. In the definition, all expectations are taken with respect to $P_{\mathsf {xy}}$. This is the model that we need to learn in most problems, so it is not often available. In such cases, it is natural to replace the expectation with the empirical average over a dataset. This is the first advantage of the H-score: it is in a “data-friendly” form.</li> <li> <p><strong>H-score Network:</strong> Now, to compute the modal decomposition from data, we can use the following approach. Suppose \((x_i, y_i), i=1, \ldots, N\) are i.i.d. samples from an unknown model \(P_{\mathsf {xy}}\). We can use two separate neural network modules, with input \(x\) and \(y\) respectively, to generate \(k\) dimensional features \(\underline{f}(\mathsf{x})\) and \(\underline{g}(\mathsf {y}\). Each module can have its own network architecture of choice. We can then feed the given set of samples to the network to compute the two sets of features and the empirical average version of the H-score. Then, we can use backpropagation to adjust the network parameters to maximize the H-score. Here is a picture to illustrate this network in contrast to a common forward network.</p> </li> <li><strong>Local Approximation:</strong> The local approximation is required in the derivation of the H-score. In other words, it is required when we want to justify the maximization of the H-score as solving (approximately) the modal decomposition problem and, hence, picking the informative features. We will, for the rest of this post and all follow-up posts that use the H-score, adopt this approximation. One particular consequence is that we no longer distinguish between the \(\mathrm{PMI}\) function and the approximated version \(\widetilde{\mathrm{PMI}}= \frac{P_{\mathsf{xy}}}{P_{\mathsf{x}} P_{\mathsf {y}}}-1\).</li> <li> <p><strong>Constraints:</strong> The H-score is just a computable version of the objective function in the optimization problem (1). Thus, by design, the H-score network can compute a “mixed” rank \(k\) approximation of the model: it does not find the ordered orthonormal features as defined in <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a>, but only an arbitrary linear combination of the top \(k\) feature functions, with some minor caveat.</p> <p>a. One can observe that the features that maximize the H-score must be zero mean. This is because the first term in the definition does not depend on the means \(\mathbb E[\underline{f}]\) and \(\mathbb E[\underline{g}]\); and the 2nd term is optimized if all features have zero-mean, \(\mathbb E[\underline{f} \cdot \underline{f}^T]\) and \(\mathbb E[\underline{g} \cdot \underline{g}^T]\) becomes \(\mathrm {cov}[\underline{f}]\) and \(\mathrm{cov}[\underline{g}]\).</p> <p>b. Although the H-score does not force the optimizing feature functions into a more restrictive or desirable form, it is indeed not difficult to add such constraints back in. This is another major advantage of using the H-score networks, which we will discuss in the rest of this page and with examples in the next few posts.</p> </li> </ol> <table> <thead> <tr> <th style="text-align: center"><img src="/assets/img/autocoder.png" alt="test image" width="350" style="float:left; padding-right:30px"/></th> <th><img src="/assets/img/Hscorenetwork.png" alt="test image" width="250"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><b> Conventional Neural Network </b></td> <td><b> H-Score Network </b></td> </tr> </tbody> </table> <p><br/></p> <h4 id="blacktriangle-demo-1-h-score-network">\(\blacktriangle\) Demo 1: H-score Network</h4> <p>Here is a <a href="https://colab.research.google.com/drive/1unwIT5Y23_2owWVlFuRvIL4yCtbhuQOo?usp=sharing">colab demo</a> of how to implement an H-score network to learn the first mode, where the learned features are compared with theoretical results.</p> <p><br/></p> <h2 id="h-score-network-conventional-neural-network-and-svd">H-score Network, Conventional Neural Network, and SVD</h2> <p>At this point, we have made connections between three different problems:</p> <ol> <li>the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a> problem, which is equivalent as a SVD of the \(\mathrm{PMI}\) function,</li> <li>the conventional neural networks, especially when we use the <a href="https://en.wikipedia.org/wiki/Cross-entropy">cross entropy loss</a>) to train a classifier,</li> <li>the H-score network.</li> </ol> <p>With the local approximation, all three problems have the same or equivalent objective function. The SVD requires ordered orthonormal features, and the other two do not. In exchange, both neural-network-based options are more suitable for working with data than directly with the probability model.</p> <p>Compared with the normal neural networks, the H-score network method has a number of conceptual and practical advantages, which we will discuss now.</p> <h3 id="feature-space-operations">Feature Space Operations</h3> <p>Conceptually, the H-score network is a more appealing setup as it turns the objective of the system from learning a probability model to learning feature functions. For a specific inference task, we sometimes do not have a clear way to choose the metric to measure the quality of the decisions. There are many options for the loss metric to choose from, such as the cross entropy loss and the MSE loss with various regulators. When training a neural network, we are using task-specific metrics to encourage the chosen features to carry the “right” information.</p> <p>In contrast, the H-score directly measures the quality of the information carried by the features. This helps to separate the learning of information features and how to use these features, or the information they carry, to make decisions.</p> <p>One of the benefits of this <strong>separation</strong> is that the feature functions can be reused for other purposes. An immediate example is that after learning the \(\underline{f}^\ast, \underline{g}^*\) above, we can assemble these features in both</p> \[\begin{align*} {P}_{\mathsf{y|x}} \approx P_{\mathsf{y}} \cdot \left(1 + \underline{f}^\ast \otimes \underline{g}^\ast\right) \qquad \mbox{ and } \qquad P_{\mathsf{x|y}} \approx P_{\mathsf{x}} \cdot \left(1 + \underline{f}^\ast \otimes \underline{g}^\ast\right) \end{align*}\] <p>as estimators in both directions.</p> <p>It is worth mentioning that this shift from solving specific inference tasks to learning informative features is a widely accepted view in the literature under the names of “semantic information” or “representation learning,” etc.</p> <h3 id="weak-dependence-and-large-alphabets">Weak Dependence and Large Alphabets</h3> <p>Another reason for the importance of the above separation is that in some applications, the reconstruction quality is not a meaningful metric and thus cannot be used to encourage the learning of informative features.</p> <p>A rather common situation is when the alphabet \(\mathcal{X}, \mathcal{Y}\) are both quite large, yet the dependence between \(\mathsf{x}\) and \(\mathsf{y}\) is rather weak. Put this in an example, consider two episodes of the “Star Wars” movies: the common theme between the two movies is obvious, but it would be pointless if we want to reconstruct one entire movie from the other one. Thus, it would be difficult if we use one movie as the input \(\mathsf{x}\) of a neural network and try to predict the other movie as the output \(\mathsf{y}\). On the other hand, training an H-score network to find correlated features of the two movies can find meaningful results.</p> <p>We use the following experiment to demonstrate this point. Of course, we do not work with movies, but only synthetic time-sequences for the experiments.</p> <h4 id="blacktriangle-demo-2-h-score-for-sequential-data">\(\blacktriangle\) Demo 2: H-score for Sequential Data</h4> <p>Here is a <a href="https://colab.research.google.com/drive/1xHvEg1CsBMYA60bNqzviB41SIN_UXwDw#scrollTo=MZso0nK37s6m">colab demo</a> to demonstrate how we apply H-score network to learn the dependence structure among high-dimensional data. Again, we generate the dataset from given probability laws to help us analyze the trained results and compare them with theoretical optimal values. However, unlike previous demos where \(\sf x\) and \(\sf y\) are categorical, here \(\sf x\) and \(\sf y\) are both sequences, and the cardinalities \(\vert\mathcal{X}\vert\), \(\vert\mathcal{Y}\vert\) are way larger than the sample size.</p> <h3 id="flexible-control-of-feature-functions">Flexible Control of Feature Functions</h3> <p>The most significant advantage of the H-score network is it allows flexible control of the learning of feature functions. We will use a few posts to discuss various ways of taking this advantage to solve more involved learning problems. At a glance, the obvious reason for this flexibility is that the feature functions are separated into individual modules in H-sore networks. This makes it easy to change individual feature functions according to the specific situation. In extreme cases, we might know one of the feature functions from domain knowledge and thus do not have to learn that function. It turns out that this flexibility opens up an entire line of work on <strong>feature space operations</strong>. Since the feature functions are the new information carriers, these operations will become the new “atomic operations” used in a wide range of problems.</p> <h2 id="going-forward">Going Forward</h2> <p>H-score networks are our first step in processing based on modal decomposition. It offers some moderate benefits and convenience in the standard <em>bi-variate</em> “data-label” problems. To us, it is more of a conceptual step, where our focus of designing neural networks is no longer to predict the labels but rather shifted to designing the feature functions since our loss metric is now about the feature functions. In a way, this is better aligned with our goals since these features are indeed the carriers of our knowledge, and we would often need to store, exchange, and even use these features for multiple purposes in the more complex <em>multi-variate</em> problems.</p> <p>In our next step, we will develop one more tool to directly process the feature functions, which is, in a sense, to make projections in the functional space using neural networks. This will be a useful addition to our toolbox before we start to address multi-variate learning problems.</p> <p><br/></p> <hr/> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/">Dr. Xiagxiang Xu</a>.</p>]]></content><author><name>Lizhong Zheng</name></author><category term="ML-Theory"/><category term="H-Score"/><category term="modal-decomposition"/><summary type="html"><![CDATA[The Key Points H-score is our first step in introducing neural-network-based methods to compute modal decomposition from data. It is a loss metric used in the training of neural networks in a specific configuration, which we call the “H-Score Network.” The result is approximately equivalent to a conventional end-to-end neural network with cross-entropy loss, with the additional benefit of allowing direct control of the chosen feature functions. This is an important conceptual step to turn the goal of learning from learning a probability model to learning feature functions. In practice, H-score networks have a number of natural extensions that allow more flexible operations to incorporate external knowledge.]]></summary></entry><entry><title type="html">Modal Decomposition</title><link href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/" rel="alternate" type="text/html" title="Modal Decomposition"/><published>2024-06-23T00:00:00+00:00</published><updated>2024-06-23T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/modal-decomposition</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/modal-decomposition/"><![CDATA[<blockquote> <h3 id="the-key-points">The Key Points</h3> <p><code class="language-plaintext highlighter-rouge">Statistical dependence</code> is the reason that we can guess the value of one random variable based on the observation of another. This is the basis of most inference problems like decision-making, estimation, prediction, classification, etc.</p> <p>It is, however, a somewhat ill-posed question to ask, “How much does a random variable \(\mathsf x\) depend on another random variable \(\mathsf y\).” It turns out that, in general, statistical dependence should be understood and quantified as a high dimensional relation: two random variables are dependent through a number of <strong>orthogonal modes</strong>, and each mode can have a different <strong>strength</strong>.</p> <p>The goal of this page is to define these modes mathematically, explain why they are important in practice, and show by examples that many statistical concepts and learning algorithms are directly related to this modal decomposition idea. With that, we will also build the mathematical foundation and notations for the more advanced processing using modal decomposition in the later pages.</p> </blockquote> <p><br/></p> <h2 id="modal-decomposition-of-statistical-dependence">Modal Decomposition of Statistical Dependence</h2> <p>Let’s start by motivating and defining the concept of modal decomposition.</p> <h3 id="inner-product-of-functions">Inner Product of Functions</h3> <p>We start by defining an <strong>inner product</strong> in the functional space. Given an alphabet \(\mathcal X\), the space of all real-valued functions,</p> \[\mathcal {F_X} = \{f: \mathcal X \to \mathbb R \},\] <p>can be viewed as a vector space. Here, we need to fix a distribution \(R_\mathsf x\) on \(\mathcal X\), which we call the <strong>reference distribution</strong>. Based on that, we can define the inner product: for any \(f_1, f_2 \in \mathcal F_\mathcal X\),</p> \[\langle f_1, f_2\rangle \stackrel{\Delta}{=} \mathbb E_{\mathsf x \sim R_\mathsf x}[f_1(\mathsf x) \cdot f_2(\mathsf x)]\] <blockquote> <p><strong>Note:</strong> In almost all cases, we can, without loss of generality, restrict functions to have zero mean w.r.t. \(R_\mathsf x\). Thus, the inner product is really the covariance of \(f_1(\mathsf x)\) and \(f_2(\mathsf x)\). Furthermore, on this page, we would not change the reference distribution once chosen, so we could use the above notation for inner products. Otherwise, we could put a subscript to indicate the reference, like \(\langle f_1, f_2\rangle_{R_\mathsf x}\).</p> </blockquote> <p>We can similarly define the inner product on the space of functions on a different alphabet \(\mathcal Y\), with respect to a reference distribution \(R_\mathsf y\).</p> <p><br/></p> <h3 id="the-pmi-function">The PMI Function</h3> <p>Now we are ready to address the joint distributions \(P_{\mathsf {xy}}\) on \(\mathcal {X\times Y}\). Again we need to choose a reference distribution \(R_\mathsf {xy}\). For the purpose of this page, we use the product distribution \(R_{\mathsf {xy}} = R_\mathsf x\cdot R_\mathsf y\) and take the resulting definition of the inner product of functions in \(\mathcal F_{\mathcal X\times \mathcal Y}\).</p> <p>A particular function of interest in \(\mathcal {F_{X\times Y}}\) is the <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information">Point-wise Mutual Information (PMI)</a></p> \[\mathrm{PMI}(x,y) \stackrel{\Delta}{=}\log \frac{P_{\mathsf {xy}}(x,y)}{P_{\mathsf x}(x) \cdot P_{\mathsf y}(y)}, \quad x\in \mathcal X, y \in \mathcal Y\] <p>where \(P_\mathsf x\) and \(P_\mathsf y\) are the \(\mathsf x\) and \(\mathsf y\) marginal distributions of \(P_\mathsf {xy}\). It is clear from the definition that \(\mathrm{PMI}(x,y) = 0, \forall x\) if and only if the two random variables \(\mathsf{x, y}\) are independent; and in general, this function gives a complete description of how the two are dependent to each other. Consequently, the PMI function, or in some equivalent or reduced forms, is the target of almost all learning problems. The main difficulty in practice is that the alphabets \(\mathcal {X, Y}\) are often very big, causing the PMI function to be very high dimensional, which makes these learning tasks difficult.</p> <p>Here, we need to make a technical assumption. For a pair of functions \(f \in \mathcal {F_X}\) and \(g \in \mathcal {F_Y}\), we denote \(f\otimes g \in \mathcal {F_{X\times Y}}\) as the “tensor product function” or simply the product function, with \(f\otimes g(x,y) \stackrel{\Delta}{=} f(x) g(y), \forall x, y\). Now we assume that the joint distribution \(P_{\mathsf {xy}}\) satisfies there exists a possibly infinite sequence of pairs of functions \((f_i, g_i), f_i \in \mathcal {F_X}, g_i \in \mathcal {F_Y}, i=1, 2, \ldots\), such that</p> \[\lim_{n\to \infty} \left\Vert \mathrm{PMI} - \sum_{i=1}^n f_i \otimes g_i \right\Vert^2 = \lim_{n\to \infty} \mathbb E_{\mathsf {x,y} \sim R_\mathsf xR_\mathsf y}\left[ \left(\mathrm{PMI}(\mathsf {x, y}) - \sum_{i=1}^n f_i(\mathsf x) g_i(\mathsf y) \right)^2\right] = 0\] <blockquote> <p><strong>Note:</strong> In other words, this assumption says that the PMI function can be approached, in an L2 sense, by the sum of a countable collection of product functions, with L2 defined w.r.t. the given reference distribution. This assumption is always true for the cases that both \(\mathcal X\) and \(\mathcal Y\) are discrete alphabets. For more general cases, the assumption of a countable basis in the L2 sense is a commonly used assumption, which is not restrictive at all in most practical applications, and convenient for us to rule out some of the “unpleasant” distributions.</p> </blockquote> <p><br/></p> <h3 id="a-single-mode">A Single Mode</h3> <p>Why are we so interested in such product functions? In short, it represents a very simple kind of dependence. Imagine a joint distribution \(P_{\mathsf {xy}}\) whose PMI function can be written as</p> \[\log \frac{P_{\mathsf {xy}}(x,y)}{P_\mathsf x(x) P_\mathsf y(y)} = f(x) \cdot g(y), \qquad \forall x, y.\] <p>This can be rewritten as \(P_{\mathsf {y\vert x}}(y\vert x) = P_\mathsf y (y) \cdot \exp(f(x)\cdot g(y)), \forall x, y\). That is, the conditional distribution is on a 1-D exponential family with \(g(\mathsf y)\) as the natural statistic. To make an inference of \(\mathsf y\), we only need to know the value \(f(\mathsf x)\), which is a sufficient statistic. In fact, the only thing we can infer about \(\mathsf y\) is the value of \(g(\mathsf y)\). In general, we could extrapolate from this observation to state that if the PMI function is the sum of a limited number of product functions, then that correspondingly limits the scope of inference tasks we can hope to solve while allowing us to only look at a limited set of statistics, or <strong>features</strong>, of the data.</p> <blockquote> <p>Here, to clarify the terminology, we refer to <strong>feature functions</strong> of a random variable as real-valued functions on the alphabet, such as \(f: \mathcal X \to \mathbb R\). Feature functions are often evaluated with the observed data samples, and the function values, which we refer to as <strong>features</strong>, are used for further inference and learning tasks instead of the raw data. Thus, these features are indeed the <code class="language-plaintext highlighter-rouge">information carrying device.</code> Since any known shifting and scaling do not change the information contents that these features carry, for convenience, we sometimes require a standard form, that the feature functions satisfying \(\mathbb E[f(\mathsf x)] = 0\) and \(\mathbb E[f^2(\mathsf x)]=1\), where both expectations are taken w.r.t. the reference distribution \(R_\mathsf x\).</p> </blockquote> <p>When we write a product function like the one above in this standard form, we need to write out the scaling factor explicitly. That is, instead of \(f\otimes g\), we need to write \(\sigma f\otimes g\), with \(\sigma \geq 0\). We call this triple, \((\sigma, f, g)\), a single <strong>mode</strong>. That is, a mode consists of a strength \(\sigma\), and a pair of feature functions in \(\mathcal {F_X}\) and \(\mathcal {F_Y}\).</p> <p><br/></p> <h3 id="modal-decomposition-the-definition">Modal Decomposition: the Definition</h3> <p>Obviously, there are many ways to write a given PMI function into a sum of modes. We hope to have as few modes as possible. In some cases, we might even wish to compromise the precision of the sum and try to have a reasonable approximation of the given PMI with a sum of even fewer modes. That is, for a given finite \(k\), we would like to solve the problem.</p> \[\min_{ (\sigma_i, f_i, g_i), i=1, \ldots, k} \, \left \Vert \mathrm{PMI} - \sum_{i=1}^k \sigma_i f_i\otimes g_i\right\Vert^2\] <p>This optimization is, in fact, a well-studied one. For the case with finite alphabets, the target PMI function can be thought as a \(\vert\mathcal X\vert \times \vert\mathcal Y\vert\) matrix, with the \((x,y)\) entry being the function value \(\mathrm {PMI}(x,y)\); and the above optimization problem is solved by finding the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition">singular value decomposition (SVD)</a> of this matrix. The result is a decomposition, which is a diagonalization, turning the PMI matrix into the sum of orthogonal rank-1 matrices, each of which corresponds to one mode in our definition. Here, we will define the modal decomposition with a sequential construction, which is indeed a standard way to define SVD.</p> <p><br/></p> <hr/> <dl> <dt>Definition: Rank-1 Approximation</dt> <dd>For a function \(B \in \mathcal {F_{X\times Y}}\), and a given reference distribution \(R_{\mathsf {xy}} = R_\mathsf x R_\mathsf y\), the rank-1 approximation of \(B\) is a map: \(B \mapsto (\sigma, f^\ast, g^\ast)\),</dd> </dl> \[(\sigma, f^\ast, g^\ast)\stackrel{\Delta}{=} \arg\min_{\sigma, f, g} \; \Vert B - \sigma\cdot f\otimes g\Vert^2\] <p>where the optimization has the constraints: \(\sigma \geq 0\), \(f^\ast \in \mathcal {F_X}, g^\ast\in \mathcal {F_Y}\), are standard feature functions, i.e., \(f^\ast, g^\ast\) both have zero mean and unit variance w.r.t. \(R_\mathsf{x}, R_\mathsf{y}\), respectively.</p> <hr/> <p><br/></p> <p>We will state here without proof an intuitive property of this approximation, which we will use rather frequently: the approximation error is orthogonal to the optimal feature functions, i.e.</p> \[\begin{align*} &amp;\sum_{x\in \mathcal X} \; R_{\mathsf x}(x) \cdot \left[ \left(B(x,y) - \sigma\cdot f^\ast (x) g^\ast (y)\right) \cdot f^\ast (x) \right] = 0 , \qquad \forall y\\ &amp;\sum_{y\in \mathcal Y} \; R_{\mathsf y}(y) \cdot \left[ \left(B(x,y) - \sigma\cdot f^\ast (x) g^\ast (y)\right) \cdot g^\ast (y) \right] = 0 , \qquad \forall x \end{align*}\] <p>Based on this, we have the following definition of modal decomposition.</p> <p><br/></p> <hr/> <dl> <dt>Definition: Modal Decomposition \(\zeta\)</dt> <dd> <p>For a given joint distribution \(P_{\mathsf {xy}}\) on \(\mathcal {X \times Y}\) and a reference distribution \(R_{\mathsf {xy}} = R_\mathsf x R_\mathsf y\). We denote the rank-1 approximation of the PMI as</p> </dd> </dl> \[\zeta_1(P_{\mathsf {xy}}) = (\sigma_1, f_1^\ast, g_1^\ast) \stackrel{\Delta}{=} \arg \min_{\sigma, f, g}\;\left\Vert \left(\log \frac{P_{\mathsf {xy}}}{P_\mathsf xP\_\mathsf y}\right) - \sigma\cdot f\otimes g\right\Vert^2\] <p>and for \(i=2, 3, \ldots\), \(\zeta_i\) as the rank-1 approximation of the approximation error of all the previous steps:</p> \[\zeta_i(P_{\mathsf{xy}}) = (\sigma_i, f_i^\ast, g_i^\ast ) \stackrel{\Delta}{=} \arg\min_{\sigma, f, g} \left\Vert\left(\mathrm{PMI} - \sum_{j=1}^{i-1} \sigma_j \cdot f_j^\ast \otimes g_j^\ast \right) - \sigma\cdot f\otimes g\right\Vert^2\] <p>Collectively, \(\lbrace \zeta_i \rbrace : P_{\mathsf {xy}} \mapsto \lbrace(\sigma_i, f^\ast_i, g^\ast_i), i=1, 2, \ldots\rbrace\) is called the <strong>modal decomposition operation</strong></p> <hr/> <p><br/></p> <p>A few remarks are in order.</p> <ol> <li>The following facts are similar to those of SVD, following similar proof, which we omit: <ul> <li>\(\sigma_1 \geq \sigma_2 \geq \ldots\) in descending order</li> <li>\(\langle f^\ast_i, f^\ast_j \rangle = \langle g^\ast_i, g^\ast_j \rangle = \delta_{ij}\), i.e., the feature functions in different modes are orthogonal to each other.</li> </ul> </li> <li>We denote this decomposition as \(\lbrace\zeta_i \rbrace (P_{\mathsf {xy}})\), or simply \(\zeta(P_{\mathsf {xy}})\), which should be read as “the \(\zeta\)-operation for the \(\mathsf{x-y}\) dependence defined by the joint distribution \(P_{\mathsf{xy}}\)”. While we write the functional decomposition as an L2 approximation to the PMI function, the PMI is not the unique way to describe the dependence. Later, we will have examples where it is convenient to use a slightly different target function, with the resulting choices of the feature functions also being a bit different. We consider all such operations to decompose the dependence as the same general idea.</li> <li>The definition says that for each model \(P_{\mathsf {xy}}\) there is an ideal sequence of modes for the orthogonal decomposition. In practice, we do not observe either the model or the s. We will show later that learning algorithms often try to learn an approximate version of the modes. For example, it is common only to learn the first \(k\) modes, to learn the decomposition of an empirical distribution from a finite dataset, or to have extra restrictions of the learned features due to the limited expressive power of a network, etc. In more complex problems, sometimes it might not even be clear which dependence we are trying to decompose. The purpose of defining the \(\zeta\) operation is to help us clarify what type of compromises are taken in finding a computable approximate solution to the idealized decomposition problem.</li> </ol> <p><br/></p> <h2 id="properties-of-modal-decomposition">Properties of Modal Decomposition</h2> <p>There are many nice properties of this modal decomposition. The best way to see them is to go through our <a href="http://lizhongzheng.mit.edu/sites/default/files/documents/mace_final.pdf">survey paper</a>. On this page, we will only state some of them as facts without any proof and sometimes with intuitive but not-so-precise statements. The central point of this is to make the following statement.</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Modal decomposition</code>, the \(\zeta\) operation of a model \(P_{\mathsf {xy}}\), decomposes the dependence between two random variables \(\mathsf x\) and \(\mathsf y\) into a sequence of pairwise correlation between features \(f^\ast_i(\mathsf x)\) and \(g^\ast_i(\mathsf y)\), with correlation coefficient \(\sigma_i\), for \(i=1, 2, \ldots\).</p> </blockquote> <p>This statement is important since in both learning the model \(P_{\mathsf {xy}}\) and using it for inference tasks. We no longer have to carry the entire model, which is often far too complex. Instead, we can learn and use only a subset of modes. Because we have \(\sigma_i\)’s to quantify the strengths of these modes, we would know exactly how to choose the more important modes and how good is the resulting approximate model.</p> <p>One technical issue is the <strong>local assumption</strong>. Many nice properties and connections for the modal decomposition are asymptotic statements, proved in the limiting regime where \(P_{\mathsf {xy}}, P_\mathsf x \cdot P_\mathsf y\), and \(R_\mathsf x\cdot R_\mathsf y\) are all “close” to each other. Such local assumptions are indeed a fundamental concept: The space of probability distributions is not a linear vector space but a manifold. The local assumption allows us to focus on a neighborhood that can be approximated by the tangent plane of the manifold and, hence, get the geometry linearized. Details of this can be found in the literature of <a href="https://www.amazon.com/Information-Translations-Mathematical-Monographs-Tanslations/dp/0821843028">information geometry</a> and <a href="https://en.wikipedia.org/wiki/Correspondence_analysis">correspondence analysis</a>. A quick example is that the following approximation to the PMI function is often used in our development with the assumption that the precision is acceptable.</p> \[\mathrm{PMI}(x,y) = \log \left( \frac{P_{\mathsf {xy}}(x,y)}{P_\mathsf x(x) P_\mathsf y(y)} \right)\approx \widetilde{\mathrm{PMI}}(x,y) = \frac{P_{\mathsf {xy}}(x,y) - P_\mathsf x(x) P_\mathsf y(y)}{P_\mathsf x(x) P_\mathsf y(y)}\] <p>This inevitably leads to some technical details when making mathematical statements. Different statements might require different strengths of the local assumptions, and in some cases, one can even circumvent such assumptions by making a slightly different statement. To avoid leading our readers into such discussions, we will simply call all of such things the “local approximation” and assume they are given for all statements regardless of what is needed. Furthermore, we will hide the rest of these statements in a toggled block. If the reader is comfortable with our main message about decomposing the dependence and not interested in the mathematical steps, this <a href="#an-example-of-numerical-computation-of-modal-decomposition">link</a> can be used to skip to the algorithm part of our story.</p> <p><br/></p> <h3 id="the-conditional-expectation-operator">The Conditional Expectation Operator</h3> <p>Now, we enter the regime with the local assumptions. That is, the \(\mathrm{PMI}\) and \(\widetilde{\mathrm{PMI}}\) are now considered the same function. For convenience, we will just take the reference \(R_\mathsf x = P_\mathsf x, R_\mathsf y= P_\mathsf y\) to further simplify things.</p> <p>We start with the interesting fact about \({\mathrm{PMI}}\): when viewed as an operator on the functional space, it is closely related to the conditional expectation operator.</p> <p><br/></p> <hr/> <dl> <dt>Property 1: PMI and the Conditional Expectation Operator</dt> <dd>Let \(B : \mathcal {F_X} \to \mathcal {F_Y}\) be defined as: for \(a\in \mathcal {F_X}\), \(B(a) \in \mathcal {F_Y}\) with</dd> </dl> \[\begin{align*} \left(B(a)\right) (y) &amp;\stackrel{\Delta}{=} \sum_{x\in \mathcal X} {\mathrm{PMI}}(x,y)\cdot (P_{\mathsf x} (x) \cdot a(x))\\ &amp;= \sum_{x\in \mathcal X}\frac{P_{\mathsf {xy}}(x,y) - P_{\mathsf x}(x) P_{\mathsf y}(y)}{P_{\mathsf x}(x) P_{\mathsf y}(y)} \cdot (P_{\mathsf x}(x) \cdot a(x))\\ &amp;= \mathbb {E} [a({\mathsf x}) | {\mathsf y} = y ] \end{align*}\] <hr/> <p>We write sum over \(x\) in the above, which, of course, can be turned into an integral when \(x\) is continuous-valued. The \({\mathrm{PMI}}\) function does not directly act on the input \(a(\cdot)\), but instead needs an extra \(P_\mathsf x(x)\) multiplied. This is “natural” if we think of integrals under the measure specified by the reference.</p> <p>One can also define a transpose operator \(B^T: \mathcal {F_Y}\to \mathcal {F_X}\), for \(b \in \mathcal {F_Y}\),</p> \[\left(B^T(b)\right)(x) = \sum_{y\in \mathcal Y}\frac{P_{\mathsf {xy}}(x,y) - P_{\mathsf x}(x) P_{\mathsf y}(y)}{P_{\mathsf x}(x) P_{\mathsf y}(y)} \cdot (P_{\mathsf y}(y) \cdot b(y))= \mathbb E[b({\mathsf y})|{\mathsf x}=x], \forall x.\] <p>Now if we have the modal decomposition \(\lbrace \zeta_i \rbrace\) of \(P_{\mathsf {xy}}\) as \(\lbrace(\sigma_i, f_i^\ast, g_i^\ast), i=1, 2, \ldots\rbrace\) as defined, we have the following facts.</p> <p>What this property says is that the model \(\mathrm{PMI}\) that we try to learn in a learning system is, in fact, equivalent to the conditional expectation operator of feature functions.</p> <p><br/></p> <hr/> <dl> <dt>Property 2: Correlation Between \(f_i^\ast({\mathsf x}), g_i^\ast({\mathsf y})\)</dt> <dd> <p>Let \(f_i^\ast, g_i^\ast\) as the modal decomposition defined above for a given dependence model, then</p> </dd> </dl> \[{\mathbb E}_{\mathsf{x,y} \sim P_{\mathsf{x,y}}} [ f^\ast_i ({\mathsf x}) \cdot g^\ast_j({\mathsf y})] =\sigma_i \cdot \delta_{ij}\] <details><summary>Proof:</summary> <p>Consider</p> \[(B(f^\ast_j))(y) = \sum_x \left(\sum_i \sigma_i \cdot f^\ast_i(x) g^\ast_i(y)\right) \cdot \left(P_{\mathsf x}(x) \cdot f^\ast_j(x)\right) = \sigma_j g^\ast_j(y), \quad \forall y\] <p>since \(\mathbb E[f^\ast_i({\mathsf x}) f^\ast_j({\mathsf x})] = \delta_{ij}\). With the same math, we also have \(B^T(g^\ast_j) = \sigma_j \cdot f^\ast_j\).</p> <p>That is, each \(g^\ast_j\) is the image of the \(B(\cdot)\) operator acting on \(f^\ast_j\), scaled by the corresponding \(\sigma_i\), and vice versa.</p> <p>Now, we have \({\mathbb E}_{\mathsf{x,y} \sim P_{\mathsf{x,y}}} [ f^\ast_i ({\mathsf x}) \cdot g^\ast_j({\mathsf y})]= {\mathbb E}[f^\ast_i ({\mathsf x}) \cdot {\mathbb E}[g^\ast_j({\mathsf y})|{\mathsf x}]] =\sigma_i \cdot \delta_{ij}\)</p> </details> <hr/> <p>This result says that each feature, \(f^\ast_i(\mathsf x)\), is only correlated with the corresponding \(g^\ast_i\) feature of \(\mathsf y\), and uncorrelated with all other features. \(\sigma_i\) is the correlation coefficient. This gives an alternative understanding of the strength value associated with each mode as the correlation coefficient between feature pairs. Furthermore, the dependence between \(\mathsf x\) and \(\mathsf y\) is, in fact, written as a sequence of correlation between feature pairs, each with a strength quantified by the corresponding \(\sigma_i\).</p> <p>In this <a href="https://static.renyi.hu/renyi_cikkek/1959_on_measures_of_dependence.pdf">1959 paper</a>, the HGR maximal correlation is defined for a given joint distribution \(P_{\mathsf {xy}}\) as</p> \[\rho_{\mathrm{HGR}} \stackrel{\Delta}{=} \max_{f \in \mathcal {F_X}, g \in \mathcal {F_Y}} \; \rho (f(\mathsf x), g(\mathsf y)),\] <p>where \(\rho\) denotes the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearson correlation coefficient</a>. This maximal correlation coefficient \(\rho_{\mathrm{HGR}}\) is used as a measure of dependence between the two random variables. At this point, it should be clear that the modal decomposition structure is a natural generalization. \(\sigma_1\), the correlation between the strongest correlated feature pairs is exactly the HGR maximal correlation coefficient. Beyond that, there is indeed a sequence of correlated feature pairs in descending order of strengths.</p> <p><br/></p> <h3 id="divergence-and-fisher-information">Divergence and Fisher Information</h3> <p>As we stated with the <a href="#a-single-mode">definition of modes</a>, writing the PMI function as a sum of product functions puts the model \(P_{\mathsf {xy}}\) on an exponential family. Locally, the behavior of such an exponential family is fully determined by the <a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information</a>. For example, if \(\mathrm{PMI}= \log\frac{P_{\mathsf{xy}}}{P_\mathsf x P_\mathsf y} = \sum_{i=1}^k f_i \otimes g_i,\)</p> <p>then the conditional distribution \(P_{\mathsf {x \vert y}}(\cdot \vert y)\), for different values of \(y\), are on a \(k\) - dimensional exponential family with \(f_i(\cdot), i=1, \ldots, k\) as the natural statistics, and \(g_i(y), i=1, \ldots k\) as the corresponding parameters. The Fisher information for this family is a \(k\times k\) matrix \(\mathcal I\), with entries</p> \[[\mathcal I]_{ij} = \mathbb E_{\mathbb x \sim P_\mathsf x} \left[ \left(\frac{\partial}{\partial g_i} \mathrm {PMI}\right) \cdot \left(\frac{\partial}{\partial g_j} \mathrm {PMI}\right)\right] = \mathbb E_{\mathbb x \sim P_\mathsf x} [f_i(\mathsf x) f_j(\mathsf x)] = \langle f_i, f_j \rangle\] <p>which is exactly the definition of the inner product we started with. In this context, we can also understand the <a href="#modal-decomposition">orthogonal modal decomposition</a> as a special and nice case where the Fisher information matrix is diagonalized.</p> <p>There are some direct consequences of this connection.</p> <p><br/></p> <hr/> <dl> <dt>Property 3: K-L divergence</dt> <dd>If two distribution on \(\mathcal X\), \(P_\mathsf x\) and \(Q_\mathsf x\), are both in the neighborhood of the reference distribution \(R_\mathsf x\), with \(\log P_\mathsf x/Q_\mathsf x = f\), then \(D(P_\mathsf x \Vert Q_\mathsf x) \approx \frac{1}{2} \Vert f\Vert^2\)</dd> <dd> <p>where \(D(P \Vert Q)\) is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">Kullback-Leibler divergence</a>. The relation between the K-L divergence and the Fisher information can be found <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Fisher_information_metric">here</a>.</p> </dd> </dl> <hr/> <p><br/></p> <p>Applying this fact to the PMI function, we have the following statement.</p> <hr/> <dl> <dt>Property 4: Mutual Information</dt> <dd> \[\frac{1}{2} \Vert \mathrm{PMI} \Vert^2 \approx D(P_{\mathsf {xy}} \Vert P_\mathsf x P_\mathsf y) = I(\mathsf x; \mathsf y)\] </dd> <dd> <p>where \(I(\mathsf x; \mathsf y)\) is the <a href="https://en.wikipedia.org/wiki/Mutual_information">mutual information</a> between \(\mathsf x\) and \(\mathsf y\), which is another popular way to measure how much the two random variables depend on each other.</p> </dd> </dl> <hr/> <p><br/></p> <p>Now if we have the modal decomposition \(\zeta(P_\mathsf {xy}) = [(\sigma_i, f^\ast_i, g^\ast_i), i=1, 2, \ldots]\), we have the following result.</p> <hr/> <dl> <dt>Property 5: Decomposition of the Mutual Information</dt> <dd> \[I(\mathsf x; \mathsf y) = \frac{1}{2} \Vert \mathrm{PMI} \Vert^2 = \frac{1}{2} \sum_i \sigma_i^2\] </dd> </dl> <hr/> <p>This is probably the cleanest way to understand the modal decomposition: it breaks the mutual information into the sum of a number of modes, as the (squared) strengths of these modes add up to the mutual information. As stated earlier, it is often difficult to learn or store the PMI function in practice due to the high dimensionality of the data. In these cases, it is a good idea to approximate the PMI function with a truncated version that only keeps the first \(k\) strongest modes. This not only gives the best rank-limited approximation of the joint distribution, as stated in equation (2) in the <a href="#definition-modal-decomposition-zeta">definition</a>, but also captures the most significant dependence relation (the most strongly correlated feature pairs) and in that sense makes the approximation useful in inference tasks.</p> <p><br/></p> <h2 id="blacktriangle-demo-modal-decomposition-and-neural-networks">\(\blacktriangle\) <strong>Demo:</strong> Modal Decomposition and Neural Networks</h2> <p>To wrap up this introduction page, we will show one simple example, where we have a small synthesized dataset to train a simple neural network. When the training procedure converges, we demonstrate that the learned features match with the result of the \(\zeta\) operation. The purpose of this numerical example is to show that, with a simple dataset and a carefully chosen neural network, the learning in the <strong>neural network</strong> is indeed finding a low-rank approximation to the true model, which is consistent with the <strong>modal decomposition</strong> operation defined in this page.</p> <p>Here is a <a href="https://colab.research.google.com/drive/1n4qk69shPL0LvGcaUJ4WIeJJRdyp2zA-?usp=sharing">colab demo</a> for illustrating the connection. A more theoretical discussion can be found in <a href="https://doi.org/10.3390/e24010135">this paper</a>.</p> <p><br/></p> <hr/> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/">Dr. Xiagxiang Xu</a>.</p>]]></content><author><name>Lizhong Zheng</name></author><category term="ML-Theory"/><category term="H-Score"/><category term="modal-decomposition"/><summary type="html"><![CDATA[The Key Points Statistical dependence is the reason that we can guess the value of one random variable based on the observation of another. This is the basis of most inference problems like decision-making, estimation, prediction, classification, etc. It is, however, a somewhat ill-posed question to ask, “How much does a random variable \(\mathsf x\) depend on another random variable \(\mathsf y\).” It turns out that, in general, statistical dependence should be understood and quantified as a high dimensional relation: two random variables are dependent through a number of orthogonal modes, and each mode can have a different strength. The goal of this page is to define these modes mathematically, explain why they are important in practice, and show by examples that many statistical concepts and learning algorithms are directly related to this modal decomposition idea. With that, we will also build the mathematical foundation and notations for the more advanced processing using modal decomposition in the later pages.]]></summary></entry><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://lizhongzheng.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://lizhongzheng.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/tabs</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="ac321997-12b3-4e04-ab8b-3db3410b288f" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="ac321997-12b3-4e04-ab8b-3db3410b288f" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="1884ac2d-820c-4026-85e9-902928f408b5" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="1884ac2d-820c-4026-85e9-902928f408b5" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="40faba5e-2efe-4e82-9808-3de93f2a7807" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="40faba5e-2efe-4e82-9808-3de93f2a7807" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://lizhongzheng.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/typograms</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://lizhongzheng.github.io/blog/2024/post-citation-2/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://lizhongzheng.github.io/blog/2024/post-citation%202</id><content type="html" xml:base="https://lizhongzheng.github.io/blog/2024/post-citation-2/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry></feed>