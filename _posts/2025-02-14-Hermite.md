---
title: "Hermite Polynomials and Applications in ML"
layout: distill
tags: math
categories: ML-Theory
featured: false

mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Lizhong Zheng
    url: "https://lizhongzheng.github.io/"
    affiliations:
      name: EECS, MIT


bibliography: blog_references.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: The Key Points
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: What are Hermite Polynomials? Definitions and Notations
  - name: Informative Features from Observation with Gaussian Additive Noise
  - name: Multi-Variate Hermite Polynomials
    subsections:
      -name: Separate Learning of Non-Linearity and Memory
 
---

## The Key Points
The power of Neural Networks is largely derived from the ability to generate complex, non-linear functions for data processing. This marks a significant departure from classical model-based approaches, which often rely on linearity assumptions. Traditional tools such as the Fourier Transform, spectral methods, eigenfunctions, and least mean-square error (LMS) estimators lose their theoretical justification when linearity is removed. Interestingly, the challenge of designing non-linear processing is not new. Foundational work in this area includes Wiener's works on the [Volterra Series](https://en.wikipedia.org/wiki/Volterra_series) and multivariate orthogonal polynomials, which is now a largely overlooked literature known as [the Wiener G-Functional Expansion](https://en.wikipedia.org/wiki/Wiener_series), or the Wiener-Hermite Expansion. This page aims to provide a tutorial on these concepts in the context of modern machine learning applications. In particular, we will explore the connections between these classical techniques and the emerging field of Reservoir Computing, also with our own research on [H-Score Networks](https://lizhongzheng.github.io/blog/2024/H-Score/). 

There is a huge literature related to orthogonal polynomials, or more specifically to the Hermite polynomials. Some of the treatments can be quite elaborate, involving complex analysis and combinatorics. Some references are hard to find these days. We will also try to include some of the older papers or present the shortest proofs we know to some of the important classical facts.
 

## What are Hermite Polynomials: Definitions and Notations

There are several versions of the [Hermite Polynomials](https://en.wikipedia.org/wiki/Hermite_polynomials) defined in the literature. Unfortunately, we won't use any of them, but will define our own before making connections to the more widely used notations. 

To start, we denote $${\mathcal N}(x; \mu, \sigma^2)$$ as the Gaussian probability density function (pdf) with mean $\mu$ and variance $\sigma^2$ for variable $x$. We often call this distribution a **reference distribution**.

---
Definition: Hermite Basis Functions
: For a given reference distribution as a Gaussian density function $${\mathcal N}(x; \mu, \sigma^2)$$, the Hermite polynomials are the sequence of polynomials $\phi_0, \phi_1, \ldots, \phi_k, \ldots$, where $\phi_k(\cdot; \mu, \sigma^2): \mathbb {R \longrightarrow R}$ is a $k$-degree polynomial, with parameter $\mu, \sigma^2$, and satisfy

\begin{equation}
\label{eqn:Hermite}
\langle \phi_i(\cdot;\mu, \sigma^2), \phi_j(\cdot; \mu, \sigma^2) \rangle \stackrel{\Delta}{=} \int_{-\infty}^\infty \; {\mathcal N} (x; \mu, \sigma^2) \cdot \phi_i(x; \mu, \sigma^2) \cdot \phi_j (x; \mu, \sigma^2) \; dx = \delta_{ij}
\end{equation}

---

**Remark: The Reference Distribution and Change of Parameter**

The Hermite basis functions are defined with the parameter $\mu, \sigma^2$, i.e. the Normal distribution used to define the inner product. We can define the "standard" Hermite basis functions as $\phi_k(\cdot; 0, 1)$, i.e. defined with respect to the standard Normal distribution. For the standard basis functions, we often drop the parameters and write $\phi_k(x)$ for convenience.  

There is a simple conversion between basis functions with different parameters as follows.


Proposition: Change of Parameter
: For any $k, \mu, \sigma^2$ and any $x$,

\begin{equation}
\label{eqn:change_parameter}
\phi_k(x; \mu, \sigma^2) = \phi_k\left( \frac{x-\mu}{\sigma}; 0, 1\right)
\end{equation}


{% details This can be verified with a simple change of variable.  %}

We only need to verify that with $s = \frac{x-\mu}{\sigma}$:

$$
\begin{align*}
& \int_{-\infty}^\infty {\mathcal N}(x; \mu, \sigma^2) \cdot \phi_i \left(\frac{x-\mu}{\sigma}; 0, 1\right) \cdot \phi_j \left(\frac{x-\mu}{\sigma}; 0, 1\right)\; dx\\\\
&= \int_{-\infty}^\infty {\mathcal N}(s; 0, 1) \cdot \phi_i(s; 0,1) \cdot \phi_j(s; 0, 1) \; ds \\\\
&= \delta_{ij}
\end{align*}
$$
{% enddetails %}


**Remark: These basis functions are closely related to Hermite Polynomials.**

We deliberately use a slightly different notation and a different name to separate the basis functions from the existing terminology on Hermite polynomials. There are many nice properties the Hermite polynomials, but we will use only a few of them. For example, our discussion is generally known in the literature as the "Hermite Expansion", which starts with a key fact that the basis functions in our definition form a complete basis to all L2 functions <d-cite key="Davis2024HermiteExpansion"></d-cite>. It would be nice to see how our definition of the basis functions is related to Hermite polynomials. 


{% details Relation to the literature.%}

In the [literature](https://en.wikipedia.org/wiki/Hermite_polynomials), the "probabilist's Hermite polynomials" are defined as

$$ He_n (x) = (-1)^n e^{\frac{x^2}{2}} \frac{d^n}{dx^n} e^{-\frac{x^2}{2}}$$

which satisfy the orthogonality condition

$$\int_{-\infty}^\infty He_i(x)\cdot He_j(x) \cdot e^{-\frac{x^2}{2}} \; dx = \sqrt{2\pi}\cdot  i! \cdot \delta_{ij}$$

These are nice because the resulting polynomials are *monic*, and because there is a nice recursion as

$$He_{k+1} (x) = x \cdot He_k(x) - He_k'(x)$$

In our development, we do not care much about how to find the Hermite polynomials, or how complex are the coefficients. We thus choose a more convenient normalization

$$\phi_k(x) \stackrel{\Delta}{=} \frac{1}{\sqrt{k!}} \cdot He_k(x)$$

so that the basis function have unit energy. 

---
For the sake of completeness, I will also hide a few important facts about Hermite polynomials in here. 

Proposition: Hermite Expansion
: 
For an $L_2$ function $f(x)$, we have 

$$ f(x) = \sum_{k=0}^\infty d_k \cdot \phi_k(x)$$

where 

$$ d_k = \frac{1}{\sqrt{k!}} \int_{-\infty}^\infty \frac{d^k f(x)}{dx^k} \cdot {\mathcal N}(x; 0,1)\; dx$$

**Proof:**

By the definition of $\phi_k}'s as a complete orthogonal basis, we have 

$$ d_k = \int {\mathcal N}(x; 0, 1) \cdot f(x) \cdot \phi_k(x) \; dx$$

This is where we need the definition of the Hermite polynomials:

$$
\begin{align*}
d_k &= \int \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot f(x) \cdot \left( \frac{1}{\sqrt{k!}} \cdot (-1)^k e^{\frac{x^2}{2}} \frac{d^k}{dx^k} e^{-\frac{x^2}{2}} \right) \; dx\\
&= \frac{1}{\sqrt{k!}} \cdot \int  (-1)^k f(x) \cdot \frac{d^k}{dx^k} \mathcal N(x; 0,1)\; dx
\end{align*}
$$

Now the integral needs a $k$-step integral by part to get the desired coefficient. 



Proposition: Generating Function
: 

$$e^{xt - \frac{t^2}{2}} = \sum_{k=0}^\infty \frac{t^k}{k!} \cdot He_k(x)$$

**Proof:**

This is a simple application of the Hermite expansion result above. Consider $f(x) = e^{xt}$ as a function of $x$:

$$
\begin{align*}
f(x) = \sum_{k=0}^\infty \frac{d_k}{\sqrt{k!}} \cdot He_k(x)
\end{align*}
$$

where 

$$
\begin{align*}
\frac{d_k}{\sqrt{k!}} &=\frac{1}{k!} \int_{-infty}^\infty {\mathcal N}(x; 0, 1) \cdot \frac{\partial^k}{\partial x^k} e^{xt} \; dx\\
&= \frac{1}{k!} \int_{-infty}^\infty {\mathcal N}(x; 0, 1) \cdot t^n \cdot e^{xt} \; dx\\
&= \frac{1}{k!} t^n \cdot e^{\frac{t^2}{2}}
\end{align*}
$$

which finishes the proof.

Propositiion:Appell Sequence
: 
$$
\begin{align*}
He_k'(x) &= k \cdot He_{k-1}(x)\\
\frac{d}{dx} [{\mathcal N}(x; 0,1) \cdot He_k(x))] &= -\sqrt{k+1} \cdot [{\mathcal N}(x; 0,1) \cdot He_{k+1}(x)]\\
\frac{d}{dx} [{\mathcal N}(x; 0,\sigma^2) \cdot \phi_k(x; 0, \sigma^2)] &= -\frac{1}{\sigma}  [{\mathcal N}(x; 0,\sigma^2) \cdot \phi_{k+1}(x; 0, \sigma^2)]
\end{align*}
$$

These can be proved by directly plugging in the definition of the Hermite polynomials. The last identity requires a change of variable to scale $x$ properly. 

{% enddetails %}






Finally, here are the first a few standard Hermite polynomials and a plot. 

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/Hermite/Hermite_Plot.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        $$
            \begin{align*}
                \phi_0(x) & = 1\\
                \phi_1(x) & = x\\
                \phi_2(x) &= \frac{1}{\sqrt{2}} (x^2-1)\\
                \phi_3(x) &= \frac{1}{\sqrt{3!}} (x^3-3x)\\
                \phi_4(x) &= \frac{1}{\sqrt{4!}} (x^4 - 6x^2 + 3)\\
                \phi_5(x) &= \frac{1}{\sqrt{5!}} (x^5 - 10x^3 + 15x)
            \end{align*}
        $$
    </div>
</div>
<div class="caption">
   Standard Hermite Basis Functions
</div>


## Informative Features from Observation with Gaussian Additive Noise

The first fact of Hermite polynomials that we will use is the [Mehler's Formula](https://en.wikipedia.org/wiki/Mehler_kernel). This was proved in some classical results <d-cite key="Watson1933"></d-cite> with many extensions. Some of our own works <d-cite key="abbeTIT12"></d-cite><d-cite key="huangFN24"></d-cite> also gave alternative views of this property. Here, we will simply state the fact. 

---
Theorem: Mehler's Formula

: Consider $X, Y$ as bi-variate Normal random variables with zero-mean, and covariance matrix 

$$K_{XY} = \left[\begin{array}{cc} 1 & \rho \\ \rho & 1 \end{array}\right]$$

Then the joint density $p_{XY}$ satisfies 

\begin{align}
\label{eqn:Mehler}
\frac{p_{XY}(x,y)}{  p_X(x) p_Y(y)} &= \frac{1}{\sqrt{1-\rho^2}} \exp \left(-\frac{\rho^2 x^2 + \rho^2 y^2 -2\rho xy}{2(1-\rho^2)}\right) 
= \sum_{k=0}^\infty {\rho^k} \phi_k(x) \cdot \phi_k(y)
\end{align}

---

The first thing to recognize from this fact is that with the orthogonality between $\phi_k$'s, what is written out is a singular value decomposition of the density ratio on the left hand side. If we further recognize that the $k=0$ term is $1$, Mehler's formula can also be written as 

\begin{equation}
\label{eqn:svd}
\frac{p_{XY}(x,y)}{  p_X(x) p_Y(y)} -1 = \sum_{k=1}^\infty {\rho^k} \phi_k(x) \cdot \phi_k(y)
\end{equation}

This is very similar to the concept of [Modal Decomposition](https://lizhongzheng.github.io/blog/2024/modal-decomposition/) in the series of our posts on the H-score. The slight difference is we studied the modal decomposition of the point-wise mutual information (PMI) function, which is the log density ratio; and here, we define the decomposition of the  density ratio minus $1$. The two are equivalent if we allow a "local approximation."

## Multi-Variate Hermite Polynomials

### Separate Learning of Non-Linearity and Memory


Here is a reference <d-cite key="Rahman2017"></d-cite>, good one.
