---
title: "Hermite Polynomials and Applications"
layout: distill
tags: math
categories: ML-Theory
featured: false

mermaid:
  enabled: true
  zoomable: true
code_diff: true
map: true
chart:
  chartjs: true
  echarts: true
  vega_lite: true
tikzjax: true
typograms: true

authors:
  - name: Lizhong Zheng
    url: "https://lizhongzheng.github.io/"
    affiliations:
      name: EECS, MIT


bibliography: blog_references.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: The Key Points
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: What are Hermite Polynomials? Definitions and Notations
  - name: Informative Features from Observation with Gaussian Additive Noise
  - name: Multi-Variate Hermite Polynomials
    subsections:
      -name: Separate Learning of Non-Linearity and Memory
 
---

## The Key Points
The power of Neural Networks is largely derived from the ability to generate complex, non-linear functions for data processing. This marks a significant departure from classical model-based approaches, which often rely on linearity assumptions. Traditional tools such as the Fourier Transform, spectral methods, eigenfunctions, and least mean-square error (LMS) estimators lose their theoretical justification when linearity is removed. Interestingly, the challenge of designing non-linear processing is not new. Foundational work in this area includes Wiener's works on the [Volterra Series](https://en.wikipedia.org/wiki/Volterra_series) and multivariate orthogonal polynomials, which is now a largely overlooked literature known as [the Wiener G-Functional Expansion](https://en.wikipedia.org/wiki/Wiener_series), or the Wiener-Hermite Expansion. This page aims to provide a tutorial on these concepts in the context of modern machine learning applications. In particular, we will explore the connections between these classical techniques and the emerging field of Reservoir Computing, also with our own research on [H-Score Networks](https://lizhongzheng.github.io/blog/2024/H-Score/).
 

## What are Hermite Polynomials: Definitions and Notations

There are several versions of the [Hermite Polynomials](https://en.wikipedia.org/wiki/Hermite_polynomials) defined in the literature. Unfortunately, we won't use any of them, but will define our own before making connections to the more widely used notations. 


Here is a reference <d-cite key="Rahman2017"></d-cite>, good one.


## Informative Features from Observation with Gaussian Additive Noise

## Multi-Variate Hermite Polynomials

### Separate Learning of Non-Linearity and Memory


