<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Learning with Side Information | Lizhong Zheng </title> <meta name="author" content="Lizhong Zheng"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lizhongzheng.github.io/blog/2024/Side-Information/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lizhong</span> Zheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Learning with Side Information</h1> <p class="post-meta"> Created in July 01, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/h-score"> <i class="fa-solid fa-hashtag fa-sm"></i> H-Score</a>   <a href="/blog/tag/modal-decomposition"> <i class="fa-solid fa-hashtag fa-sm"></i> modal-decomposition</a>   ·   <a href="/blog/category/ml-theory"> <i class="fa-solid fa-tag fa-sm"></i> ML-Theory</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#the-key-points">The Key Points</a></li> <li class="toc-entry toc-h2"><a href="#previously">Previously</a></li> <li class="toc-entry toc-h2"><a href="#learning-with-side-information">Learning with Side Information</a></li> <li class="toc-entry toc-h2"><a href="#decomposition-of-multi-variate-dependence">Decomposition of Multi-Variate Dependence</a></li> <li class="toc-entry toc-h2"> <a href="#solution-by-nested-h-score-networks">Solution by Nested H-Score Networks</a> <ul> <li class="toc-entry toc-h3"><a href="#pytorch-implementation">Pytorch Implementation</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#going-forward">Going Forward</a></li> </ul> </div> <hr> <div id="markdown-content"> <blockquote> <h3 id="the-key-points">The Key Points</h3> <p>We introduce our first multi-variate feature extraction problem, the problem of learning with side information. Here, we wish to extract a \(k\)-dimensional feature of the data \(\mathsf x\) that carries useful information to infer the value of the label \(\mathsf y\) while excluding the information that a jointly distributed side information \(\mathsf s\) can provide. We use this example to develop the underlying geometric structure of multi-variate dependence and demonstrate how to use the nested H-Score networks to make projections according to these structures to get good solutions.</p> </blockquote> <h2 id="previously">Previously</h2> <p>We started by observing that the dependence between two random variables \(\mathsf x\) and \(\mathsf y\) can be decomposed into a number of modes, each mode as a simple correlation between a feature \(f_i(\mathsf x)\) and a feature \(g_i(\mathsf y)\). We formulated the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a> as the optimization problem</p> \[\begin{equation} (f^\ast_i, g^\ast_i, i=1, \ldots, k) =\arg\min_{\substack{f_1, \ldots f_k \in \mathcal {F_X},\\g_1, \ldots g_k \in \mathcal {F_Y}}} \; \left\Vert \mathrm{PMI}- \left(\sum_{i=1}^k f_i \otimes g_i\right)\right\Vert^2 \end{equation}\] <p>with the constraints that both \(f^\ast_1, \ldots, f^\ast_k\) and \(g^\ast_1, \ldots, g^\ast_k\) are collections of orthogonal feature functions; and that the correlation \(\sigma_i = \rho(f^\ast_i(\mathsf x), g^\ast_i(\mathsf y))\) are arranged in a descending order.</p> <p>We proposed the <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-Score networks</a> as a numerical approach, using interconnected neural networks to learn the modal decomposition from data. For two collections of feature functions written in vector form as \(\underline{f}, \underline{g}\), the H-score is defined as</p> \[\mathscr{H}(\underline{f}, \underline{g}) =\mathrm{trace} (\mathrm{cov} (\underline{f}, \underline{g})) - \frac{1}{2} \mathrm{trace}(\mathbb E[\underline{f} \cdot \underline{f}^T] \cdot \mathbb E[\underline{g} \cdot \underline{g}^T])\] <table> <tbody> <tr> <td><img src="/assets/img/Hscorenetwork.png" alt="test image" width="250" style="float:left; padding-right:30px"></td> <td><img src="/assets/img/nested%20H2.png" alt="test image" width="450"></td> </tr> <tr> <td><b> H-Score Network </b></td> <td><b> Nested H-Score Network to find features orthogonal to a given \(\bar{f}\) </b></td> </tr> </tbody> </table> <p><br></p> <p>We showed that maximizing the H-score is equivalent to solving the optimization problem in (1) but without some of the constraints. As a step to enrich our toolbox for extracting feature functions, we also developed the <a href="https://lizhongzheng.github.io/blog/2024/nested-H-score/">Nested H-Score networks</a> to learn feature functions that are orthogonal to a given functional subspace. We give examples to demonstrate that such a projection operation in the functional space can be quite versatile, including enforcing all the desired constraints in the original modal decomposition problem (1).</p> <p>These previous results now include the main tools we need to proceed: to find a limited number of information-carrying feature functions and to make projections in functional space, which can all be learned directly from data using interconnected neural networks. In this page, we start to make the case that these are the critical building blocks for more complex multi-variate learning problems.</p> <p><br></p> <h2 id="learning-with-side-information">Learning with Side Information</h2> <table> <tbody> <tr> <td><img src="/assets/img/sideinfo.png" alt="test image" width="350"></td> </tr> <tr> <td><b> Feature Extraction with Side Information </b></td> </tr> </tbody> </table> <p><br></p> <p>We consider the multi-variate learning problem as shown in the figure. Here, we assume that the data \(\mathsf x\), the label \(\mathsf y\), and the side information \(\mathsf s\) are jointly distributed according to some model \(P_{\mathsf {xys}}\) which is not known. Our goal is to find a \(k\)-dimensional feature function \(f_{[k]} = [f_1, \ldots, f_k] : \mathcal X \to \mathbb R^k\), such that when we observe the value of \(\mathsf x\), we can infer the value of \(\mathsf y\) based on these \(k\) features. The only difference between this and a conventional learning problem is that we assume the side information \(\mathsf s\) can be observed at the decision maker. That is, our decision is based on the value of \(\mathsf s\) and the features: \(\widehat{\mathsf y} (f_{[k]}(x), s)\). More importantly, when we extract the feature functions, we know that such side information is available to the decision-maker.</p> <p>As a starting point, we assume we have plenty of samples \((x_i, y_i, s_i), i=1, \ldots\) jointly sampled from the unknown model. Also, we assume that the decision-maker employs the optimal way to combine the side information \(\mathsf s\) and the received \(k\) features of \(\mathsf x\) to estimate the value of \(\mathsf y\). We can imagine a neural network is used to learn this decision function perfectly. The focus of this problem is how to find the \(k\) feature functions to best facilitate this decision-making.</p> <p>The main tension of the problem is on having a limited number of \(k\) features, which is often a much lower dimensional representation of the data \(\mathsf x\). Intuitively, we want the features to be about the dependence between \(\mathsf x\) and \(\mathsf y\) so that we can make good predictions, yet we would like to avoid reporting any information that the side information \(\mathsf s\) can provide. Related problems can be found in the context of <a href="https://en.wikipedia.org/wiki/Information-theoretic_security" rel="external nofollow noopener" target="_blank">protecting sensitive information</a>, <a href="https://en.wikipedia.org/wiki/Fairness_(machine_learning)" rel="external nofollow noopener" target="_blank">fairness in machine learning</a>, and many other <a href="http://web.eng.ucsd.edu/~yhk/nit.html" rel="external nofollow noopener" target="_blank">multi-terminal information theory</a> problems. The difficulty here is that we would like to learn these feature functions using neural networks and thus enjoy the computational efficiency and flexibility therein, but we have the additional task of tuning the feature functions to avoid overlapping contents. It turns out that what we need is a projection operation in the functional space.</p> <p><br></p> <h2 id="decomposition-of-multi-variate-dependence">Decomposition of Multi-Variate Dependence</h2> <p>For this problem, the dependence we would like to work on is the dependence between \(\mathsf x\) and \((\mathsf {s,y})\). We write the PMI as</p> \[\mathrm{PMI}_{\mathsf {x; s,y}} = \log \frac{P_{\mathsf {xsy}}}{P_{\mathsf x}\cdot P_{\mathsf {sy}}} \; \in \mathcal {F_{X\times S\times Y}}\] <p>For this space of joint functions, we define inner product with reference distribution \(R_{\mathsf {xsy}} = R_{\mathsf x}R_{\mathsf {sy}}\), parallel the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">definition of modal decomposition</a>, with random variable \(\mathsf y\) replaced by the tuple \(\mathsf {(s,y)}\). We will take the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/#properties-of-modal-decomposition">local assumption</a> that \(P_\mathsf x \approx R_\mathsf x, P_{\mathsf {sy}} \approx R_{\mathsf {sy}}\). Under this assumption, in the following, we do not distinguish between \(\mathrm{PMI}_\mathsf{x;s,y}\) and its approximation</p> \[\widetilde{\mathrm{PMI}}_{\mathsf{x; s,y}} = \frac{P_\mathsf{xsy}}{P_\mathsf x\cdot P_{\mathsf {sy}}}-1.\] <p>Now we consider the subset of joint distributions that satisfy the Markov condition \(\mathsf {x-s-y}\), which means that \(\mathsf x\) is independent of \(\mathsf y\) given \(\mathsf s\). The corresponding PMI functions form a linear subspace. The linearity follows from the fact that conditional independence is a set of linear (equality) constraints on the joint distribution. We denote this subspace as \(\mathcal M\).</p> <p>For a general PMI function, we now decompose that into the component in \(\mathcal M\) and that orthogonal to \(\mathcal M\).</p> <p><br></p> <hr> <dl> <dt>Definition: Markov Component and Conditional Dependence Components</dt> <dd> <p>For any given PMI function, $\mathrm{PMI}_{\mathsf {x; s,y}} $, the <strong>Markov component</strong> is</p> </dd> </dl> \[\pi_M \stackrel{\Delta}{=}\Pi_M(\mathrm{PMI}_{\mathsf{x; s,y}}) = \arg\min_{\pi \in \mathcal M}\; \left\Vert \mathrm{PMI}_{\mathsf{x; s,y}} - \pi\right\Vert^2\] <p>The optimization is overall all valid PMI functions \(\pi \in \mathcal M\), i.e., with a corresponding joint distribution of \(\mathsf{x, s, y}\) that satisfies the Markov constraint \(\mathsf {x-s-y}\). The norm \(\Vert\cdot \Vert\) in the objective function is defined on the functional space with reference distribution \(R_{\mathsf {xsy}} = P_{\mathsf x}P_{\mathsf{sy}}\).</p> <dl> <dt>Definition: Conditional Dependence Component</dt> <dd> <p>The conditional dependence component of the above PMI function is</p> </dd> </dl> \[\pi_C \stackrel{\Delta}{=}\mathrm{PMI}_{\mathsf{x; s,y}} - \pi_M\] <hr> <p><br></p> <p>By definition, \(\pi_C\) is the error of a linear projection, so we have \(\pi_C \perp \mathcal M\), and the Pythagorean relation.</p> \[\begin{align*} \Vert \mathrm{PMI}_{\mathsf{x; s,y}}\Vert^2 &amp;= \Vert \pi_M\Vert^2 + \Vert \pi_C\Vert^2\\ I (\mathsf{x; (s,y)}) &amp;= I(\mathsf{x;s}) + I(\mathsf{x;y|s}) \end{align*}\] <p>As we stated in the <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/#properties-of-modal-decomposition">properties of modal decomposition</a>, under the local assumption, we have \(\Vert \mathrm{PMI}_{\mathsf {x; s,y}}\Vert^2 \approx 2\cdot I(\mathsf x; \mathsf {s,y})\). Here one can show an additional fact that \(\Vert \pi_M\Vert^2 \approx 2 \cdot I(\mathsf x; \mathsf s)\). From that, we also have \(\Vert \pi_c \Vert^2 \approx 2 \cdot I(\mathsf x; \mathsf y \vert \mathsf s)\). Thus, the above Pythagorean relation is simply a geometric version of the chain rule of mutual information.</p> <p>It decomposes the dependence between \(\mathsf x\) and a pair of random variables \(\mathsf {s,y}\) into two orthogonal components: one follows the Markov constraint and only captures the \(\mathsf {x-s}\) dependence, and the other is the conditional dependence between \(\mathsf x\) and \(\mathsf y\) conditioned on \(\mathsf s\).</p> <p>Going back to our problem of learning with side information. It is clear at this point that in extracting the features of $\mathsf x$, we do not want any component in \(\mathcal M\) since that is only helpful to predict the value of \(\mathsf s\), which is already available at the decision maker.</p> <p>The optimal choice of feature functions under this setup should be the \(k\) strongest modes of \(\pi_C\)! By definition, we need to find the component of the PMI that is orthogonal to \(\mathcal M\), which requires a projection operation in the functional space, and we can do that with a nested H-Score network.</p> <p><br></p> <h2 id="solution-by-nested-h-score-networks">Solution by Nested H-Score Networks</h2> <table> <tbody> <tr> <td><img src="/assets/img/nn_side.png" alt="test image" width="400"></td> </tr> <tr> <td><b> Nested H-Score Network for Learning with Side Information </b></td> </tr> </tbody> </table> <p><br></p> <p>The figure shows a solution to use nested H-Score networks to learn the modal decomposition of the \(\pi_C\) component. Again, it is easier to see how it works with sequential training. We can first train the \(\bar{f}, \bar{g}\) functions using the top H-score, which gives a modal decomposition of the \(\mathsf {x-s}\) dependence, or equivalently, the \(\pi_M\) component. Then we can freeze these choices of \(\bar{f}, \bar{g}\), and train \(f, g\) with the lower H-score, which gives the modal decomposition of the \(\mathsf {x-(s,y)}\) dependence that is orthogonal to \(\mathrm{span}(\bar{f} \otimes \bar{g})\), i.e., the span of all \(\bar{f}_i \otimes \bar{g}_i, i = 1, \dots, \bar{k}\), which is aligned with the \(\pi_C\) component that we want to find. Of course, in practice, we train all networks simultaneously and get the same desired result.</p> <p>A slightly subtle point here is that in order to make the learned \(f, g\) feature functions to be orthogonal to \(\mathcal{M}\), we need to make sure that \(\mathrm{span}(\bar{f} \otimes \bar{g})\) is large enough to cover \(\mathcal M\). In particular, the dimensionality, i.e.,\(\bar{k}\) the number of nodes at the output layer of the \(\bar{f}\) and \(\bar{g}\) networks, need to be large enough. More precisely, we will need \(\bar{k} \geq \mathrm{rank} (P_{\mathsf {xs}})\). This is feasible, particularly if the side information \(\mathsf s\) is a small categorical variable. In such “nice” cases, we can show that the learned \(f, g\) feature functions, with expressive neural networks and a sufficient amount of training, are the desired modal decomposition of the \(\pi_C\) component and hence are the optimal choice of features for the side information problem. A proof of this can be found in this <a href="http://lizhongzheng.mit.edu/sites/default/files/documents/Multivariate%20Feature%20Extraction.pdf" rel="external nofollow noopener" target="_blank">paper</a>.</p> <p>In the case that we cannot guarantee that \(\bar{k}\) is “large enough,” the situation is a bit more complex. Optimizing the top H-Score would fix \(\bar{f}, \bar{g}\) on a \(\bar{k}\) dimensional subspace of \(\mathcal M\), and the nested structure guarantees that \(f, g\) are trained to be orthogonal to this subspace, instead of \(\mathcal M\), and thus can have components in \(\mathcal M\). In the context of learning with side information, this means that the extracted features would have some reduced level of repetitive information of the side information \(\mathsf s\), but not completely repetition-free.</p> <h3 id="pytorch-implementation">Pytorch Implementation</h3> <p>Here is a <a href="https://colab.research.google.com/drive/1g3_vgDWDpHtKgV1x6aoiRJzZ1DaZTCOo#scrollTo=1cZgm3V9Ad9K" rel="external nofollow noopener" target="_blank">colab demo</a> of PyTorch implementation, which also illustrates how to explore the dependence among multiple data observations using this nested H-score.</p> <h2 id="going-forward">Going Forward</h2> <p>The important message here is that multi-variate dependence, represented in the functional space, can be decomposed into a number of subspaces, each corresponding to the dependence of a subset of the variables. Thus, in problems with more than two random variables involved, such as distributed learning, learning with time-varying models, multiple tasks, sensitive information, etc., we often need to separate the observed information according to these subspaces and treat different parts of the information differently. Nested H-Score networks are a good method for this task of separating different types of information by making projections in the functional space without changing the objective function or brute-force post-processing that is separated from the training process. We view this as an attestation of the power of the geometric view of the statistical dependence and the algorithms based on such insights. We will post some more examples in the same spirit, where the information geometric view can lead to some unconventional ways to use neural networks.</p> <p><br></p> <hr> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/" rel="external nofollow noopener" target="_blank">Dr. Xiagxiang Xu</a>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Hermite-Extension/">Extended Properties of Hermite Polynomials</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Hermite/">Hermite Polynomials and Applications in ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/OTFS/">Delay-Doppler Domain Signaling and OTFS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lizhong Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>