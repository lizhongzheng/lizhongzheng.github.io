<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Modal Decomposition | Lizhong Zheng </title> <meta name="author" content="Lizhong Zheng"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lizhong</span> Zheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Modal Decomposition</h1> <p class="post-meta"> Created in June 23, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/h-score"> <i class="fa-solid fa-hashtag fa-sm"></i> H-Score</a>   <a href="/blog/tag/modal-decomposition"> <i class="fa-solid fa-hashtag fa-sm"></i> modal-decomposition</a>   ·   <a href="/blog/category/ml-theory"> <i class="fa-solid fa-tag fa-sm"></i> ML-Theory</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#the-key-points">The Key Points</a></li> <li class="toc-entry toc-h2"> <a href="#modal-decomposition-of-statistical-dependence">Modal Decomposition of Statistical Dependence</a> <ul> <li class="toc-entry toc-h3"><a href="#inner-product-of-functions">Inner Product of Functions</a></li> <li class="toc-entry toc-h3"><a href="#the-pmi-function">The PMI Function</a></li> <li class="toc-entry toc-h3"><a href="#a-single-mode">A Single Mode</a></li> <li class="toc-entry toc-h3"><a href="#modal-decomposition-the-definition">Modal Decomposition: the Definition</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#properties-of-modal-decomposition">Properties of Modal Decomposition</a> <ul> <li class="toc-entry toc-h3"><a href="#the-conditional-expectation-operator">The Conditional Expectation Operator</a></li> <li class="toc-entry toc-h3"><a href="#divergence-and-fisher-information">Divergence and Fisher Information</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#blacktriangle-demo-modal-decomposition-and-neural-networks">\(\blacktriangle\) Demo: Modal Decomposition and Neural Networks</a></li> </ul> </div> <hr> <div id="markdown-content"> <blockquote> <h3 id="the-key-points">The Key Points</h3> <p><code class="language-plaintext highlighter-rouge">Statistical dependence</code> is the reason that we can guess the value of one random variable based on the observation of another. This is the basis of most inference problems like decision-making, estimation, prediction, classification, etc.</p> <p>It is, however, a somewhat ill-posed question to ask, “How much does a random variable \(\mathsf x\) depend on another random variable \(\mathsf y\).” It turns out that, in general, statistical dependence should be understood and quantified as a high dimensional relation: two random variables are dependent through a number of <strong>orthogonal modes</strong>, and each mode can have a different <strong>strength</strong>.</p> <p>The goal of this page is to define these modes mathematically, explain why they are important in practice, and show by examples that many statistical concepts and learning algorithms are directly related to this modal decomposition idea. With that, we will also build the mathematical foundation and notations for the more advanced processing using modal decomposition in the later pages.</p> </blockquote> <p><br></p> <h2 id="modal-decomposition-of-statistical-dependence">Modal Decomposition of Statistical Dependence</h2> <p>Let’s start by motivating and defining the concept of modal decomposition.</p> <h3 id="inner-product-of-functions">Inner Product of Functions</h3> <p>We start by defining an <strong>inner product</strong> in the functional space. Given an alphabet \(\mathcal X\), the space of all real-valued functions,</p> \[\mathcal {F_X} = \{f: \mathcal X \to \mathbb R \},\] <p>can be viewed as a vector space. Here, we need to fix a distribution \(R_\mathsf x\) on \(\mathcal X\), which we call the <strong>reference distribution</strong>. Based on that, we can define the inner product: for any \(f_1, f_2 \in \mathcal F_\mathcal X\),</p> \[\langle f_1, f_2\rangle \stackrel{\Delta}{=} \mathbb E_{\mathsf x \sim R_\mathsf x}[f_1(\mathsf x) \cdot f_2(\mathsf x)]\] <blockquote> <p><strong>Note:</strong> In almost all cases, we can, without loss of generality, restrict functions to have zero mean w.r.t. \(R_\mathsf x\). Thus, the inner product is really the covariance of \(f_1(\mathsf x)\) and \(f_2(\mathsf x)\). Furthermore, on this page, we would not change the reference distribution once chosen, so we could use the above notation for inner products. Otherwise, we could put a subscript to indicate the reference, like \(\langle f_1, f_2\rangle_{R_\mathsf x}\).</p> </blockquote> <p>We can similarly define the inner product on the space of functions on a different alphabet \(\mathcal Y\), with respect to a reference distribution \(R_\mathsf y\).</p> <p><br></p> <h3 id="the-pmi-function">The PMI Function</h3> <p>Now we are ready to address the joint distributions \(P_{\mathsf {xy}}\) on \(\mathcal {X\times Y}\). Again we need to choose a reference distribution \(R_\mathsf {xy}\). For the purpose of this page, we use the product distribution \(R_{\mathsf {xy}} = R_\mathsf x\cdot R_\mathsf y\) and take the resulting definition of the inner product of functions in \(\mathcal F_{\mathcal X\times \mathcal Y}\).</p> <p>A particular function of interest in \(\mathcal {F_{X\times Y}}\) is the <a href="https://en.wikipedia.org/wiki/Pointwise_mutual_information" rel="external nofollow noopener" target="_blank">Point-wise Mutual Information (PMI)</a></p> \[\mathrm{PMI}(x,y) \stackrel{\Delta}{=}\log \frac{P_{\mathsf {xy}}(x,y)}{P_{\mathsf x}(x) \cdot P_{\mathsf y}(y)}, \quad x\in \mathcal X, y \in \mathcal Y\] <p>where \(P_\mathsf x\) and \(P_\mathsf y\) are the \(\mathsf x\) and \(\mathsf y\) marginal distributions of \(P_\mathsf {xy}\). It is clear from the definition that \(\mathrm{PMI}(x,y) = 0, \forall x\) if and only if the two random variables \(\mathsf{x, y}\) are independent; and in general, this function gives a complete description of how the two are dependent to each other. Consequently, the PMI function, or in some equivalent or reduced forms, is the target of almost all learning problems. The main difficulty in practice is that the alphabets \(\mathcal {X, Y}\) are often very big, causing the PMI function to be very high dimensional, which makes these learning tasks difficult.</p> <p>Here, we need to make a technical assumption. For a pair of functions \(f \in \mathcal {F_X}\) and \(g \in \mathcal {F_Y}\), we denote \(f\otimes g \in \mathcal {F_{X\times Y}}\) as the “tensor product function” or simply the product function, with \(f\otimes g(x,y) \stackrel{\Delta}{=} f(x) g(y), \forall x, y\). Now we assume that the joint distribution \(P_{\mathsf {xy}}\) satisfies there exists a possibly infinite sequence of pairs of functions \((f_i, g_i), f_i \in \mathcal {F_X}, g_i \in \mathcal {F_Y}, i=1, 2, \ldots\), such that</p> \[\lim_{n\to \infty} \left\Vert \mathrm{PMI} - \sum_{i=1}^n f_i \otimes g_i \right\Vert^2 = \lim_{n\to \infty} \mathbb E_{\mathsf {x,y} \sim R_\mathsf xR_\mathsf y}\left[ \left(\mathrm{PMI}(\mathsf {x, y}) - \sum_{i=1}^n f_i(\mathsf x) g_i(\mathsf y) \right)^2\right] = 0\] <blockquote> <p><strong>Note:</strong> In other words, this assumption says that the PMI function can be approached, in an L2 sense, by the sum of a countable collection of product functions, with L2 defined w.r.t. the given reference distribution. This assumption is always true for the cases that both \(\mathcal X\) and \(\mathcal Y\) are discrete alphabets. For more general cases, the assumption of a countable basis in the L2 sense is a commonly used assumption, which is not restrictive at all in most practical applications, and convenient for us to rule out some of the “unpleasant” distributions.</p> </blockquote> <p><br></p> <h3 id="a-single-mode">A Single Mode</h3> <p>Why are we so interested in such product functions? In short, it represents a very simple kind of dependence. Imagine a joint distribution \(P_{\mathsf {xy}}\) whose PMI function can be written as</p> \[\log \frac{P_{\mathsf {xy}}(x,y)}{P_\mathsf x(x) P_\mathsf y(y)} = f(x) \cdot g(y), \qquad \forall x, y.\] <p>This can be rewritten as \(P_{\mathsf {y\vert x}}(y\vert x) = P_\mathsf y (y) \cdot \exp(f(x)\cdot g(y)), \forall x, y\). That is, the conditional distribution is on a 1-D exponential family with \(g(\mathsf y)\) as the natural statistic. To make an inference of \(\mathsf y\), we only need to know the value \(f(\mathsf x)\), which is a sufficient statistic. In fact, the only thing we can infer about \(\mathsf y\) is the value of \(g(\mathsf y)\). In general, we could extrapolate from this observation to state that if the PMI function is the sum of a limited number of product functions, then that correspondingly limits the scope of inference tasks we can hope to solve while allowing us to only look at a limited set of statistics, or <strong>features</strong>, of the data.</p> <blockquote> <p>Here, to clarify the terminology, we refer to <strong>feature functions</strong> of a random variable as real-valued functions on the alphabet, such as \(f: \mathcal X \to \mathbb R\). Feature functions are often evaluated with the observed data samples, and the function values, which we refer to as <strong>features</strong>, are used for further inference and learning tasks instead of the raw data. Thus, these features are indeed the <code class="language-plaintext highlighter-rouge">information carrying device.</code> Since any known shifting and scaling do not change the information contents that these features carry, for convenience, we sometimes require a standard form, that the feature functions satisfying \(\mathbb E[f(\mathsf x)] = 0\) and \(\mathbb E[f^2(\mathsf x)]=1\), where both expectations are taken w.r.t. the reference distribution \(R_\mathsf x\).</p> </blockquote> <p>When we write a product function like the one above in this standard form, we need to write out the scaling factor explicitly. That is, instead of \(f\otimes g\), we need to write \(\sigma f\otimes g\), with \(\sigma \geq 0\). We call this triple, \((\sigma, f, g)\), a single <strong>mode</strong>. That is, a mode consists of a strength \(\sigma\), and a pair of feature functions in \(\mathcal {F_X}\) and \(\mathcal {F_Y}\).</p> <p><br></p> <h3 id="modal-decomposition-the-definition">Modal Decomposition: the Definition</h3> <p>Obviously, there are many ways to write a given PMI function into a sum of modes. We hope to have as few modes as possible. In some cases, we might even wish to compromise the precision of the sum and try to have a reasonable approximation of the given PMI with a sum of even fewer modes. That is, for a given finite \(k\), we would like to solve the problem.</p> \[\min_{ (\sigma_i, f_i, g_i), i=1, \ldots, k} \, \left \Vert \mathrm{PMI} - \sum_{i=1}^k \sigma_i f_i\otimes g_i\right\Vert^2\] <p>This optimization is, in fact, a well-studied one. For the case with finite alphabets, the target PMI function can be thought as a \(\vert\mathcal X\vert \times \vert\mathcal Y\vert\) matrix, with the \((x,y)\) entry being the function value \(\mathrm {PMI}(x,y)\); and the above optimization problem is solved by finding the <a href="https://en.wikipedia.org/wiki/Singular_value_decomposition" rel="external nofollow noopener" target="_blank">singular value decomposition (SVD)</a> of this matrix. The result is a decomposition, which is a diagonalization, turning the PMI matrix into the sum of orthogonal rank-1 matrices, each of which corresponds to one mode in our definition. Here, we will define the modal decomposition with a sequential construction, which is indeed a standard way to define SVD.</p> <p><br></p> <hr> <dl> <dt>Definition: Rank-1 Approximation</dt> <dd>For a function \(B \in \mathcal {F_{X\times Y}}\), and a given reference distribution \(R_{\mathsf {xy}} = R_\mathsf x R_\mathsf y\), the rank-1 approximation of \(B\) is a map: \(B \mapsto (\sigma, f^\ast, g^\ast)\),</dd> </dl> \[(\sigma, f^\ast, g^\ast)\stackrel{\Delta}{=} \arg\min_{\sigma, f, g} \; \Vert B - \sigma\cdot f\otimes g\Vert^2\] <p>where the optimization has the constraints: \(\sigma \geq 0\), \(f^\ast \in \mathcal {F_X}, g^\ast\in \mathcal {F_Y}\), are standard feature functions, i.e., \(f^\ast, g^\ast\) both have zero mean and unit variance w.r.t. \(R_\mathsf{x}, R_\mathsf{y}\), respectively.</p> <hr> <p><br></p> <p>We will state here without proof an intuitive property of this approximation, which we will use rather frequently: the approximation error is orthogonal to the optimal feature functions, i.e.</p> \[\begin{align*} &amp;\sum_{x\in \mathcal X} \; R_{\mathsf x}(x) \cdot \left[ \left(B(x,y) - \sigma\cdot f^\ast (x) g^\ast (y)\right) \cdot f^\ast (x) \right] = 0 , \qquad \forall y\\ &amp;\sum_{y\in \mathcal Y} \; R_{\mathsf y}(y) \cdot \left[ \left(B(x,y) - \sigma\cdot f^\ast (x) g^\ast (y)\right) \cdot g^\ast (y) \right] = 0 , \qquad \forall x \end{align*}\] <p>Based on this, we have the following definition of modal decomposition.</p> <p><br></p> <hr> <dl> <dt>Definition: Modal Decomposition \(\zeta\)</dt> <dd> <p>For a given joint distribution \(P_{\mathsf {xy}}\) on \(\mathcal {X \times Y}\) and a reference distribution \(R_{\mathsf {xy}} = R_\mathsf x R_\mathsf y\). We denote the rank-1 approximation of the PMI as</p> </dd> </dl> \[\zeta_1(P_{\mathsf {xy}}) = (\sigma_1, f_1^\ast, g_1^\ast) \stackrel{\Delta}{=} \arg \min_{\sigma, f, g}\;\left\Vert \left(\log \frac{P_{\mathsf {xy}}}{P_\mathsf xP\_\mathsf y}\right) - \sigma\cdot f\otimes g\right\Vert^2\] <p>and for \(i=2, 3, \ldots\), \(\zeta_i\) as the rank-1 approximation of the approximation error of all the previous steps:</p> \[\zeta_i(P_{\mathsf{xy}}) = (\sigma_i, f_i^\ast, g_i^\ast ) \stackrel{\Delta}{=} \arg\min_{\sigma, f, g} \left\Vert\left(\mathrm{PMI} - \sum_{j=1}^{i-1} \sigma_j \cdot f_j^\ast \otimes g_j^\ast \right) - \sigma\cdot f\otimes g\right\Vert^2\] <p>Collectively, \(\lbrace \zeta_i \rbrace : P_{\mathsf {xy}} \mapsto \lbrace(\sigma_i, f^\ast_i, g^\ast_i), i=1, 2, \ldots\rbrace\) is called the <strong>modal decomposition operation</strong></p> <hr> <p><br></p> <p>A few remarks are in order.</p> <ol> <li>The following facts are similar to those of SVD, following similar proof, which we omit: <ul> <li>\(\sigma_1 \geq \sigma_2 \geq \ldots\) in descending order</li> <li>\(\langle f^\ast_i, f^\ast_j \rangle = \langle g^\ast_i, g^\ast_j \rangle = \delta_{ij}\), i.e., the feature functions in different modes are orthogonal to each other.</li> </ul> </li> <li>We denote this decomposition as \(\lbrace\zeta_i \rbrace (P_{\mathsf {xy}})\), or simply \(\zeta(P_{\mathsf {xy}})\), which should be read as “the \(\zeta\)-operation for the \(\mathsf{x-y}\) dependence defined by the joint distribution \(P_{\mathsf{xy}}\)”. While we write the functional decomposition as an L2 approximation to the PMI function, the PMI is not the unique way to describe the dependence. Later, we will have examples where it is convenient to use a slightly different target function, with the resulting choices of the feature functions also being a bit different. We consider all such operations to decompose the dependence as the same general idea.</li> <li>The definition says that for each model \(P_{\mathsf {xy}}\) there is an ideal sequence of modes for the orthogonal decomposition. In practice, we do not observe either the model or the s. We will show later that learning algorithms often try to learn an approximate version of the modes. For example, it is common only to learn the first \(k\) modes, to learn the decomposition of an empirical distribution from a finite dataset, or to have extra restrictions of the learned features due to the limited expressive power of a network, etc. In more complex problems, sometimes it might not even be clear which dependence we are trying to decompose. The purpose of defining the \(\zeta\) operation is to help us clarify what type of compromises are taken in finding a computable approximate solution to the idealized decomposition problem.</li> </ol> <p><br></p> <h2 id="properties-of-modal-decomposition">Properties of Modal Decomposition</h2> <p>There are many nice properties of this modal decomposition. The best way to see them is to go through our <a href="http://lizhongzheng.mit.edu/sites/default/files/documents/mace_final.pdf" rel="external nofollow noopener" target="_blank">survey paper</a>. On this page, we will only state some of them as facts without any proof and sometimes with intuitive but not-so-precise statements. The central point of this is to make the following statement.</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Modal decomposition</code>, the \(\zeta\) operation of a model \(P_{\mathsf {xy}}\), decomposes the dependence between two random variables \(\mathsf x\) and \(\mathsf y\) into a sequence of pairwise correlation between features \(f^\ast_i(\mathsf x)\) and \(g^\ast_i(\mathsf y)\), with correlation coefficient \(\sigma_i\), for \(i=1, 2, \ldots\).</p> </blockquote> <p>This statement is important since in both learning the model \(P_{\mathsf {xy}}\) and using it for inference tasks. We no longer have to carry the entire model, which is often far too complex. Instead, we can learn and use only a subset of modes. Because we have \(\sigma_i\)’s to quantify the strengths of these modes, we would know exactly how to choose the more important modes and how good is the resulting approximate model.</p> <p>One technical issue is the <strong>local assumption</strong>. Many nice properties and connections for the modal decomposition are asymptotic statements, proved in the limiting regime where \(P_{\mathsf {xy}}, P_\mathsf x \cdot P_\mathsf y\), and \(R_\mathsf x\cdot R_\mathsf y\) are all “close” to each other. Such local assumptions are indeed a fundamental concept: The space of probability distributions is not a linear vector space but a manifold. The local assumption allows us to focus on a neighborhood that can be approximated by the tangent plane of the manifold and, hence, get the geometry linearized. Details of this can be found in the literature of <a href="https://www.amazon.com/Information-Translations-Mathematical-Monographs-Tanslations/dp/0821843028" rel="external nofollow noopener" target="_blank">information geometry</a> and <a href="https://en.wikipedia.org/wiki/Correspondence_analysis" rel="external nofollow noopener" target="_blank">correspondence analysis</a>. A quick example is that the following approximation to the PMI function is often used in our development with the assumption that the precision is acceptable.</p> \[\mathrm{PMI}(x,y) = \log \left( \frac{P_{\mathsf {xy}}(x,y)}{P_\mathsf x(x) P_\mathsf y(y)} \right)\approx \widetilde{\mathrm{PMI}}(x,y) = \frac{P_{\mathsf {xy}}(x,y) - P_\mathsf x(x) P_\mathsf y(y)}{P_\mathsf x(x) P_\mathsf y(y)}\] <p>This inevitably leads to some technical details when making mathematical statements. Different statements might require different strengths of the local assumptions, and in some cases, one can even circumvent such assumptions by making a slightly different statement. To avoid leading our readers into such discussions, we will simply call all of such things the “local approximation” and assume they are given for all statements regardless of what is needed. Furthermore, we will hide the rest of these statements in a toggled block. If the reader is comfortable with our main message about decomposing the dependence and not interested in the mathematical steps, this <a href="#an-example-of-numerical-computation-of-modal-decomposition">link</a> can be used to skip to the algorithm part of our story.</p> <p><br></p> <h3 id="the-conditional-expectation-operator">The Conditional Expectation Operator</h3> <p>Now, we enter the regime with the local assumptions. That is, the \(\mathrm{PMI}\) and \(\widetilde{\mathrm{PMI}}\) are now considered the same function. For convenience, we will just take the reference \(R_\mathsf x = P_\mathsf x, R_\mathsf y= P_\mathsf y\) to further simplify things.</p> <p>We start with the interesting fact about \({\mathrm{PMI}}\): when viewed as an operator on the functional space, it is closely related to the conditional expectation operator.</p> <p><br></p> <hr> <dl> <dt>Property 1: PMI and the Conditional Expectation Operator</dt> <dd>Let \(B : \mathcal {F_X} \to \mathcal {F_Y}\) be defined as: for \(a\in \mathcal {F_X}\), \(B(a) \in \mathcal {F_Y}\) with</dd> </dl> \[\begin{align*} \left(B(a)\right) (y) &amp;\stackrel{\Delta}{=} \sum_{x\in \mathcal X} {\mathrm{PMI}}(x,y)\cdot (P_{\mathsf x} (x) \cdot a(x))\\ &amp;= \sum_{x\in \mathcal X}\frac{P_{\mathsf {xy}}(x,y) - P_{\mathsf x}(x) P_{\mathsf y}(y)}{P_{\mathsf x}(x) P_{\mathsf y}(y)} \cdot (P_{\mathsf x}(x) \cdot a(x))\\ &amp;= \mathbb {E} [a({\mathsf x}) | {\mathsf y} = y ] \end{align*}\] <hr> <p>We write sum over \(x\) in the above, which, of course, can be turned into an integral when \(x\) is continuous-valued. The \({\mathrm{PMI}}\) function does not directly act on the input \(a(\cdot)\), but instead needs an extra \(P_\mathsf x(x)\) multiplied. This is “natural” if we think of integrals under the measure specified by the reference.</p> <p>One can also define a transpose operator \(B^T: \mathcal {F_Y}\to \mathcal {F_X}\), for \(b \in \mathcal {F_Y}\),</p> \[\left(B^T(b)\right)(x) = \sum_{y\in \mathcal Y}\frac{P_{\mathsf {xy}}(x,y) - P_{\mathsf x}(x) P_{\mathsf y}(y)}{P_{\mathsf x}(x) P_{\mathsf y}(y)} \cdot (P_{\mathsf y}(y) \cdot b(y))= \mathbb E[b({\mathsf y})|{\mathsf x}=x], \forall x.\] <p>Now if we have the modal decomposition \(\lbrace \zeta_i \rbrace\) of \(P_{\mathsf {xy}}\) as \(\lbrace(\sigma_i, f_i^\ast, g_i^\ast), i=1, 2, \ldots\rbrace\) as defined, we have the following facts.</p> <p>What this property says is that the model \(\mathrm{PMI}\) that we try to learn in a learning system is, in fact, equivalent to the conditional expectation operator of feature functions.</p> <p><br></p> <hr> <dl> <dt>Property 2: Correlation Between \(f_i^\ast({\mathsf x}), g_i^\ast({\mathsf y})\)</dt> <dd> <p>Let \(f_i^\ast, g_i^\ast\) as the modal decomposition defined above for a given dependence model, then</p> </dd> </dl> \[{\mathbb E}_{\mathsf{x,y} \sim P_{\mathsf{x,y}}} [ f^\ast_i ({\mathsf x}) \cdot g^\ast_j({\mathsf y})] =\sigma_i \cdot \delta_{ij}\] <details><summary>Proof:</summary> <p>Consider</p> \[(B(f^\ast_j))(y) = \sum_x \left(\sum_i \sigma_i \cdot f^\ast_i(x) g^\ast_i(y)\right) \cdot \left(P_{\mathsf x}(x) \cdot f^\ast_j(x)\right) = \sigma_j g^\ast_j(y), \quad \forall y\] <p>since \(\mathbb E[f^\ast_i({\mathsf x}) f^\ast_j({\mathsf x})] = \delta_{ij}\). With the same math, we also have \(B^T(g^\ast_j) = \sigma_j \cdot f^\ast_j\).</p> <p>That is, each \(g^\ast_j\) is the image of the \(B(\cdot)\) operator acting on \(f^\ast_j\), scaled by the corresponding \(\sigma_i\), and vice versa.</p> <p>Now, we have \({\mathbb E}_{\mathsf{x,y} \sim P_{\mathsf{x,y}}} [ f^\ast_i ({\mathsf x}) \cdot g^\ast_j({\mathsf y})]= {\mathbb E}[f^\ast_i ({\mathsf x}) \cdot {\mathbb E}[g^\ast_j({\mathsf y})|{\mathsf x}]] =\sigma_i \cdot \delta_{ij}\)</p> </details> <hr> <p>This result says that each feature, \(f^\ast_i(\mathsf x)\), is only correlated with the corresponding \(g^\ast_i\) feature of \(\mathsf y\), and uncorrelated with all other features. \(\sigma_i\) is the correlation coefficient. This gives an alternative understanding of the strength value associated with each mode as the correlation coefficient between feature pairs. Furthermore, the dependence between \(\mathsf x\) and \(\mathsf y\) is, in fact, written as a sequence of correlation between feature pairs, each with a strength quantified by the corresponding \(\sigma_i\).</p> <p>In this <a href="https://static.renyi.hu/renyi_cikkek/1959_on_measures_of_dependence.pdf" rel="external nofollow noopener" target="_blank">1959 paper</a>, the HGR maximal correlation is defined for a given joint distribution \(P_{\mathsf {xy}}\) as</p> \[\rho_{\mathrm{HGR}} \stackrel{\Delta}{=} \max_{f \in \mathcal {F_X}, g \in \mathcal {F_Y}} \; \rho (f(\mathsf x), g(\mathsf y)),\] <p>where \(\rho\) denotes the <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" rel="external nofollow noopener" target="_blank">Pearson correlation coefficient</a>. This maximal correlation coefficient \(\rho_{\mathrm{HGR}}\) is used as a measure of dependence between the two random variables. At this point, it should be clear that the modal decomposition structure is a natural generalization. \(\sigma_1\), the correlation between the strongest correlated feature pairs is exactly the HGR maximal correlation coefficient. Beyond that, there is indeed a sequence of correlated feature pairs in descending order of strengths.</p> <p><br></p> <h3 id="divergence-and-fisher-information">Divergence and Fisher Information</h3> <p>As we stated with the <a href="#a-single-mode">definition of modes</a>, writing the PMI function as a sum of product functions puts the model \(P_{\mathsf {xy}}\) on an exponential family. Locally, the behavior of such an exponential family is fully determined by the <a href="https://en.wikipedia.org/wiki/Fisher_information" rel="external nofollow noopener" target="_blank">Fisher information</a>. For example, if \(\mathrm{PMI}= \log\frac{P_{\mathsf{xy}}}{P_\mathsf x P_\mathsf y} = \sum_{i=1}^k f_i \otimes g_i,\)</p> <p>then the conditional distribution \(P_{\mathsf {x \vert y}}(\cdot \vert y)\), for different values of \(y\), are on a \(k\) - dimensional exponential family with \(f_i(\cdot), i=1, \ldots, k\) as the natural statistics, and \(g_i(y), i=1, \ldots k\) as the corresponding parameters. The Fisher information for this family is a \(k\times k\) matrix \(\mathcal I\), with entries</p> \[[\mathcal I]_{ij} = \mathbb E_{\mathbb x \sim P_\mathsf x} \left[ \left(\frac{\partial}{\partial g_i} \mathrm {PMI}\right) \cdot \left(\frac{\partial}{\partial g_j} \mathrm {PMI}\right)\right] = \mathbb E_{\mathbb x \sim P_\mathsf x} [f_i(\mathsf x) f_j(\mathsf x)] = \langle f_i, f_j \rangle\] <p>which is exactly the definition of the inner product we started with. In this context, we can also understand the <a href="#modal-decomposition">orthogonal modal decomposition</a> as a special and nice case where the Fisher information matrix is diagonalized.</p> <p>There are some direct consequences of this connection.</p> <p><br></p> <hr> <dl> <dt>Property 3: K-L divergence</dt> <dd>If two distribution on \(\mathcal X\), \(P_\mathsf x\) and \(Q_\mathsf x\), are both in the neighborhood of the reference distribution \(R_\mathsf x\), with \(\log P_\mathsf x/Q_\mathsf x = f\), then \(D(P_\mathsf x \Vert Q_\mathsf x) \approx \frac{1}{2} \Vert f\Vert^2\)</dd> <dd> <p>where \(D(P \Vert Q)\) is the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="external nofollow noopener" target="_blank">Kullback-Leibler divergence</a>. The relation between the K-L divergence and the Fisher information can be found <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Fisher_information_metric" rel="external nofollow noopener" target="_blank">here</a>.</p> </dd> </dl> <hr> <p><br></p> <p>Applying this fact to the PMI function, we have the following statement.</p> <hr> <dl> <dt>Property 4: Mutual Information</dt> <dd> \[\frac{1}{2} \Vert \mathrm{PMI} \Vert^2 \approx D(P_{\mathsf {xy}} \Vert P_\mathsf x P_\mathsf y) = I(\mathsf x; \mathsf y)\] </dd> <dd> <p>where \(I(\mathsf x; \mathsf y)\) is the <a href="https://en.wikipedia.org/wiki/Mutual_information" rel="external nofollow noopener" target="_blank">mutual information</a> between \(\mathsf x\) and \(\mathsf y\), which is another popular way to measure how much the two random variables depend on each other.</p> </dd> </dl> <hr> <p><br></p> <p>Now if we have the modal decomposition \(\zeta(P_\mathsf {xy}) = [(\sigma_i, f^\ast_i, g^\ast_i), i=1, 2, \ldots]\), we have the following result.</p> <hr> <dl> <dt>Property 5: Decomposition of the Mutual Information</dt> <dd> \[I(\mathsf x; \mathsf y) = \frac{1}{2} \Vert \mathrm{PMI} \Vert^2 = \frac{1}{2} \sum_i \sigma_i^2\] </dd> </dl> <hr> <p>This is probably the cleanest way to understand the modal decomposition: it breaks the mutual information into the sum of a number of modes, as the (squared) strengths of these modes add up to the mutual information. As stated earlier, it is often difficult to learn or store the PMI function in practice due to the high dimensionality of the data. In these cases, it is a good idea to approximate the PMI function with a truncated version that only keeps the first \(k\) strongest modes. This not only gives the best rank-limited approximation of the joint distribution, as stated in equation (2) in the <a href="#definition-modal-decomposition-zeta">definition</a>, but also captures the most significant dependence relation (the most strongly correlated feature pairs) and in that sense makes the approximation useful in inference tasks.</p> <p><br></p> <h2 id="blacktriangle-demo-modal-decomposition-and-neural-networks">\(\blacktriangle\) <strong>Demo:</strong> Modal Decomposition and Neural Networks</h2> <p>To wrap up this introduction page, we will show one simple example, where we have a small synthesized dataset to train a simple neural network. When the training procedure converges, we demonstrate that the learned features match with the result of the \(\zeta\) operation. The purpose of this numerical example is to show that, with a simple dataset and a carefully chosen neural network, the learning in the <strong>neural network</strong> is indeed finding a low-rank approximation to the true model, which is consistent with the <strong>modal decomposition</strong> operation defined in this page.</p> <p>Here is a <a href="https://colab.research.google.com/drive/1n4qk69shPL0LvGcaUJ4WIeJJRdyp2zA-?usp=sharing" rel="external nofollow noopener" target="_blank">colab demo</a> for illustrating the connection. A more theoretical discussion can be found in <a href="https://doi.org/10.3390/e24010135" rel="external nofollow noopener" target="_blank">this paper</a>.</p> <p><br></p> <hr> <p>This post is based on the joint work with <a href="https://www.linkedin.com/in/xiangxiangxu/" rel="external nofollow noopener" target="_blank">Dr. Xiagxiang Xu</a>.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Hermite-Extension/">Extended Properties of Hermite Polynomials</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/Hermite/">Hermite Polynomials and Applications in ML</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/OTFS/">Delay-Doppler Domain Signaling and OTFS</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lizhong Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>