<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Hermite Polynomials and Applications in ML | Lizhong Zheng </title> <meta name="author" content="Lizhong Zheng"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://lizhongzheng.github.io/blog/2025/Hermite/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.css" integrity="sha256-q9ba7o845pMPFU+zcAll8rv+gC+fSovKsOoNQ6cynuQ=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github.min.css" integrity="sha256-Oppd74ucMR5a5Dq96FxjEzGF7tTw2fZ/6ksAqDCM8GY=" crossorigin="anonymous" media="screen and (prefers-color-scheme: light)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.9.0/styles/github-dark.min.css" integrity="sha256-nyCNAiECsdDHrr/s2OQsp5l9XeY2ZJ0rMepjCT2AkBk=" crossorigin="anonymous" media="screen and (prefers-color-scheme: dark)"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/css/diff2html.min.css" integrity="sha256-IMBK4VNZp0ivwefSn51bswdsrhk0HoMTLc2GqFHFBXg=" crossorigin="anonymous"> <link defer rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css"> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Hermite Polynomials and Applications in ML",
            "description": "",
            "published": "February 14, 2025",
            "authors": [
              
              {
                "author": "Lizhong Zheng",
                "authorURL": "https://lizhongzheng.github.io/",
                "affiliations": [
                  {
                    "name": "EECS, MIT",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Lizhong</span> Zheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Hermite Polynomials and Applications in ML</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#the-key-points">The Key Points</a> </div> <div> <a href="#what-are-hermite-polynomials-definitions-and-notations">What are Hermite Polynomials? Definitions and Notations</a> </div> <div> <a href="#informative-features-from-observation-with-gaussian-additive-noise">Informative Features from Observation with Gaussian Additive Noise</a> </div> <div> <a href="#multi-variate-hermite-polynomials">Multi-Variate Hermite Polynomials</a> </div> <ul> <li> <a href="#"></a> </li> </ul> </nav> </d-contents> <h2 id="the-key-points">The Key Points</h2> <p>The power of Neural Networks is largely derived from the ability to generate complex, non-linear functions for data processing. This marks a significant departure from classical model-based approaches, which often rely on linearity assumptions. Traditional tools such as the Fourier Transform, spectral methods, eigenfunctions, and least mean-square error (LMS) estimators lose their theoretical justification when linearity is removed. Interestingly, the challenge of designing non-linear processing is not new. Foundational work in this area includes Wiener’s works on the <a href="https://en.wikipedia.org/wiki/Volterra_series" rel="external nofollow noopener" target="_blank">Volterra Series</a> and multivariate orthogonal polynomials, which is now a largely overlooked literature known as <a href="https://en.wikipedia.org/wiki/Wiener_series" rel="external nofollow noopener" target="_blank">the Wiener G-Functional Expansion</a>, or the Wiener-Hermite Expansion. This page aims to provide a tutorial on these concepts in the context of modern machine learning applications. In particular, we will explore the connections between these classical techniques and the emerging field of Reservoir Computing, also with our own research on <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-Score Networks</a>.</p> <p>There is a huge literature related to orthogonal polynomials, or more specifically to the Hermite polynomials. Some of the treatments can be quite elaborate, involving complex analysis and combinatorics. Some references are hard to find these days. We will also try to include some of the older papers or present the shortest proofs we know to some of the important classical facts.</p> <h2 id="what-are-hermite-polynomials-definitions-and-notations">What are Hermite Polynomials: Definitions and Notations</h2> <p>There are several versions of the <a href="https://en.wikipedia.org/wiki/Hermite_polynomials" rel="external nofollow noopener" target="_blank">Hermite Polynomials</a> defined in the literature. Unfortunately, we won’t use any of them, but will define our own before making connections to the more widely used notations.</p> <p>To start, we denote \({\mathcal N}(x; \mu, \sigma^2)\) as the Gaussian probability density function (pdf) with mean $\mu$ and variance $\sigma^2$ for variable $x$. We often call this distribution a <strong>reference distribution</strong>.</p> <hr> <dl> <dt>Definition: Hermite Basis Functions</dt> <dd>For a given reference distribution as a Gaussian density function \({\mathcal N}(x; \mu, \sigma^2)\), the Hermite polynomials are the sequence of polynomials $\phi_0, \phi_1, \ldots, \phi_k, \ldots$, where $\phi_k(\cdot; \mu, \sigma^2): \mathbb {R \longrightarrow R}$ is a $k$-degree polynomial, with parameter $\mu, \sigma^2$, and satisfy</dd> </dl> <p>\begin{equation} \label{eqn:Hermite} \langle \phi_i(\cdot;\mu, \sigma^2), \phi_j(\cdot; \mu, \sigma^2) \rangle \stackrel{\Delta}{=} \int_{-\infty}^\infty \; {\mathcal N} (x; \mu, \sigma^2) \cdot \phi_i(x; \mu, \sigma^2) \cdot \phi_j (x; \mu, \sigma^2) \; dx = \delta_{ij} \end{equation}</p> <hr> <p><strong>Remark: The Reference Distribution and Change of Parameter</strong></p> <p>The Hermite basis functions are defined with the parameter $\mu, \sigma^2$, i.e. the Normal distribution used to define the inner product. We can define the “standard” Hermite basis functions as $\phi_k(\cdot; 0, 1)$, i.e. defined with respect to the standard Normal distribution. When $\mu=0$, we often drop that parameter and write $\phi_k(x; \sigma^2)$; and when $\mu=0, \sigma^2=1$, we may drop both the parameters and write $\phi_k(x)$ for convenience.</p> <p>There is a simple conversion between basis functions with different parameters as follows.</p> <dl> <dt>Proposition: Change of Parameter</dt> <dd>For any $k, \mu, \sigma^2$ and any $x$,</dd> </dl> <p>\begin{equation} \label{eqn:change_parameter} \phi_k(x; \mu, \sigma^2) = \phi_k\left( \frac{x-\mu}{\sigma}; 0, 1\right) \end{equation}</p> <details><summary>This can be verified with a simple change of variable.</summary> <p>We only need to verify that with $s = \frac{x-\mu}{\sigma}$:</p> \[\begin{align*} &amp; \int_{-\infty}^\infty {\mathcal N}(x; \mu, \sigma^2) \cdot \phi_i \left(\frac{x-\mu}{\sigma}; 0, 1\right) \cdot \phi_j \left(\frac{x-\mu}{\sigma}; 0, 1\right)\; dx\\\\ &amp;= \int_{-\infty}^\infty {\mathcal N}(s; 0, 1) \cdot \phi_i(s; 0,1) \cdot \phi_j(s; 0, 1) \; ds \\\\ &amp;= \delta_{ij} \end{align*}\] </details> <p><strong>Remark: These basis functions are closely related to Hermite Polynomials.</strong></p> <p>We deliberately use a slightly different notation and a different name to separate the basis functions from the existing terminology on Hermite polynomials. There are many nice properties the Hermite polynomials, but we will use only a few of them. For example, our discussion is generally known in the literature as the “Hermite Expansion”, which starts with a key fact that the basis functions in our definition form a complete basis to all L2 functions <d-cite key="Davis2024HermiteExpansion"></d-cite>. It would be nice to see how our definition of the basis functions is related to Hermite polynomials.</p> <details><summary>Relation to the literature and Some Known Properties.</summary> <p>In the <a href="https://en.wikipedia.org/wiki/Hermite_polynomials" rel="external nofollow noopener" target="_blank">literature</a>, the “probabilist’s Hermite polynomials” are defined as</p> \[He_n (x) = (-1)^n e^{\frac{x^2}{2}} \frac{d^n}{dx^n} e^{-\frac{x^2}{2}}\] <p>which satisfy the orthogonality condition</p> \[\int_{-\infty}^\infty He_i(x)\cdot He_j(x) \cdot e^{-\frac{x^2}{2}} \; dx = \sqrt{2\pi}\cdot i! \cdot \delta_{ij}\] <p>These are nice because the resulting polynomials are <em>monic</em>, and because there is a nice recursion as</p> \[He_{k+1} (x) = x \cdot He_k(x) - He_k'(x)\] <p>In our development, we do not care much about how to find the Hermite polynomials, or how complex are the coefficients. We thus choose a more convenient normalization</p> \[\phi_k(x) \stackrel{\Delta}{=} \frac{1}{\sqrt{k!}} \cdot He_k(x)\] <p>so that the basis function have unit energy.</p> <hr> <p>For the sake of completeness, I will also hide a few important facts about Hermite polynomials in here.</p> <dl> <dt>Proposition: Hermite Expansion</dt> <dd>For an $L_2$ function $f(x)$, we have</dd> </dl> \[f(x) = \sum_{k=0}^\infty d_k \cdot \phi_k(x)\] <p>where</p> \[d_k = \frac{1}{\sqrt{k!}} \int_{-\infty}^\infty \frac{d^k f(x)}{dx^k} \cdot {\mathcal N}(x; 0,1)\; dx\] <p><strong>Proof:</strong></p> <p>By the definition of $\phi_k}’s as a complete orthogonal basis, we have</p> \[d_k = \int {\mathcal N}(x; 0, 1) \cdot f(x) \cdot \phi_k(x) \; dx\] <p>This is where we need the definition of the Hermite polynomials:</p> \[\begin{align*} d_k &amp;= \int \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot f(x) \cdot \left( \frac{1}{\sqrt{k!}} \cdot (-1)^k e^{\frac{x^2}{2}} \frac{d^k}{dx^k} e^{-\frac{x^2}{2}} \right) \; dx\\ &amp;= \frac{1}{\sqrt{k!}} \cdot \int (-1)^k f(x) \cdot \frac{d^k}{dx^k} \mathcal N(x; 0,1)\; dx \end{align*}\] <p>Now the integral needs a $k$-step integral by part to get the desired coefficient.</p> <dl> <dt>Proposition: Generating Function</dt> <dd></dd> </dl> \[e^{xt - \frac{t^2}{2}} = \sum_{k=0}^\infty \frac{t^k}{k!} \cdot He_k(x)\] <p><strong>Proof:</strong></p> <p>This is a simple application of the Hermite expansion result above. Consider $f(x) = e^{xt}$ as a function of $x$:</p> \[\begin{align*} f(x) = \sum_{k=0}^\infty \frac{d_k}{\sqrt{k!}} \cdot He_k(x) \end{align*}\] <p>where</p> \[\begin{align*} \frac{d_k}{\sqrt{k!}} &amp;=\frac{1}{k!} \int_{-infty}^\infty {\mathcal N}(x; 0, 1) \cdot \frac{\partial^k}{\partial x^k} e^{xt} \; dx\\ &amp;= \frac{1}{k!} \int_{-infty}^\infty {\mathcal N}(x; 0, 1) \cdot t^n \cdot e^{xt} \; dx\\ &amp;= \frac{1}{k!} t^n \cdot e^{\frac{t^2}{2}} \end{align*}\] <p>which finishes the proof.</p> <p><a id="Appell-sequence"></a></p> <dl> <dt>Propositiion:Recurrence Relations</dt> <dd></dd> </dl> \[\begin{align*} He_k'(x) &amp;= k \cdot He_{k-1}(x)\\ \frac{d}{dx} [{\mathcal N}(x; 0,1) \cdot He_k(x))] &amp;= - [{\mathcal N}(x; 0,1) \cdot He_{k+1}(x)]\\ \frac{d}{dx} [{\mathcal N}(x; 0,\sigma^2) \cdot \phi_k(x; 0, \sigma^2)] &amp;= -\sqrt{\frac{k+1}{\sigma^2}} [{\mathcal N}(x; 0,\sigma^2) \cdot \phi_{k+1}(x; 0, \sigma^2)] \end{align*}\] <p><strong>Proof:</strong></p> <p>The first identity is also known as Hermite polynomials are an “Appell sequence”. To prove it, we start with the generating function property</p> \[e^{xt - \frac{t^2}{2}} = \sum_{k=0}^\infty \frac{t^k}{k!} \cdot He_k(x)\] <p>Take derivative w.r.t. $x$ on both sides,</p> \[\begin{align*} \mbox{On the LHS:} \quad \frac{\partial}{\partial x} (e^{xt - \frac{t^2}{2}}) &amp;= t \cdot e^{xt - \frac{t^2}{2}}\\ &amp;= t \cdot \left(\sum_{k=0}^\infty \frac{t^k}{k!} \cdot He_k(x)\right)\\ \mbox{On the RHS:} \quad \frac{\partial}{\partial x}\left(\sum_{k=0}^\infty \frac{t^k}{k!} \cdot He_k(x)\right) &amp;= \sum_{k=0}^n \frac{t^k}{k!} \cdot He'_k(x) \end{align*}\] <p>The coefficients on $t^{k}$ must be equal, which gives</p> \[\frac{He'_k(x)}{k!} = He_{k-1}(x){(k-1)!}\] <p>The second identity is a direct consequence of the definition:</p> \[\begin{align*} \frac{d}{dx} [{\mathcal N}(x; 0,1) \cdot He_k(x))] &amp;= \frac{d}{dx} \left[\frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} \cdot (-1)^k e^{\frac{x^2}{2}} \frac{d^k}{dx^k} e^{-\frac{x^2}{2}}\right]\\ &amp;= \frac{1}{\sqrt{2\pi}} (-1)^k \frac{d^{k+1}}{dx^{k+1}} e^{-\frac{x^2}{2}} \\ &amp;= (-1) {\mathcal N}(x; 0,1) \cdot He_{k+1}(x) \end{align*}\] <p>The last one follows by plugging in the definition of the normalized basis functions.</p> </details> <p>Finally, here are the first a few standard Hermite polynomials and a plot.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Hermite/Hermite_Plot-480.webp 480w,/assets/img/Hermite/Hermite_Plot-800.webp 800w,/assets/img/Hermite/Hermite_Plot-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/Hermite/Hermite_Plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> $$ \begin{align*} \phi_0(x) &amp; = 1\\ \phi_1(x) &amp; = x\\ \phi_2(x) &amp;= \frac{1}{\sqrt{2}} (x^2-1)\\ \phi_3(x) &amp;= \frac{1}{\sqrt{3!}} (x^3-3x)\\ \phi_4(x) &amp;= \frac{1}{\sqrt{4!}} (x^4 - 6x^2 + 3)\\ \phi_5(x) &amp;= \frac{1}{\sqrt{5!}} (x^5 - 10x^3 + 15x) \end{align*} $$ </div> </div> <div class="caption"> Standard Hermite Basis Functions </div> <h2 id="informative-features-from-observation-with-gaussian-additive-noise">Informative Features from Observation with Gaussian Additive Noise</h2> <p>The first fact of Hermite polynomials that we will use is the <a href="https://en.wikipedia.org/wiki/Mehler_kernel" rel="external nofollow noopener" target="_blank">Mehler’s Formula</a>. This was proved in some classical results <d-cite key="Watson1933"></d-cite> with many extensions. Some of our own works <d-cite key="abbeTIT12"></d-cite><d-cite key="huangFN24"></d-cite> also gave alternative views of this property. Here, we will simply state the fact.</p> <hr> <dl> <dt>Theorem: Mehler’s Formula</dt> <dd> <p>Consider $X, Y$ as bi-variate Normal random variables with zero-mean, and covariance matrix</p> </dd> </dl> \[K_{XY} = \left[\begin{array}{cc} 1 &amp; \rho \\ \rho &amp; 1 \end{array}\right]\] <p>We define the likelihood ratio function as \(L_{XY}(x,y) \stackrel{\Delta}{=} \frac{p_{XY}(x,y)}{ p_X(x) p_Y(y)} = \frac{1}{\sqrt{1-\rho^2}} \exp \left(-\frac{\rho^2 x^2 + \rho^2 y^2 -2\rho xy}{2(1-\rho^2)}\right)\)</p> <p>then</p> <p>\begin{align} \label{eqn:Mehler} L_{XY}(x,y) = \sum_{k=0}^\infty {\rho^k} \phi_k(x) \cdot \phi_k(y) \end{align}</p> <hr> <p>The first thing to recognize from this fact is that with the orthogonality between $\phi_k$’s, what is written out is a singular value decomposition of the density ratio on the left hand side. If we further recognize that the $k=0$ term is $1$, Mehler’s formula can also be written as</p> <p>\begin{equation} \label{eqn:svd} \frac{p_{XY}(x,y)}{ p_X(x) p_Y(y)} -1 = \sum_{k=1}^\infty {\rho^k} \phi_k(x) \cdot \phi_k(y) \end{equation}</p> <p>This is very similar to the concept of <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">Modal Decomposition</a> in the series of our posts on the H-score. The slight difference is we studied the modal decomposition of the point-wise mutual information (PMI) function, which is the log density ratio; and here, we define the decomposition of the density ratio $L_{XY}$ minus $1$. The two are equivalent if we allow a “local approximation.”</p> <p>Mehler’s formula is known to be not trivial to prove. A 1933 paper <d-cite key="Watson1933"></d-cite> presented three different proofs. In our work <d-cite key="abbeTIT12"></d-cite>, Theorem 2, we provided a proof that we did not find in the literature. ( and, from our biased view, a simpler one. )</p> <details><summary>Our proof to the Mehler’s formula.</summary> <p>The key step is to consider instead of jointly distributed $X,Y$ as in the Theorem, we define $Z = X + W$, with $W$ being an additive noise with $W \sim \mathcal N(0, \sigma_W^2)$, and $Z = \sqrt{1+ \sigma_W^2} \cdot Y$ as a scaling of $Y$. To make this consistent with the original setting, we need</p> \[\rho = \frac{1}{1+\sigma_W^2}.\] <p>With this scaling, the Mehler’s formula becomes</p> \[L_{XZ}(x,z) = \frac{p_{XZ}(x,z)}{p_X(x)p_Z(z)} = \sum_{k=0}^\infty \rho^k \phi_k(x) \cdot \phi_k(z; 1+\sigma_W^2)\] <p>Recognizing this is an SVD structure, all we need to prove is for all $z$</p> <p>\begin{equation} \label{eqn:scaled_Mehler} \int p_X(x) \cdot L_{XZ}(x,z) \cdot \phi_k(x) \; dx = \rho^k \phi_k(z; 1+\sigma_W^2) \end{equation}</p> <p>This should be read as an inner product between $L_{XZ}(\cdot, z)$ and $\phi_k(\cdot)$, weighted by $p_X = {\mathcal N}(\cdot; 0, 1)$. We can rewrite this integral as</p> \[\begin{align*} \int p_{Z\vert X}(z\vert x) \cdot (p_X(x) \cdot \phi_k(x)) \; dx = \rho^k \cdot (p_Z(z) \cdot \phi_k(z; 1+ \sigma_W^2)) \end{align*}\] <p>This new form works if we replace $Z$ by $Y$ and do the proper scaling of the basis functions. It is now clear why this particular scaling is the easiest: the conditional density $p_{Z\vert X}(z\vert x) = {\mathcal N}(z-x; 0, \sigma_W^2)$, which makes the integral a convolution.</p> \[\begin{equation} \label{eqn:induction_target} \int {\mathcal N} (z-x; \sigma_W^2) \cdot ({\mathcal N}(x; 1) \cdot \phi_k(x)) \; dx = \rho^k \cdot ({\mathcal N}(z; 1+\sigma_W^2) \cdot \phi_k(z; 1+ \sigma_W^2)) \end{equation}\] <p>We will see how this makes the proof really easy.</p> <p>First, since $\phi_0(x; \sigma^2) =1$, the above is clearly true for $k=0$.</p> <p>Now, by induction, suppose \eqref{eqn:induction_target} is true for case $k$. We take derivative $\partial/\partial z$ on both side.</p> \[\begin{align*} \frac{\partial}{\partial z} (\mathsf {LHS}) &amp;= {\mathcal N}(\cdot; \sigma_W^2) * \frac{\partial}{\partial x} ({\mathcal N}(x; 1) \phi_k(x)) \\ &amp;= {\mathcal N}(\cdot; \sigma_W^2) * (- \frac{\sqrt{k+1}}{\sigma}{\mathcal N}(x; 1) \phi_{k+1}(x))\\ \\ \frac{\partial}{\partial z} (\mathsf {RHS}) &amp;= \rho^k \cdot \left( - \frac{\sqrt{k+1}}{\sqrt{1+\sigma_W^2}} ({\mathcal N}(z; 1+\sigma_W^2) \cdot \phi_{k+1}(z; 1+ \sigma_W^2) \right) \end{align*}\] <p>The desired result follows directly from this. In both steps above we used the recurrence relation of the Hermite polynomials.</p> \[\frac{d}{dx} [{\mathcal N}(x; 0,\sigma^2) \cdot \phi_k(x; 0, \sigma^2)] = -\frac{\sqrt{k+1}}{\sigma} [{\mathcal N}(x; 0,\sigma^2) \cdot \phi_{k+1}(x; 0, \sigma^2)]\] <p>The proof of this is included in the collapsed session titled <a href="#Appell-sequence">“relation to the literature”</a>.</p> </details> <p>Mehler’s formula with the fact that $\phi_k(x)$’s are a complete orthonormal basis leads to the following useful fact.</p> <dl> <dt>Proposition: Modal Decomposition</dt> <dd>For the bi-variate Normal distributed $X,Y$ defined in the Mehler’s Formula, suppose</dd> </dl> \[f(x) = \sum_k \alpha_k \cdot \phi_k(x),\] <p>Then we have</p> \[\mathbb E[f(X) |Y=y ] = \sum_k \alpha_k\cdot \rho^k \cdot \phi_k(y)\] <p><strong>Proof:</strong></p> <p>This can be verified as follows:</p> \[\begin{align*} \mathbb E [f(X)|Y=y] &amp;= \int_{-\infty}^\infty \frac{p_{XY}(x,y)}{p_Y(y)} \cdot f(x) \; dx\\ &amp;= \int_{-\infty}^\infty p_X(x) \cdot L(x; y) \cdot f(x) \; dx\\ &amp;= \int_{-\infty}^\infty p_X(x) \cdot \left(\sum_l \rho^l \cdot \phi_l(x) \phi_l(y) \right) \cdot \left(\sum_k \alpha_k \cdot \phi_k(x)\right) \; dx \end{align*}\] <p>Use the orthogonality between $\phi_k$’s, we get the desired fact.</p> <p>This fact can be used to generate the following important properties:</p> <ol> <li>$\mathbb E[\phi_k(X) \phi_l(Y)] = \rho^k \cdot \delta_{kl}$. That is, the Hermite basis functions evaluated on $X$ and $Y$ have an one-to-one correspondence, with descending correlation coefficient.</li> <li>The Hermite basis functions are the solutions for the HGR maximal correlation problems <d-cite key="Renyi1959"></d-cite>. That is</li> </ol> \[\{(\phi_i(x), \phi_i(y)), i=1, \ldots, k\} = \arg\max_{(f_i, g_i), i=1, \ldots, k} \; \mathbb \sum_{i=1}^k E[f_i(X) g_i(Y)]\] <ol start="3"> <li>The optimization above can be solved with the <a href="https://en.wikipedia.org/wiki/Alternating_conditional_expectations" rel="external nofollow noopener" target="_blank">Alternating Conditional Expectations (ACE)</a> algorithm.</li> </ol> <p>In our previous post, we defined <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">Modal Decomposition</a>, which breaks down a given statistical dependence between $X$ ad $Y$ into a sequence of pairwise correlation between pairs of features $f_i(X)$ and $g_i(Y)$ for $i=1, 2, \ldots$. The Hermite basis functions are the solution to this decomposition to the special case with bi-variate Normal distributed $X, Y$.</p> <p>Related to this, we also developed in the follow up posts that in general, the correlated pairs of feature functions can be learned from samples using the <a href="https://lizhongzheng.github.io/blog/2024/H-Score/">H-Score Network</a>. Specialized to the bi-variate Normal case, we in fact do not have to learn these feature functions as we already have analytical solutions to them. Alternatively, if we see people learning representations of data by adding Gaussian noise to it, (yes! we mean <a href="https://arxiv.org/abs/2209.00796" rel="external nofollow noopener" target="_blank">diffusion models</a>,) we should know what feature functions they try to learn.</p> <h2 id="multi-variate-hermite-polynomials">Multi-Variate Hermite Polynomials</h2> <p>Polynomials are a natural way to study non-linear functions extending from our understanding of linear functions. Most of the power of neural networks comes from generating non-linear functions of high-dimensional input. A classical tool to study multivariate non-linear functions is the <a href="https://en.wikipedia.org/wiki/Wiener_series" rel="external nofollow noopener" target="_blank">Wiener Series</a>, which is closely related to the <a href="https://en.wikipedia.org/wiki/Volterra_series" rel="external nofollow noopener" target="_blank">Volterra Series</a> and the so-called Wiener-Hermite expansion. This approach was first used by Wiener to analyze non-linear systems with memory. The input to the non-linear function is a collection of past samples—information the system “remembers”—which is then mapped to an observable output. Wiener’s aim was to parameterize all such multivariate non-linear functions by expressing them as linear combinations of multivariate polynomials.</p> <p>A multivariate polynomial over $M$ variables with degree $K$ is a function $f : {\mathbb R}^k \to \mathbb R$, which can be written as</p> \[\begin{align*} f(x_1, \ldots, x_M) &amp;= \sum_{k=0}^K H_k(x_1, \ldots, x_M) \\ &amp;= H_0 + \sum_{k=1}^K \sum_{i_1 = 1}^m \cdots \sum_{i_k = 1}^m h_p(i_1, \ldots, i_k) \prod_{j=1}^k x_{i_j} \end{align*}\] <p>where $H_k$ is a degree-$k$ polynomial, written as a linear combination of terms, each as a produce to $k$ variables (allow repetition) chosen from the input variable set.</p> <p>One can see the indexing quickly becomes cumbersome: it can be done, but makes the real concept hidden behind heavy notations. In our development, we will mostly be limited to the discussions on the bi-variate case with $M=2$, and limit the polynomial to be up to degree $K=3$. Such a polynomial can have a constant term, and terms on $x_1, x_2, x_1^2, x_1x_2, x_2^2, x_1^3, x_1^2 x_2, x_1x_2^2, x_2^3$. Our discussions on this, however, can be generalized to more complex cases.</p> <p>Our question is: given a set of input-output samples ${(x_1[n], x_2[n]; y[n]), n= 1, \ldots, N}$, can we learn the function $f$ efficiently? We will further assume that $(x_1[n], x_2[n])$ are samples from a bi-variate Normal distribution. Our goal is to use the knowledge of Hermite polynomials to construct a basis of this set of non-linear functions, and based on that, some good ways to learn the function from input-output sample sets.</p> <p>For the purpose of defining orthogonal basis functions, we need a reference distribution, which we write as a joint pdf $r_{X_1X_2}$. For two functions $f_A,f_B$, we define the inner product as</p> \[\langle f_A, f_B \rangle \stackrel{\Delta}{=} \int_{x_1, x_2} r_{X_1, X_2} (x_1, x_2) \cdot f_A(x_1, x_2) \cdot f_B (x_1, x_2) \; dx_1 dx_2\] <p>For the rest of this page, we pick $r_{X_1X_2}$ as the bi-variate Normal density with zero mean, and covariance matrix</p> \[\Sigma = \left[\begin{array}{cc} 1 &amp; \rho \\ \rho &amp; 1 \end{array}\right].\] <p>So $r_{X_1X_2}$ is also written as ${\mathcal N}(x_1, x_2; \Sigma)$.</p> <details><summary>This might be confusable with our setting for Mehler’s Formula, which is explained here.</summary> <p>In Mehler’s formula, or equivalently <a href="https://lizhongzheng.github.io/blog/2024/modal-decomposition/">modal decomposition</a>, we try to decompose a likelihood ratio function $L_{XY}$ into the sum of simpler functions in the form of $\phi_k(x)\phi_k(y)$. This is similar to what we do here for multi-variate functions, where we also try to write $f(x_1, x_2)$ into sum of simpler functions.</p> <p>The difference is that in modal decomposition, the orthogonality is defined for functions of a single variables. For example, the Hermite basis functions $\phi_i(x)$’s are orthonormal with respect to the standard Gaussian density. This basis allows us to decompose any function of $x$. We then extend the concept to decompose joint functions into modes. The likelihood ration $L_{XY}$ is a target function we decompose.</p> <p>The study of multi-variate problems here, $[X_1, X_2]$ is considered one variable. The orthogonality is defined for bi-variate function $f(x_1, x_2)$, with the joint Gaussian density $r_{X_1,X_2}$ described above as the reference distribution. It is a coincidence that this is the same bi-variate density we try to decompose in the earlier problem.</p> </details> <dl> <dt>Theorem: Multi-Variate Orthogonal Polynomials Basis</dt> <dd>For a given reference distribution $r_{X_1, X_2} (x_1, x_2) = {\mathcal N} (x_1, x_2; \Sigma)$, the following set of functions ${\phi_{ij}}$ are a set of ortho-normal basis for real-valued functions over $x_1, x_2$.</dd> </dl> \[\phi_{ij}(x_1, x_2; \sigma) = \phi_i (x_1; 0, 1) \cdot \phi_j (x_2; \rho\cdot x, 1-\rho^2)\quad x_1, x_2 \in {\mathbb R}, i, j = 0, 1, 2, \ldots\] <p>in the sense that</p> <p>\begin{equation} \label{eqn:mv_basis} \int\int r_{X_1, X_2}(x_1, x_2) \cdot \phi_{ij}(x_1, x_2; \Sigma) \cdot \phi_{kl}(x_1, x_2; \Sigma) \; dx_1 dx_2 = \delta_{ik} \cdot \delta_{jl} \end{equation}</p> <p>Furthermore, if we construct $Y_i = X_i + W_i, i=1, 2$ with independently distributed additive noise $W_1, W_2 \sim {\mathcal N}(0, \sigma_W^2),$ then the above set of basis functions are the solution to the modal decomposition of the dependence model $p_{XY}$, where $X = [X_1, X_2], Y= [Y_1, Y_2]$.</p> <hr> <h3 id="separate-learning-of-non-linearity-and-memory">Separate Learning of Non-Linearity and Memory</h3> <p>Here is a reference <d-cite key="Rahman2017"></d-cite>, good one.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/blog_references.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Lizhong Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/mermaid@10.7.0/dist/mermaid.min.js" integrity="sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js" integrity="sha256-1rA678n2xEx7x4cTZ5x4wpUCj6kUMZEZ5cxLSVSFWxw=" crossorigin="anonymous"></script> <script defer src="/assets/js/mermaid-setup.js?38ca0a0126f7328d2d9a46bad640931f" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/diff2html@3.4.47/bundles/js/diff2html-ui.min.js" integrity="sha256-eU2TVHX633T1o/bTQp6iIJByYJEtZThhF9bKz/DcbbY=" crossorigin="anonymous"></script> <script defer src="/assets/js/diff2html-setup.js?80a6e52ce727518bbd3aed2bb6ba5601" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/leaflet@1.9.4/dist/leaflet.min.js" integrity="sha256-MgH13bFTTNqsnuEoqNPBLDaqxjGH+lCpqrukmXc8Ppg=" crossorigin="anonymous"></script> <script defer src="/assets/js/leaflet-setup.js?b6313931e203b924523e2d8b75fe8874" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/chart.js@4.4.1/dist/chart.umd.min.js" integrity="sha256-0q+JdOlScWOHcunpUk21uab1jW7C1deBQARHtKMcaB4=" crossorigin="anonymous"></script> <script defer src="/assets/js/chartjs-setup.js?183c5859923724fb1cb3c67593848e71" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/dist/echarts.min.js" integrity="sha256-QvgynZibb2U53SsVu98NggJXYqwRL7tg3FeyfXvPOUY=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/echarts@5.5.0/theme/dark-fresh-cut.js" integrity="sha256-sm6Ui9w41++ZCWmIWDLC18a6ki72FQpWDiYTDxEPXwU=" crossorigin="anonymous"></script> <script defer src="/assets/js/echarts-setup.js?738178999630746a8d0cfc261fc47c2c" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega@5.27.0/build/vega.min.js" integrity="sha256-Yot/cfgMMMpFwkp/5azR20Tfkt24PFqQ6IQS+80HIZs=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-lite@5.16.3/build/vega-lite.min.js" integrity="sha256-TvBvIS5jUN4BSy009usRjNzjI1qRrHPYv7xVLJyjUyw=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/vega-embed@6.24.0/build/vega-embed.min.js" integrity="sha256-FPCJ9JYCC9AZSpvC/t/wHBX7ybueZhIqOMjpWqfl3DU=" crossorigin="anonymous"></script> <script defer src="/assets/js/vega-setup.js?7c7bee055efe9312afc861b128fe5f36" type="text/javascript"></script> <script defer src="https://tikzjax.com/v1/tikzjax.js" integrity="sha256-+1qyucCXRZJrCg3lm3KxRt/7WXaYhBid4/1XJRHGB1E=" crossorigin="anonymous"></script> <script src="/assets/js/typograms.js?062e75bede72543443762dc3fe36c7a5"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>